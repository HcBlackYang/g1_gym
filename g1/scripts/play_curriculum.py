# # play_curriculum.py
# import sys
# import os
# import isaacgym
# import torch
# import numpy as np
#
# # --- ä¿®æ”¹å¯¼å…¥è·¯å¾„ä»¥åŒ¹é…ä½ çš„é¡¹ç›®ç»“æ„ ---
# # å‡è®¾ G1_ROOT_DIR æŒ‡å‘ g1_gym ç›®å½•
# G1_ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # æ¨æ–­é¡¹ç›®æ ¹ç›®å½•
# sys.path.append(G1_ROOT_DIR) # å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„
#
# # ä»ä½ çš„é¡¹ç›®ä¸­å¯¼å…¥å¿…è¦çš„æ¨¡å—
# import g1.envs # ç¡®ä¿ç¯å¢ƒå’Œä»»åŠ¡è¢«æ³¨å†Œ
# from g1.utils import get_args, task_registry, set_seed # å¯¼å…¥ä½ çš„ get_args, task_registry, set_seed
# # from g1.utils.helpers import export_policy_as_jit # å‡è®¾ä½ æœ‰ç±»ä¼¼çš„å¯¼å‡ºå‡½æ•°
# # --- ç»“æŸä¿®æ”¹å¯¼å…¥è·¯å¾„ ---
#
# # --- Helper function to load a specific stage config (optional but cleaner) ---
# # You might need a way to load the specific config object used for training that stage,
# # especially if StageXLocomotionConfig has structure beyond what's in LeggedRobotCfg.
# # For now, we'll rely on task_registry.get_cfgs and override parameters.
#
#
# def play_curriculum(args):
#     """åŠ è½½æŒ‡å®šä»»åŠ¡çš„æ¨¡å‹å¹¶åœ¨ç¯å¢ƒä¸­è¿è¡Œè¿›è¡Œè¯„ä¼°"""
#
#     # --- 1. é…ç½®åŠ è½½å’Œå‚æ•°è¦†ç›– ---
#     print(f"å‡†å¤‡åŠ è½½ä»»åŠ¡ '{args.task}' çš„é…ç½®...")
#     try:
#         # ä» task_registry è·å–ä¸è¯¥ä»»åŠ¡å…³è”çš„åŸºç¡€é…ç½®
#         # æ³¨æ„ï¼šè¿™è·å–çš„æ˜¯æ³¨å†Œæ—¶çš„ *åŸºç¡€* é…ç½®ï¼Œå¯èƒ½ä¸åŒ…å«è¯¾ç¨‹åˆå¹¶çš„å‚æ•°
#         # ä½†å¯¹äº play æ¨¡å¼ï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨ä¿®æ”¹åçš„åŸºç¡€é…ç½®
#         env_cfg, train_cfg = task_registry.get_cfgs(name=args.task)
#     except ValueError:
#         print(f"é”™è¯¯: ä»»åŠ¡ '{args.task}' æœªåœ¨ task_registry ä¸­æ³¨å†Œã€‚")
#         print(f"å¯ç”¨ä»»åŠ¡: {list(task_registry.task_classes.keys())}")
#         return
#     except Exception as e:
#          print(f"åŠ è½½ä»»åŠ¡ '{args.task}' çš„é…ç½®æ—¶å‡ºé”™: {e}")
#          return
#
#
#     print("ä¸ºæµ‹è¯•æ¨¡å¼è¦†ç›–é…ç½®å‚æ•°...")
#     # è¦†ç›–ç¯å¢ƒé…ç½®ä»¥è¿›è¡Œæµ‹è¯•
#     env_cfg.env.num_envs = min(getattr(env_cfg.env, 'num_envs', 64), args.num_envs if args.num_envs is not None else 64) # å‡å°‘ç¯å¢ƒæ•°é‡ä»¥ä¾¿è§‚å¯Ÿ
#     if hasattr(env_cfg, 'terrain'): # æ£€æŸ¥æ˜¯å¦æœ‰ terrain é…ç½®
#         env_cfg.terrain.num_rows = 5    # å‡å°‘åœ°å½¢å¤æ‚åº¦ (å¦‚æœä½¿ç”¨åœ°å½¢)
#         env_cfg.terrain.num_cols = 5
#         env_cfg.terrain.curriculum = False # å…³é—­åœ°å½¢è¯¾ç¨‹
#     if hasattr(env_cfg, 'noise'): # æ£€æŸ¥æ˜¯å¦æœ‰ noise é…ç½®
#         env_cfg.noise.add_noise = False   # å…³é—­è§‚æµ‹å™ªå£°
#     if hasattr(env_cfg, 'domain_rand'): # æ£€æŸ¥æ˜¯å¦æœ‰ domain_rand é…ç½®
#         env_cfg.domain_rand.randomize_friction = False # å…³é—­æ‘©æ“¦éšæœºåŒ–
#         env_cfg.domain_rand.push_robots = False      # å…³é—­æœºå™¨äººæ¨åŠ¨
#
#     env_cfg.env.test = True # è®¾ç½®æµ‹è¯•æ ‡å¿— (å¦‚æœä½ çš„ç¯å¢ƒç±»ä½¿ç”¨å®ƒ)
#
#
#     # --- 2. åˆ›å»ºç¯å¢ƒ ---
#     print(f"åˆ›å»ºç¯å¢ƒ '{args.task}' (æ•°é‡: {env_cfg.env.num_envs})...")
#     try:
#         # ä½¿ç”¨ä¿®æ”¹åçš„ env_cfg åˆ›å»ºç¯å¢ƒ
#         # æ³¨æ„ï¼šæˆ‘ä»¬ä¸ä¼ å…¥è¯¾ç¨‹åˆå¹¶çš„é…ç½®ï¼Œå› ä¸º play é€šå¸¸åœ¨å›ºå®šé…ç½®ä¸‹è¿›è¡Œ
#         env, _ = task_registry.make_env(name=args.task, args=args, env_cfg=env_cfg)
#         print("ç¯å¢ƒåˆ›å»ºæˆåŠŸã€‚")
#     except Exception as e:
#         print(f"åˆ›å»ºç¯å¢ƒæ—¶å‡ºé”™: {e}")
#         import traceback
#         traceback.print_exc()
#         return
#
#     obs = env.get_observations()
#
#     # --- 3. åŠ è½½ç­–ç•¥ ---
#     print("åŠ è½½ç­–ç•¥æ¨¡å‹...")
#     # è®¾ç½®åŠ è½½æ£€æŸ¥ç‚¹æ‰€éœ€çš„è®­ç»ƒé…ç½®å‚æ•°
#     # train_cfg.runner.resume = True # make_alg_runner ä¼šæ ¹æ® args.checkpoint å¤„ç†
#     # train_cfg.runner.load_run = args.load_run # ç”± args æ§åˆ¶
#     # train_cfg.runner.checkpoint = args.checkpoint # ç”± args æ§åˆ¶
#
#     # åˆ›å»º Runner (ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†è·å–ç­–ç•¥åŠ è½½åŠŸèƒ½å’Œæ¨ç†æ¥å£)
#     # æ³¨æ„ï¼šè¿™é‡Œ env_cfg å·²ç»æ˜¯ä¿®æ”¹è¿‡çš„æµ‹è¯•é…ç½®
#     try:
#         # ä¸éœ€è¦è®­ç»ƒé…ç½® train_cfgï¼Œå› ä¸ºæˆ‘ä»¬åªåŠ è½½æ¨¡å‹
#         # ä½† make_alg_runner å¯èƒ½éœ€è¦å®ƒï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥åªåˆ›å»ºç­–ç•¥å¯¹è±¡
#         # ä¸ºäº†ä¸æ ·æ¿ä¸€è‡´ï¼Œæˆ‘ä»¬åˆ›å»º runner
#         # ç¡®ä¿ args.checkpoint æŒ‡å‘ä½ è¦åŠ è½½çš„æ¨¡å‹æ–‡ä»¶
#         if not args.checkpoint:
#              print("é”™è¯¯ï¼šè¯·ä½¿ç”¨ --checkpoint æŒ‡å®šè¦åŠ è½½çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„ã€‚")
#              env.close()
#              return
#
#         print(f"ä½¿ç”¨æ£€æŸ¥ç‚¹: {args.checkpoint}")
#         # åˆ›å»º Runnerï¼Œå®ƒå†…éƒ¨ä¼šæ ¹æ® args.checkpoint åŠ è½½æ¨¡å‹
#         ppo_runner, train_cfg = task_registry.make_alg_runner(env=env, name=args.task, args=args, train_cfg=train_cfg)
#
#         # è·å–ç”¨äºæ¨ç†çš„ç­–ç•¥å‡½æ•°
#         policy = ppo_runner.get_inference_policy(device=env.device)
#         print("ç­–ç•¥åŠ è½½æˆåŠŸã€‚")
#
#     except FileNotFoundError:
#          print(f"é”™è¯¯: æ£€æŸ¥ç‚¹æ–‡ä»¶æœªæ‰¾åˆ°: {args.checkpoint}")
#          env.close()
#          return
#     except Exception as e:
#          print(f"åˆ›å»º Runner æˆ–åŠ è½½ç­–ç•¥æ—¶å‡ºé”™: {e}")
#          import traceback
#          traceback.print_exc()
#          env.close()
#          return
#
#
#     # --- 4. å¯¼å‡ºç­–ç•¥ (å¯é€‰) ---
#     if args.export_policy:
#         export_path = os.path.join(os.path.dirname(args.checkpoint), 'exported') # ä¿å­˜åœ¨æ¨¡å‹åŒç›®å½•ä¸‹çš„ exported æ–‡ä»¶å¤¹
#         # å‡è®¾ä½ çš„ export_policy_as_jit å‡½æ•°å·²å¯¼å…¥æˆ–å®šä¹‰
#         try:
#             from g1.utils.helpers import export_policy_as_jit # å°è¯•å¯¼å…¥
#             export_policy_as_jit(ppo_runner.alg.actor_critic, export_path)
#             print(f'ç­–ç•¥å·²å¯¼å‡ºä¸º JIT è„šæœ¬åˆ°: {export_path}')
#         except ImportError:
#              print("è­¦å‘Š: æœªæ‰¾åˆ° export_policy_as_jit å‡½æ•°ï¼Œè·³è¿‡å¯¼å‡ºã€‚")
#         except Exception as e:
#              print(f"å¯¼å‡ºç­–ç•¥æ—¶å‡ºé”™: {e}")
#
#
#     # --- 5. è¿è¡Œä»¿çœŸå¾ªç¯ ---
#     print("å¼€å§‹è¿è¡Œä»¿çœŸ...")
#     # è®¾ç½®ä¸€ä¸ªåˆç†çš„å¾ªç¯æ¬¡æ•°ï¼Œä¾‹å¦‚è¿è¡Œ N ä¸ªå›åˆ
#     num_episodes_to_run = 10
#     max_steps = num_episodes_to_run * env.max_episode_length
#
#     for i in range(max_steps):
#         with torch.no_grad(): # æ¨ç†æ—¶ä¸éœ€è¦è®¡ç®—æ¢¯åº¦
#              actions = policy(obs.detach())
#         obs, _, rews, dones, infos = env.step(actions.detach())
#
#         # å¯é€‰ï¼šæ·»åŠ å»¶è¿Ÿä»¥ä¾¿è§‚å¯Ÿ
#         # import time
#         # time.sleep(0.01)
#
#         # æ£€æŸ¥æ˜¯å¦æœ‰ç¯å¢ƒå®Œæˆï¼ˆç”¨äºå¯èƒ½çš„ç»Ÿè®¡æˆ–æå‰é€€å‡ºï¼‰
#         # if torch.any(dones):
#         #     print(f"Step {i+1}: æœ‰ç¯å¢ƒå®Œæˆã€‚")
#
#         # æ£€æŸ¥æ˜¯å¦éœ€è¦é€€å‡º (ä¾‹å¦‚æŒ‰ ESC) - render() æ–¹æ³•å†…éƒ¨å¤„ç†
#         # env.render() # render é€šå¸¸åœ¨ step å†…éƒ¨è°ƒç”¨
#
#     print("ä»¿çœŸè¿è¡Œç»“æŸã€‚")
#     env.close()
#
#
# if __name__ == '__main__':
#     # --- è®¾ç½® Play æ¨¡å¼çš„å‚æ•° ---
#     # ä¸è¦å¯¼å‡ºç­–ç•¥ã€å½•åˆ¶è§†é¢‘æˆ–ç§»åŠ¨ç›¸æœºï¼ˆé™¤ééœ€è¦ï¼‰
#     # EXPORT_POLICY = False # é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ§åˆ¶
#     # RECORD_FRAMES = False
#     # MOVE_CAMERA = False
#
#     # è§£æå‘½ä»¤è¡Œå‚æ•°
#     args = parse_args() # ä½¿ç”¨ä½ çš„ parse_args
#
#     # --- ä¸º Play æ¨¡å¼æ·»åŠ /ä¿®æ”¹ç‰¹å®šå‚æ•° ---
#     parser_play = argparse.ArgumentParser(description='Play policy', add_help=False)
#     parser_play.add_argument('--export_policy', action='store_true', default=False, help='Export the policy as a JIT module')
#     # --task å‚æ•°åº”è¯¥ç”± get_args å¤„ç†ï¼Œç¡®ä¿å®ƒå­˜åœ¨
#     if not hasattr(args, 'task') or args.task is None:
#          print("é”™è¯¯: è¯·ä½¿ç”¨ --task æŒ‡å®šè¦è¿è¡Œçš„ä»»åŠ¡åç§° (ä¾‹å¦‚ G1BasicLocomotion)ã€‚")
#          sys.exit(1)
#     # --checkpoint å‚æ•°åº”è¯¥ç”± get_args å¤„ç†ï¼Œç¡®ä¿å®ƒæŒ‡å‘æœ‰æ•ˆçš„æ¨¡å‹æ–‡ä»¶
#     if not hasattr(args, 'checkpoint') or args.checkpoint is None:
#         print("é”™è¯¯: è¯·ä½¿ç”¨ --checkpoint æŒ‡å®šè¦åŠ è½½çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚ logs/.../model_1000.pt)ã€‚")
#         sys.exit(1)
#     # --headless å‚æ•°ç”± get_args å¤„ç†ï¼Œç¡®ä¿ä¸º False ä»¥ä¾¿çœ‹åˆ°å¯è§†åŒ–
#     args.headless = False # å¼ºåˆ¶æ˜¾ç¤º GUI
#
#     # è§£ææ–°å¢çš„ play ç‰¹å®šå‚æ•°
#     play_args, _ = parser_play.parse_known_args()
#     # æ·»åŠ åˆ°ä¸» args å¯¹è±¡
#     for key, value in vars(play_args).items():
#         setattr(args, key, value)
#
#     # è®¾ç½®éšæœºç§å­ï¼ˆå¯é€‰ï¼Œä½†åœ¨è¯„ä¼°æ—¶è®¾ç½®å›ºå®šç§å­å¯èƒ½æ›´å¥½ï¼‰
#     if args.seed is None:
#          args.seed = 42 # è®¾ç½®ä¸€ä¸ªå›ºå®šçš„ç§å­
#     set_seed(args.seed)
#     print(f"Play æ¨¡å¼ç§å­è®¾ç½®ä¸º: {args.seed}")
#
#
#     # è°ƒç”¨ play å‡½æ•°
#     play_curriculum(args)

# play_curriculum.py
import sys
import os
import isaacgym
import torch
import numpy as np
import argparse # Use argparse for play-specific args
import copy # For deep copying config

# --- å¯¼å…¥ G1 é¡¹ç›®æ¨¡å— ---
G1_ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(G1_ROOT_DIR)
import g1.envs # è§¦å‘æ³¨å†Œ
from g1.utils import get_args as get_train_args, task_registry, set_seed # Use training arg parser as base
from g1.envs.configs.curriculum.curriculum_manager_config import CurriculumManagerConfig # Needed potentially if task configs depend on it

# --- DotDict ---
class DotDict(dict):
    """ä¸€ä¸ªæ”¯æŒç‚¹è®¿é—®çš„å­—å…¸ç±»"""
    def __init__(self, *args, **kwargs):
        super(DotDict, self).__init__(*args, **kwargs)
        for arg in args:
            if isinstance(arg, dict):
                for k, v in arg.items():
                    self[k] = DotDict(v) if isinstance(v, dict) else v
        if kwargs:
            for k, v in kwargs.items():
                self[k] = DotDict(v) if isinstance(v, dict) else v
    def __getattr__(self, attr): return self.get(attr)
    def __setattr__(self, key, value): self.__setitem__(key, value)
    def __deepcopy__(self, memo): return DotDict(deepcopy(dict(self), memo=memo))

def parse_play_args():
    """è§£æ Play æ¨¡å¼çš„å‘½ä»¤è¡Œå‚æ•°"""
    # Start with training args parser to get common arguments like --task, --seed, etc.
    parser = get_train_args(parse_known=True) # Use parse_known=True if get_args supports it
    if isinstance(parser, tuple): # If get_args returns (args, unknown)
         args, _ = parser
         # Need to create a new parser for play-specific args
         play_parser = argparse.ArgumentParser(description='Play policy for G1 Curriculum Task')
    else: # If get_args returns the parser object itself
         play_parser = parser
         args = None # Parse later

    # Add/override arguments specific to play mode
    play_parser.add_argument('--checkpoint', required=True, type=str, help='Path to the trained model checkpoint (.pt file)')
    play_parser.add_argument('--num_envs', type=int, default=16, help='Number of environments to visualize')
    play_parser.add_argument('--export_policy', action='store_true', default=False, help='Export the policy as a JIT module')
    play_parser.add_argument('--max_steps', type=int, default=10000, help='Maximum number of simulation steps to run')
    play_parser.add_argument('--task', required=True, type=str, help='Name of the task to run (e.g., G1BasicLocomotion)')

    # Ensure headless is False for visualization
    play_parser.set_defaults(headless=False)
    # Remove arguments not relevant for playing
    # parser.remove_argument('--save_interval') # Example if using standard parser
    # parser.remove_argument('--resume')
    # ... remove other training-specific args if necessary

    if args is None: # If get_train_args returned the parser
         args = play_parser.parse_args()
    else: # If get_train_args returned parsed args, parse the rest
         # Parse only known args from play_parser to avoid conflicts if get_train_args didn't use parse_known
         play_args, unknown = play_parser.parse_known_args()
         # Update the main args object
         for key, value in vars(play_args).items():
              setattr(args, key, value)

    # Final overrides
    args.headless = False # Force GUI
    args.play = True # Add a flag indicating play mode
    args.resume = False # Ensure resume is off

    return args


def play_curriculum(args):
    """åŠ è½½æŒ‡å®šä»»åŠ¡çš„æ¨¡å‹å¹¶åœ¨ç¯å¢ƒä¸­è¿è¡Œè¿›è¡Œè¯„ä¼°"""
    print("="*50)
    print(f"â–¶ï¸ å¼€å§‹è¿è¡Œ Play æ¨¡å¼ - Task: {args.task}")
    print(f"  Checkpoint: {args.checkpoint}")
    print("="*50)

    # --- 1. é…ç½®åŠ è½½å’Œå‚æ•°è¦†ç›– ---
    print("\n--- 1. åŠ è½½å’Œä¿®æ”¹é…ç½® ---")
    try:
        # Get the *base* registered configs for the task
        env_cfg, train_cfg = task_registry.get_cfgs(name=args.task)
        # Make deep copies to modify safely
        env_cfg = copy.deepcopy(env_cfg)
        # train_cfg is needed by make_alg_runner, copy it too
        train_cfg = copy.deepcopy(train_cfg)
        print(f"  åŠ è½½ä»»åŠ¡ '{args.task}' çš„åŸºç¡€é…ç½®æˆåŠŸã€‚")
    except Exception as e:
        print(f"âŒ åŠ è½½ä»»åŠ¡ '{args.task}' çš„åŸºç¡€é…ç½®å¤±è´¥: {e}")
        print(f"   å¯ç”¨ä»»åŠ¡: {task_registry.list_tasks()}")
        return

    print("  ä¸º Play æ¨¡å¼è¦†ç›–é…ç½®å‚æ•°...")
    # General play overrides
    env_cfg.env.num_envs = args.num_envs # Use num_envs from command line
    env_cfg.env.test = True # Set test flag if environment uses it
    if hasattr(env_cfg, 'terrain'):
        env_cfg.terrain.num_rows = 5
        env_cfg.terrain.num_cols = 5
        env_cfg.terrain.curriculum = False
        env_cfg.terrain.max_init_terrain_level = None # Use default terrain
        print("    - å…³é—­åœ°å½¢è¯¾ç¨‹ï¼Œä½¿ç”¨å°å‹é»˜è®¤åœ°å½¢ã€‚")
    if hasattr(env_cfg, 'noise'):
        env_cfg.noise.add_noise = False
        print("    - å…³é—­è§‚æµ‹å™ªå£°ã€‚")
    if hasattr(env_cfg, 'domain_rand'):
        env_cfg.domain_rand.randomize_friction = False
        env_cfg.domain_rand.randomize_base_mass = False
        env_cfg.domain_rand.push_robots = False
        print("    - å…³é—­é¢†åŸŸéšæœºåŒ– (æ‘©æ“¦ã€è´¨é‡ã€æ¨åŠ›)ã€‚")

    # Specific overrides for Stage 1 might be needed if its config has unique settings
    # Example: Ensure arm usage matches the trained model if Stage1LocomotionConfig had such a flag
    # if args.task == "G1BasicLocomotion" and hasattr(env_cfg.env, 'use_arm'):
    #     env_cfg.env.use_arm = True # Or False, depending on how Stage 1 was trained


    # --- 2. åˆ›å»ºç¯å¢ƒ ---
    print("\n--- 2. åˆ›å»ºç¯å¢ƒ ---")
    try:
        # Pass the modified env_cfg
        env, env_cfg = task_registry.make_env(name=args.task, args=args, env_cfg=env_cfg)
        print(f"  âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ (Num Envs: {env.num_envs})")
    except Exception as e:
        print(f"âŒ åˆ›å»ºç¯å¢ƒæ—¶å‡ºé”™: {e}")
        if args.debug: import traceback; traceback.print_exc()
        return

    # --- 3. åŠ è½½ç­–ç•¥ ---
    print("\n--- 3. åŠ è½½ç­–ç•¥ ---")
    try:
        # Create the Runner - it will load the policy based on args.checkpoint
        # Pass the (potentially modified) train_cfg
        train_cfg.runner.resume = True # Tell runner to load
        # args already contains the checkpoint path
        ppo_runner, train_cfg = task_registry.make_alg_runner(env=env, name=args.task, args=args, train_cfg=train_cfg)

        # Get the policy function for inference
        policy = ppo_runner.get_inference_policy(device=env.device)
        print(f"  âœ… ç­–ç•¥ä» {args.checkpoint} åŠ è½½æˆåŠŸã€‚")
    except FileNotFoundError:
         print(f"âŒ é”™è¯¯: æ£€æŸ¥ç‚¹æ–‡ä»¶æœªæ‰¾åˆ°: {args.checkpoint}")
         env.close()
         return
    except Exception as e:
         print(f"âŒ åˆ›å»º Runner æˆ–åŠ è½½ç­–ç•¥æ—¶å‡ºé”™: {e}")
         if args.debug: import traceback; traceback.print_exc()
         env.close()
         return

    # --- 4. å¯¼å‡ºç­–ç•¥ (å¯é€‰) ---
    if args.export_policy:
        print("\n--- 4. å¯¼å‡º JIT ç­–ç•¥ ---")
        export_dir = os.path.join(os.path.dirname(args.checkpoint), 'exported')
        os.makedirs(export_dir, exist_ok=True)
        export_path = os.path.join(export_dir, f'{os.path.basename(args.checkpoint).replace(".pt", "")}_jit.pt')
        try:
            # Assumes actor_critic model is accessible via runner.alg.actor_critic
            model_to_export = ppo_runner.alg.actor_critic
            # Get observation shape from env
            obs_shape = (env.num_observations,)
            # Create dummy input matching the observation shape
            dummy_input = torch.randn((1,) + obs_shape, device=env.device) # Batch size 1

            # Trace the model
            traced_script_module = torch.jit.trace(model_to_export.actor, dummy_input) # Export only the actor part
            traced_script_module.save(export_path)
            print(f'  âœ… ç­–ç•¥ Actor å·²å¯¼å‡ºä¸º JIT è„šæœ¬: {export_path}')
        except AttributeError:
             print("  âŒ é”™è¯¯: æ— æ³•è®¿é—® runner.alg.actor_critic.actor è¿›è¡Œå¯¼å‡ºã€‚")
        except Exception as e:
             print(f"  âŒ å¯¼å‡ºç­–ç•¥æ—¶å‡ºé”™: {e}")


    # --- 5. è¿è¡Œä»¿çœŸå¾ªç¯ ---
    print("\n--- 5. å¼€å§‹ä»¿çœŸå¾ªç¯ ---")
    obs = env.get_observations()
    if isinstance(obs, dict): # Handle dictionary observations if used
         obs = obs['obs']

    num_steps = 0
    total_reward = torch.zeros(env.num_envs, device=env.device)
    total_episodes = 0

    try:
        while num_steps < args.max_steps:
            with torch.no_grad():
                actions = policy(obs.detach())
            obs, _, rews, dones, infos = env.step(actions.detach())
            if isinstance(obs, dict): obs = obs['obs'] # Handle dict obs

            total_reward += rews
            num_steps += 1

            # Count completed episodes and print stats
            done_indices = dones.nonzero(as_tuple=False).squeeze(-1)
            if len(done_indices) > 0:
                completed_episodes = len(done_indices)
                total_episodes += completed_episodes
                avg_reward = torch.mean(total_reward[done_indices]).item()
                print(f"  Step {num_steps:>6} | Episodes Completed: {completed_episodes:>3} | Avg Reward: {avg_reward:>8.2f}")
                # Reset reward for completed envs
                total_reward[done_indices] = 0


            # Check for window close events (handled by env.step -> env.render)
            if env.viewer and env.gym.query_viewer_has_closed(env.viewer):
                print("æ£€æµ‹åˆ°æŸ¥çœ‹å™¨å…³é—­ï¼Œé€€å‡º Play æ¨¡å¼ã€‚")
                break

            # Optional delay
            # import time
            # time.sleep(0.005)

    except KeyboardInterrupt:
        print("\nğŸ›‘ Play æ¨¡å¼è¢«ç”¨æˆ·ä¸­æ–­ (KeyboardInterrupt)")
    except Exception as e:
         print(f"\nâŒ ä»¿çœŸå¾ªç¯ä¸­å‘ç”Ÿé”™è¯¯: {e}")
         if args.debug: import traceback; traceback.print_exc()

    finally:
        print("\n--- 6. Play æ¨¡å¼ç»“æŸ ---")
        print(f"  æ€»è¿è¡Œæ­¥æ•°: {num_steps}")
        print(f"  æ€»å®Œæˆå›åˆæ•°: {total_episodes}")
        env.close()
        print("  ç¯å¢ƒå·²å…³é—­ã€‚")

if __name__ == '__main__':
    # è§£æ Play æ¨¡å¼çš„å‚æ•°
    args = parse_play_args()

    # è®¾ç½®ç§å­
    if args.seed is None: args.seed = 42
    set_seed(args.seed)
    print(f"ğŸ² Play æ¨¡å¼ç§å­è®¾ç½®ä¸º: {args.seed}")

    # è¿è¡Œ Play å‡½æ•°
    play_curriculum(args)