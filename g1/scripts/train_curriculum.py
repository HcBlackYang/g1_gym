# import traceback
# import argparse
# import numpy as np
# from datetime import datetime, timedelta
# import copy
# import inspect
# import os
# import time
# import yaml # For loading curriculum state

# # å¯¼å…¥IsaacGym
# import isaacgym
# from isaacgym import gymapi, gymutil # Import necessary gym modules
# import torch

# # å¯¼å…¥ G1 ç›¸å…³
# import g1.envs # Keep this for task registration trigger
# from g1.envs.curriculum.curriculum_manager import CurriculumManager
# from g1.envs.curriculum.model_transfer import ModelTransfer
# from g1.envs.curriculum.reward_scheduler import RewardScheduler
# from g1.envs.configs.curriculum.curriculum_manager_config import CurriculumManagerConfig

# # å¯¼å…¥å·¥å…·å‡½æ•°å’Œæ³¨å†Œè¡¨
# from g1.utils import get_args, task_registry, set_seed
# # Need helpers for parsing sim params and updating cfg
# from g1.utils.helpers import update_cfg_from_args, class_to_dict, parse_sim_params

# # --- DotDict ---
# class DotDict(dict):
#     """ä¸€ä¸ªæ”¯æŒç‚¹è®¿é—®çš„å­—å…¸ç±»"""
#     def __init__(self, *args, **kwargs):
#         super(DotDict, self).__init__(*args, **kwargs)
#         for arg in args:
#             if isinstance(arg, dict):
#                 for k, v in arg.items():
#                     self[k] = DotDict(v) if isinstance(v, dict) else v
#         if kwargs:
#             for k, v in kwargs.items():
#                 self[k] = DotDict(v) if isinstance(v, dict) else v
#     def __getattr__(self, attr): return self.get(attr)
#     def __setattr__(self, key, value): self.__setitem__(key, value)
#     def __deepcopy__(self, memo): return DotDict(deepcopy(dict(self), memo=memo))


# # --- è§£æå‚æ•° ---
# def parse_args():
#     """è§£æå‘½ä»¤è¡Œå‚æ•°"""
#     args = get_args()
#     parser = argparse.ArgumentParser(description='Train with curriculum', add_help=False)
#     # Add args only if not already present from get_args
#     if not hasattr(args, 'config_class'):
#         parser.add_argument('--config_class', type=str, default='CurriculumManagerConfig', help='è¯¾ç¨‹å­¦ä¹ é…ç½®ç±»åç§°')
#     if not hasattr(args, 'resume_curriculum'):
#         parser.add_argument('--resume_curriculum', type=str, default=None, help='æ¢å¤è®­ç»ƒçš„è¯¾ç¨‹çŠ¶æ€æ–‡ä»¶ (YAML)')
#     if not hasattr(args, 'debug'):
#         parser.add_argument('--debug', action='store_true', default=False, help='å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ‰“å°æ›´å¤šä¿¡æ¯')

#     curriculum_args, _ = parser.parse_known_args()
#     for key, value in vars(curriculum_args).items():
#         setattr(args, key, value)

#     if args.headless is None: args.headless = True # Default headless for training
#     return args


# # --- åŠ è½½è¯¾ç¨‹é…ç½® ---
# def load_curriculum_config(config_class_name):
#     """åŠ è½½è¯¾ç¨‹å­¦ä¹ é…ç½®ç±»å®ä¾‹ï¼Œå¹¶è½¬æ¢ä¸º DotDict"""
#     try:
#         # TODO: Make dynamic based on name if needed
#         if config_class_name == 'CurriculumManagerConfig':
#             config_obj = CurriculumManagerConfig()
#             # Convert class instance attributes to DotDict structure
#             config_dict = {}
#             for key, value in inspect.getmembers(config_obj):
#                  if not key.startswith('_') and not inspect.ismethod(value):
#                       if isinstance(value, dict):
#                            config_dict[key] = DotDict(value)
#                       else:
#                            config_dict[key] = value
#             # Nest under 'curriculum' and 'output' keys for consistency with original access
#             nested_config = DotDict({
#                  'curriculum': DotDict({k: v for k, v in config_dict.items() if k.startswith('stage') or k in ['initial_stage', 'initial_sub_stage', 'max_stages', 'max_sub_stages', 'success_threshold', 'evaluation_window', 'min_steps_between_eval', 'model_transfer']}),
#                  'output': config_dict.get('output', DotDict()), # Handle missing output
#                  'max_env_steps': config_dict.get('max_env_steps', 100_000_000)
#             })
#             print(f"âœ… æˆåŠŸåŠ è½½è¯¾ç¨‹å­¦ä¹ é…ç½®: {config_class_name}")
#             return nested_config
#         else:
#             raise ValueError(f"æœªçŸ¥çš„é…ç½®ç±»åç§°: {config_class_name}")
#     except Exception as e:
#         print(f"âŒ æ— æ³•åŠ è½½è¯¾ç¨‹å­¦ä¹ é…ç½® {config_class_name}: {str(e)}")
#         if args.debug: import traceback; traceback.print_exc()
#         return None


# # --- è·å–ä»»åŠ¡ä¿¡æ¯ ---
# def get_task_info(curriculum_config, stage):
#     """æ ¹æ®è¯¾ç¨‹é˜¶æ®µè·å–ä»»åŠ¡åç§°å’Œè¯¥é˜¶æ®µçš„ç‰¹å®šå‚æ•°"""
#     stage_key = f'stage{stage}'
#     if stage_key not in curriculum_config.curriculum:
#         raise ValueError(f"æœªæ‰¾åˆ°é˜¶æ®µ {stage} çš„é…ç½® (Key: '{stage_key}')")

#     stage_config = curriculum_config.curriculum[stage_key] # Access as attribute/key
#     task_name = stage_config.get('env_class')
#     if not task_name:
#         raise ValueError(f"é˜¶æ®µ {stage} é…ç½®ä¸­æœªæŒ‡å®š 'env_class'")
#     return {'task_name': task_name, 'stage_params': dict(stage_config)}


# # --- éªŒè¯ä»»åŠ¡å…¼å®¹æ€§ ---
# def validate_task_compatibility(task_name):
#     """éªŒè¯ä»»åŠ¡æ˜¯å¦å·²åœ¨ task_registry ä¸­æ³¨å†Œ"""
#     if hasattr(task_registry, 'task_classes') and isinstance(task_registry.task_classes, dict):
#         available_tasks = list(task_registry.task_classes.keys())
#         if task_name not in available_tasks:
#             print(f"âŒ ä»»åŠ¡ '{task_name}' æœªåœ¨ task_registry ä¸­æ³¨å†Œ!")
#             print(f"   å¯ç”¨ä»»åŠ¡: {available_tasks}")
#             return False
#         return True
#     else:
#         print("âŒ é”™è¯¯: æ— æ³•è®¿é—® task_registry.task_classes æ¥éªŒè¯ä»»åŠ¡å…¼å®¹æ€§ã€‚")
#         return False


# # --- æ›´æ–°ç¯å¢ƒå¥–åŠ± ---
# def update_env_rewards(env, reward_scheduler, stage, sub_stage):
#     """æ ¹æ®è¯¾ç¨‹é˜¶æ®µä½¿ç”¨ RewardScheduler æ›´æ–°ç¯å¢ƒå®ä¾‹çš„å¥–åŠ±ç³»æ•°"""
#     print(f"--- Updating reward scales for Stage {stage}.{sub_stage} ---")
#     if not hasattr(env, 'cfg') or not hasattr(env.cfg, 'rewards') or not hasattr(env.cfg.rewards, 'scales'):
#         print("  âš ï¸ Cannot update rewards: env.cfg.rewards.scales structure missing.")
#         return False
#     try:
#         reward_scales_dict = reward_scheduler.get_reward_scales(stage, sub_stage)
#         scales_target = env.cfg.rewards.scales # Should be an object or dict
#         updated_count = 0
#         applied_scales = {}

#         # Get available scale names from the target object/dict
#         if isinstance(scales_target, dict): defined_in_env = list(scales_target.keys())
#         else: defined_in_env = [attr for attr in dir(scales_target) if not attr.startswith('_') and not callable(getattr(scales_target, attr))]

#         for reward_name, scale_value in reward_scales_dict.items():
#             if reward_name in defined_in_env:
#                 try:
#                     if isinstance(scales_target, dict): scales_target[reward_name] = scale_value
#                     else: setattr(scales_target, reward_name, scale_value)
#                     applied_scales[reward_name] = f"{scale_value:.4f}" # Format for printing
#                     updated_count += 1
#                 except Exception as e: print(f"  âš ï¸ Error updating scale '{reward_name}': {e}")
#             # else: print(f"  - Scale '{reward_name}' not found in env config scales.") # Optional warning

#         if updated_count > 0:
#             print(f"  âœ… Updated {updated_count} reward scales. Applied: {applied_scales}")
#             # Re-prepare reward functions to use new scales
#             if hasattr(env, '_prepare_reward_function'):
#                  print("    - Calling env._prepare_reward_function()")
#                  env._prepare_reward_function()
#             return True
#         else:
#             print(f"  âš ï¸ No matching reward scales found to update for Stage {stage}.{sub_stage}.")
#             print(f"    - Available scales in env config: {defined_in_env}")
#             print(f"    - Scales provided by scheduler: {list(reward_scales_dict.keys())}")
#             return False
#     except Exception as e:
#         print(f"âŒ Error during reward update: {str(e)}")
#         if args.debug: import traceback; traceback.print_exc()
#         return False


# # ==============================================================================
# # ä¸»è®­ç»ƒå‡½æ•°
# # ==============================================================================
# def train_curriculum(args):
#     """è¯¾ç¨‹å­¦ä¹ è®­ç»ƒä¸»å‡½æ•°"""
#     print("="*50); print("ğŸš€ å¼€å§‹ G1 è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ ğŸš€"); print("="*50)

#     # --- å…¨å±€ Gym å’Œ Sim ---
#     gym = None
#     sim = None

#     try:
#         # --- 1. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ ---
#         print("\n--- 1. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ ---")
#         curriculum_config = load_curriculum_config(args.config_class)
#         if curriculum_config is None: return

#         curriculum_mgr = CurriculumManager(curriculum_config)
#         print(f"âœ… è¯¾ç¨‹ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ (Output: {curriculum_mgr.output_dir})")

#         device_arg = getattr(args, 'rl_device', 'cuda:0')
#         model_transfer_cfg = curriculum_config.curriculum.model_transfer
#         if not isinstance(model_transfer_cfg, DotDict): model_transfer_cfg = DotDict(model_transfer_cfg)
#         model_transfer_cfg.device = device_arg
#         model_transfer = ModelTransfer(model_transfer_cfg)
#         print(f"âœ… æ¨¡å‹è¿ç§»å·¥å…·åˆ›å»ºæˆåŠŸ")

#         reward_scheduler = RewardScheduler(curriculum_config)
#         print(f"âœ… å¥–åŠ±è°ƒåº¦å™¨åˆ›å»ºæˆåŠŸ")

#         # --- !!! åœ¨è„šæœ¬å¼€å§‹æ—¶åˆ›å»º Gym å’Œ Sim !!! ---
#         print("\n--- åˆå§‹åŒ– Isaac Gym å’Œ Simulation ---")
#         gym = gymapi.acquire_gym()
#         # è§£æ sim_params
#         try:
#             # Need initial config to get sim params structure
#             initial_stage = curriculum_mgr.current_stage # Get initial stage
#             initial_task_info = get_task_info(curriculum_config, initial_stage)
#             initial_env_cfg_cls = task_registry.env_cfgs[initial_task_info['task_name']] # Get class
#             initial_env_cfg = initial_env_cfg_cls() # Create instance
#             sim_params_dict = {"sim": class_to_dict(initial_env_cfg.sim)}
#             sim_params = parse_sim_params(args, sim_params_dict) # Parse into SimParams object
#         except Exception as e:
#              print(f"âŒ è·å–åˆå§‹ Sim å‚æ•°å¤±è´¥: {e}")
#              if args.debug: import traceback; traceback.print_exc()
#              return

#         physics_engine = gymapi.SIM_PHYSX
#         sim_device_type, sim_device_id = gymutil.parse_device_str(args.sim_device)
#         graphics_device_id = sim_device_id if not args.headless else -1

#         sim = gym.create_sim(sim_device_id, graphics_device_id, physics_engine, sim_params)
#         if sim is None: raise RuntimeError("Failed to create sim!")
#         print(f"âœ… Gym å’Œ Sim åˆ›å»ºæˆåŠŸ (Sim Handle: {sim})")
#         # ---------------------------------------------

#         # --- 2. æ¢å¤çŠ¶æ€ (å¦‚æœéœ€è¦) ---
#         print("\n--- 2. æ¢å¤çŠ¶æ€æ£€æŸ¥ ---")
#         loaded_model_path = None
#         # ... (æ¢å¤é€»è¾‘ä¿æŒä¸å˜, ä¼šè®¾ç½® args.checkpoint å’Œ args.resume) ...
#         if args.resume_curriculum:
#             print(f"å°è¯•ä»è¯¾ç¨‹çŠ¶æ€æ–‡ä»¶æ¢å¤: {args.resume_curriculum}")
#             success, loaded_model_path = curriculum_mgr.load_curriculum_state(args.resume_curriculum)
#             if success:
#                 print(f"âœ… å·²æ¢å¤è¯¾ç¨‹çŠ¶æ€ã€‚å½“å‰é˜¶æ®µ: {curriculum_mgr.current_stage}.{curriculum_mgr.current_sub_stage}")
#                 if loaded_model_path:
#                      print(f"  - ä»è¯¾ç¨‹çŠ¶æ€è·å–æ¨¡å‹è·¯å¾„: {loaded_model_path}")
#                      args.checkpoint = loaded_model_path
#                      args.resume = True
#                 else: print("  - è¯¾ç¨‹çŠ¶æ€ä¸­æœªæ‰¾åˆ°æ¨¡å‹è·¯å¾„ã€‚")
#             else:
#                 print(f"âŒ æ— æ³•æ¢å¤è¯¾ç¨‹çŠ¶æ€ï¼Œå°†ä»å¤´å¼€å§‹ã€‚")
#                 args.resume_curriculum = None; args.checkpoint = None; args.resume = False
#         elif args.checkpoint or args.resume:
#              print("ä½¿ç”¨å‘½ä»¤è¡Œ --resume æˆ– --checkpointã€‚")
#              loaded_model_path = args.checkpoint


#         # --- 3. è®¾ç½®åˆå§‹é˜¶æ®µå’Œä»»åŠ¡ ---
#         print("\n--- 3. è®¾ç½®åˆå§‹é˜¶æ®µå’Œä»»åŠ¡ ---")
#         stage, sub_stage = curriculum_mgr.get_current_stage_info()
#         print(f"å½“å‰è¯¾ç¨‹é˜¶æ®µ: {stage}.{sub_stage}")

#         task_name = None; stage_params = None
#         try:
#             task_info = get_task_info(curriculum_config, stage)
#             task_name = task_info.get('task_name')
#             stage_params = task_info.get('stage_params')
#             if not task_name: raise ValueError("è·å–çš„ task_name ä¸ºç©º")
#             print(f"è·å–é˜¶æ®µ {stage} ä»»åŠ¡ä¿¡æ¯: Task='{task_name}', Params={list(stage_params.keys()) if stage_params else 'N/A'}")

#             if hasattr(task_registry, 'task_classes') and isinstance(task_registry.task_classes, dict):
#                 print("--- DEBUG train_curriculum: å¯ç”¨ä»»åŠ¡:", list(task_registry.task_classes.keys()))
#             if not validate_task_compatibility(task_name): return
#             print(f"âœ… ä»»åŠ¡ '{task_name}' éªŒè¯é€šè¿‡ã€‚")
#         except Exception as e:
#             print(f"âŒ è·å–æˆ–éªŒè¯ä»»åŠ¡é…ç½®å¤±è´¥: {str(e)}")
#             if args.debug: import traceback; traceback.print_exc(); return

#         # Update args
#         args.task = task_name
#         cmd_line_num_envs = args.num_envs # Store command line value before override
#         args.num_envs = stage_params.get('num_envs', 2048)
#         if cmd_line_num_envs is not None: args.num_envs = cmd_line_num_envs
#         print(f"  å°†ä½¿ç”¨çš„ç¯å¢ƒæ•°é‡: {args.num_envs}")


#         # --- 4. åˆ›å»ºåˆå§‹ç¯å¢ƒå’Œ Runner ---
#         print("\n--- 4. åˆ›å»ºåˆå§‹ç¯å¢ƒå’Œ Runner ---")
#         env, env_cfg = None, None
#         runner, train_cfg = None, None
#         policy_state_dict = None
#         loaded_env_dims = None

#         try:
#             # 4.1 Load base configs
#             base_env_cfg_cls = task_registry.env_cfgs[args.task] # Get class
#             base_train_cfg_cls = task_registry.train_cfgs[args.task]
#             env_cfg = base_env_cfg_cls() # Create instance
#             train_cfg = base_train_cfg_cls()
#             print(f"  åŠ è½½ä»»åŠ¡ '{args.task}' çš„åŸºç¡€ env_cfg (ç±»å‹: {type(env_cfg)}) å’Œ train_cfg (ç±»å‹: {type(train_cfg)})")

#             # 4.2 Merge curriculum/stage params
#             if not hasattr(env_cfg, 'curriculum'): env_cfg.curriculum = DotDict()
#             elif not isinstance(env_cfg.curriculum, (dict, DotDict)): env_cfg.curriculum = DotDict() # Force DotDict
#             env_cfg.curriculum.stage = stage
#             env_cfg.curriculum.sub_stage = sub_stage
#             stage_params_attr = f'stage{stage}_params'
#             setattr(env_cfg.curriculum, stage_params_attr, DotDict(stage_params)) # Use setattr
#             env_cfg.env.num_envs = args.num_envs # Ensure num_envs matches args
#             print(f"  å·²å°†é˜¶æ®µ {stage} å‚æ•°æ³¨å…¥åˆ° env_cfg.curriculum")

#             # 4.3 Create environment instance (pass gym, sim, sim_params)
#             print(f"  å‡†å¤‡åˆ›å»ºç¯å¢ƒå®ä¾‹...")
#             env, env_cfg = task_registry.make_env(
#                 name=args.task, args=args, env_cfg=env_cfg,
#                 gym_handle=gym, sim_handle=sim, sim_params=sim_params
#             )

#             # 4.4 Update rewards
#             update_env_rewards(env, reward_scheduler, stage, sub_stage)

#             # 4.5 Prepare for checkpoint loading (load data)
#             if args.checkpoint and os.path.exists(args.checkpoint):
#                  print(f"  å‡†å¤‡åŠ è½½æ£€æŸ¥ç‚¹: {args.checkpoint}")
#                  policy_state_dict, loaded_env_dims, loaded_steps, loaded_stage = model_transfer.load_checkpoint(args.checkpoint)
#                  if policy_state_dict is None:
#                       print("  âŒ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥ï¼Œå°†éšæœºåˆå§‹åŒ–æ¨¡å‹ã€‚")
#                       args.checkpoint = None; args.resume = False
#                  else:
#                       print(f"  âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ (æ¥è‡ªé˜¶æ®µ {loaded_stage} @ {loaded_steps:,} æ­¥)")
#                       # Dimension check happens *after* runner creation
#             else:
#                  if args.checkpoint: print(f"  âš ï¸ æŒ‡å®šçš„æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {args.checkpoint}")
#                  print("  å°†éšæœºåˆå§‹åŒ–æ¨¡å‹ã€‚")
#                  args.checkpoint = None; args.resume = False


#             # 4.6 Create Runner
#             print(f"  å‡†å¤‡åˆ›å»º Runner (Task: {args.task})...")
#             # Pass the train_cfg instance created from registry
#             train_cfg.runner.resume = args.resume # Ensure resume flag is correct
#             runner, train_cfg = task_registry.make_alg_runner(env=env, name=args.task, args=args, train_cfg=train_cfg)

#             # 4.7 Execute model loading/transfer *after* runner is created
#             if policy_state_dict: # If loaded from checkpoint
#                  target_policy = runner.alg.actor_critic
#                  current_env_cfg_for_dims = env_cfg # Config for current env
#                  old_env_cfg_for_dims = DotDict({'env': DotDict(loaded_env_dims)}) # Create temp cfg for old dims

#                  # Check dimensions before transfer
#                  if loaded_env_dims['num_observations'] != env.num_observations or \
#                     loaded_env_dims['num_actions'] != env.num_actions:
#                      print("  âš ï¸ æ‰§è¡Œæ¨¡å‹è¿ç§» (ç»´åº¦ä¸åŒ¹é…)...")
#                      model_transfer.transfer_policy(
#                           old_policy_state_dict=policy_state_dict,
#                           old_cfg=old_env_cfg_for_dims,
#                           new_cfg=current_env_cfg_for_dims,
#                           target_policy=target_policy
#                      )
#                  else:
#                       print("  æ£€æŸ¥ç‚¹ç»´åº¦åŒ¹é…ï¼Œç›´æ¥åŠ è½½çŠ¶æ€å­—å…¸...")
#                       try:
#                            target_policy.load_state_dict(policy_state_dict)
#                            print("    âœ… çŠ¶æ€å­—å…¸åŠ è½½æˆåŠŸã€‚")
#                       except Exception as e:
#                            print(f"    âŒ ç›´æ¥åŠ è½½çŠ¶æ€å­—å…¸å¤±è´¥: {e}ã€‚ Runner å¯èƒ½å·²å†…éƒ¨åŠ è½½æˆ–éœ€é‡æ–°è®­ç»ƒã€‚")

#                  # Update runner step/iteration counters if resuming
#                  if args.resume:
#                       if hasattr(runner, 'global_step'): runner.global_step = loaded_steps
#                       if hasattr(runner, 'current_learning_iteration'):
#                           steps_per_iter = getattr(train_cfg.runner, 'num_steps_per_env', 24) * env.num_envs
#                           runner.current_learning_iteration = int(loaded_steps / steps_per_iter) if steps_per_iter > 0 else 0
#                           print(f"    æ¢å¤ Runner è¿­ä»£è®¡æ•°åˆ°çº¦: {runner.current_learning_iteration}")

#             # Set curriculum manager's model path if resuming
#             if args.checkpoint: curriculum_mgr.set_latest_model_path(args.checkpoint)

#         except Exception as e:
#             print(f"âŒ ç¯å¢ƒæˆ–è®­ç»ƒè¿è¡Œå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
#             if args.debug: import traceback; traceback.print_exc();
#             # Cleanup Sim if initialization failed
#             if sim: gym.destroy_sim(sim)
#             return

#         # --- 5. è®­ç»ƒå¾ªç¯ ---
#         print("\n--- 5. å¼€å§‹è®­ç»ƒå¾ªç¯ ---")
#         total_env_steps = getattr(runner, 'global_step', 0)

#         # --- !!! æ£€æŸ¥ max_iterations å’Œ max_env_steps !!! ---
#         if not hasattr(train_cfg, 'runner') or not hasattr(train_cfg.runner, 'max_iterations') or not isinstance(
#                 train_cfg.runner.max_iterations, int):
#             print(f"âš ï¸ è­¦å‘Š: train_cfg.runner.max_iterations æ— æ•ˆæˆ–ç¼ºå¤±ã€‚ä½¿ç”¨é»˜è®¤å€¼ 1500ã€‚")
#             max_iterations = 1500
#         else:
#             max_iterations = train_cfg.runner.max_iterations

#         if not isinstance(curriculum_config.max_env_steps, int):
#             print(f"âš ï¸ è­¦å‘Š: curriculum_config.max_env_steps æ— æ•ˆæˆ–ç¼ºå¤±ã€‚ä½¿ç”¨é»˜è®¤å€¼ 100,000,000ã€‚")
#             max_env_steps = 100_000_000
#         else:
#             max_env_steps = curriculum_config.max_env_steps
#         # --- ç»“æŸæ£€æŸ¥ ---

#         print(f"æœ€å¤§ç¯å¢ƒæ­¥æ•°: {max_env_steps:,}")
#         print(f"æœ€å¤§è¿­ä»£æ¬¡æ•°: {max_iterations:,}")  # æ‰“å°å‡ºæ¥ç¡®è®¤
#         if total_env_steps > 0: print(f"ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œå½“å‰æ€»ç¯å¢ƒæ­¥æ•°: {total_env_steps:,}")

#         start_time_ts = time.time();
#         last_save_time_ts = start_time_ts;
#         last_log_time_ts = start_time_ts
#         if not hasattr(runner, 'current_learning_iteration'): runner.current_learning_iteration = 0


#         try:
#             while runner.current_learning_iteration < train_cfg.runner.max_iterations and total_env_steps < max_env_steps:
#                 current_iter = runner.current_learning_iteration
#                 iter_start_time_ts = time.time()

#                 # --- 5.1 è¿è¡Œä¸€ä¸ªå­¦ä¹ è¿­ä»£ ---
#                 # try: runner.learn(num_learning_iterations=1, init_at_random_ep_len=True)
#                 # except RuntimeError as e: # ... (é”™è¯¯å¤„ç†ä¿æŒä¸å˜) ...
#                 #     if "CUDA out of memory" in str(e): print("\nâŒâŒâŒ CUDA Out of Memory! âŒâŒâŒ");torch.cuda.empty_cache(); raise e
#                 #     elif "tensor a" in str(e) and "tensor b" in str(e): print(f"âŒ è¿è¡Œæ—¶é”™è¯¯ (å¼ é‡å½¢çŠ¶ä¸åŒ¹é…): {e}"); raise e
#                 #     else: print(f"âŒ è®­ç»ƒè¿­ä»£è¿è¡Œæ—¶é”™è¯¯: {str(e)}"); continue
#                 # except Exception as e: print(f"âŒ è®­ç»ƒè¿­ä»£ä¸­å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {str(e)}"); continue # Catch other potential errors

#                 try:
#                     runner.learn(num_learning_iterations=1, init_at_random_ep_len=True)
#                 except RuntimeError as e:  # ... (é”™è¯¯å¤„ç†ä¿æŒä¸å˜) ...
#                     if "CUDA out of memory" in str(e):
#                         print("\nâŒâŒâŒ CUDA Out of Memory! âŒâŒâŒ")
#                         torch.cuda.empty_cache()
#                         raise e
#                     elif "tensor a" in str(e) and "tensor b" in str(
#                             e) or "mat1 and mat2 shapes cannot be multiplied" in str(e):
#                         print("\n\n" + "=" * 50)
#                         print(f"âŒ è¿è¡Œæ—¶é”™è¯¯ (å¼ é‡å½¢çŠ¶ä¸åŒ¹é…): {e}")
#                         print("=" * 50)
#                         # æ‰“å°æ ¸å¿ƒæ¨¡å‹ä¿¡æ¯
#                         if hasattr(runner, 'alg') and hasattr(runner.alg, 'actor_critic'):
#                             actor = runner.alg.actor_critic.actor
#                             if hasattr(actor, '0'):
#                                 print(f"æ¨¡å‹ç¬¬ä¸€å±‚è¾“å…¥ç»´åº¦: {actor[0].in_features}")
#                                 print(f"æ¨¡å‹ç¬¬ä¸€å±‚è¾“å‡ºç»´åº¦: {actor[0].out_features}")
#                         # æ‰“å°ç¯å¢ƒè§‚å¯Ÿç»´åº¦
#                         print(f"ç¯å¢ƒè§‚å¯Ÿç»´åº¦: {env.obs_buf.shape}")
#                         print(f"ç¯å¢ƒé…ç½®è§‚å¯Ÿç»´åº¦: {env.num_observations}")
#                         print("=" * 50)
#                         # ç«‹å³ç»ˆæ­¢ç¨‹åº
#                         import sys
#                         sys.exit(1)
#                     else:
#                         print(f"âŒ è®­ç»ƒè¿­ä»£è¿è¡Œæ—¶é”™è¯¯: {str(e)}")
#                         # ç«‹å³ç»ˆæ­¢ç¨‹åº
#                         import sys
#                         sys.exit(1)
#                 except Exception as e:
#                     print(f"âŒ è®­ç»ƒè¿­ä»£ä¸­å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {str(e)}")
#                     traceback.print_exc()
#                     import sys
#                     sys.exit(1)



#                 # --- 5.2 è·å–ç»Ÿè®¡æ•°æ®å’Œæ›´æ–°æ­¥æ•° ---
#                 train_info = runner.current_statistics
#                 steps_this_iter = runner.num_steps_per_env * env.num_envs
#                 total_env_steps += steps_this_iter


#                 # --- 5.3 æ—¥å¿—è®°å½• ---
#                 iter_time_sec = time.time() - iter_start_time_ts
#                 elapsed_time_sec = time.time() - start_time_ts
#                 elapsed_timedelta = timedelta(seconds=elapsed_time_sec)

#                 if time.time() - last_log_time_ts > 30: # Log every 30 seconds
#                      mean_reward = train_info.get('Mean/reward', float('nan'))
#                      mean_ep_length = train_info.get('Mean/episode_length', float('nan'))
#                      success_rate = train_info.get('success_rate', 0.0) # Assumes runner calculates this

#                      log_msg = (f"S{stage}.{sub_stage} | It {current_iter+1:>5}/{train_cfg.runner.max_iterations} | "
#                                 f"Steps {total_env_steps/1e6:>6.1f}M/{max_env_steps/1e6:.1f}M | "
#                                 f"Rew {mean_reward:>6.2f} | Len {mean_ep_length:>5.1f} | "
#                                 f"SR {success_rate:.3f} | iter time {iter_time_sec:.2f}s | total time {str(elapsed_timedelta).split('.')[0]}")
#                      print(log_msg)
#                      last_log_time_ts = time.time()

#                      curriculum_mgr.update_statistics(success_rate, mean_reward, steps_this_iter)


#                 # --- 5.4 ä¿å­˜æ£€æŸ¥ç‚¹ ---
#                 time_based_save = (time.time() - last_save_time_ts) > 900
#                 iter_based_save = (current_iter + 1) % train_cfg.runner.save_interval == 0
#                 if iter_based_save or time_based_save:
#                      print(f"\n--- Saving Checkpoint (Iteration {current_iter+1}) ---")
#                      try:
#                          model_save_path = runner.save(os.path.join(runner.log_dir, f'model_{current_iter+1}.pt'))
#                          curriculum_mgr.set_latest_model_path(model_save_path)
#                          curriculum_mgr.save_curriculum_state(total_env_steps)
#                          print(f"âœ… Checkpoint saved successfully.")
#                          last_save_time_ts = time.time()
#                      except Exception as e: print(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹æˆ–è¯¾ç¨‹çŠ¶æ€å¤±è´¥: {str(e)}")


#                 # --- !!! ä¸´æ—¶ä¿®æ”¹ï¼šå¼ºåˆ¶è¿›å…¥ Stage 2 !!! ---
#                 force_advance_to_stage2 = False
#                 num_iters_to_test_stage1 = 10
#                 if stage == 1 and current_iter + 1 >= num_iters_to_test_stage1:
#                      print("\n" + "="*15 + f" å¼ºåˆ¶è¿›é˜¶æµ‹è¯• (Iteration {current_iter+1}) " + "="*15)
#                      print(f"è¾¾åˆ° Stage 1 çš„ {num_iters_to_test_stage1} æ¬¡è¿­ä»£ï¼Œå¼ºåˆ¶è§¦å‘è¿›å…¥ Stage 2...")
#                      force_advance_to_stage2 = True
#                 # --- ç»“æŸä¸´æ—¶ä¿®æ”¹ ---


#                 # --- 5.5 æ£€æŸ¥å¹¶æ¨è¿›è¯¾ç¨‹ ---
#                 should_advance = curriculum_mgr.should_advance_curriculum(total_env_steps)
#                 if force_advance_to_stage2 or should_advance:
#                     if not force_advance_to_stage2:
#                          print("\n" + "="*20 + " CURRICULUM ADVANCEMENT CHECK " + "="*20)
#                          print(f"æ¡ä»¶æ»¡è¶³ (SR >= Threshold)ï¼Œå‡†å¤‡ä»é˜¶æ®µ {stage}.{sub_stage} æ¨è¿›...")

#                     # 5.5.1 ä¿å­˜å½“å‰æ¨¡å‹
#                     print("  å¼ºåˆ¶ä¿å­˜å½“å‰æ¨¡å‹...")
#                     # åœ¨é˜¶æ®µåˆ‡æ¢å‰ï¼Œç¡®ä¿æœ‰ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹æ–‡ä»¶
#                     print("  å¼ºåˆ¶ä¿å­˜å½“å‰æ¨¡å‹...")
#                     try:
#                         model_save_path = runner.save(
#                             os.path.join(runner.log_dir, f'model_{current_iter + 1}_pre_transition.pt'))
#                         if model_save_path is None or not os.path.exists(model_save_path):
#                             raise ValueError("æ¨¡å‹ä¿å­˜è¿”å›æ— æ•ˆè·¯å¾„")
#                         curriculum_mgr.set_latest_model_path(model_save_path)
#                         print(f"  âœ… æ¨¡å‹å·²ä¿å­˜: {model_save_path}")
#                     except Exception as e:
#                         print(f"  âŒ è¯¾ç¨‹æ¨è¿›å‰ä¿å­˜æ¨¡å‹å¤±è´¥: {str(e)}")
#                         # å°è¯•å¯»æ‰¾æœ€è¿‘çš„æ¨¡å‹æ–‡ä»¶
#                         model_save_path = None
#                         log_dir = getattr(runner, 'log_dir', None)
#                         if log_dir and os.path.exists(log_dir):
#                             model_files = [os.path.join(log_dir, f) for f in os.listdir(log_dir) if
#                                            f.startswith('model_') and f.endswith('.pt')]
#                             if model_files:
#                                 # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
#                                 model_save_path = sorted(model_files, key=os.path.getmtime)[-1]
#                                 print(f"  æ‰¾åˆ°å¤‡é€‰æ¨¡å‹: {model_save_path}")


#                     # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶
#                     if model_save_path is None or not os.path.exists(model_save_path):
#                         print("  âš ï¸ æ— å¯ç”¨æ¨¡å‹æ–‡ä»¶ã€‚å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–ç­–ç•¥ã€‚")
#                         old_policy_state_dict = None
#                         old_env_dims = {'num_observations': env.num_observations, 'num_actions': env.num_actions}
#                     else:
#                         print(f"  å‡†å¤‡ä» {model_save_path} åŠ è½½æ¨¡å‹...")
#                         old_policy_state_dict, old_env_dims, _, _ = model_transfer.load_checkpoint(model_save_path)
#                         if old_policy_state_dict is None:
#                             print("  âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥ã€‚å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–ç­–ç•¥ã€‚")
#                             old_env_dims = {'num_observations': env.num_observations, 'num_actions': env.num_actions}

#                         # åªæœ‰åœ¨model_save_pathä¸ä¸ºNoneæ—¶æ‰å°è¯•åŠ è½½
#                     if model_save_path is not None:
#                         print("  å‡†å¤‡åŠ è½½/è¿ç§»æ¨¡å‹åˆ°æ–° Runner...")
#                         old_policy_state_dict, old_env_dims, _, _ = model_transfer.load_checkpoint(model_save_path)
#                         if old_policy_state_dict is None:
#                             print(f"  âš ï¸ æ— æ³•ä» {model_save_path} åŠ è½½æ¨¡å‹ï¼Œå°†åˆå§‹åŒ–æ–°æ¨¡å‹")
#                     else:
#                         print("  âš ï¸ è·³è¿‡æ¨¡å‹åŠ è½½/è¿ç§»æ­¥éª¤")
#                         old_policy_state_dict = None
#                         old_env_dims = None

#                     # 5.5.2 æ¨è¿›/è®¾ç½®è¯¾ç¨‹çŠ¶æ€
#                     target_stage = 2; target_sub_stage = 1
#                     if not force_advance_to_stage2:
#                          new_stage_str = curriculum_mgr.advance_curriculum(total_env_steps)
#                          new_stage, new_sub_stage = map(int, new_stage_str.split('.'))
#                     else:
#                          print(f"  æ‰‹åŠ¨è®¾ç½® Curriculum Manager åˆ° Stage {target_stage}.{target_sub_stage}")
#                          curriculum_mgr.current_stage = target_stage
#                          curriculum_mgr.current_sub_stage = target_sub_stage
#                          curriculum_mgr.reset_evaluation()
#                          # è®°å½•å¼ºåˆ¶è½¬æ¢
#                          curriculum_mgr.stage_history.append({
#                              "old_stage": f"{stage}.{sub_stage}", "new_stage": f"{target_stage}.{target_sub_stage}",
#                              "total_env_steps": total_env_steps, "steps_in_stage": curriculum_mgr.total_env_steps_in_stage,
#                              "success_rate_at_transition": curriculum_mgr.get_smoothed_success_rate(),
#                              "average_reward_at_transition": curriculum_mgr.get_average_reward(),
#                              "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
#                              "model_checkpoint": model_save_path, "forced_transition": True
#                          })
#                          curriculum_mgr._save_curriculum_progress_plot()
#                          curriculum_mgr.save_curriculum_state(total_env_steps)
#                          new_stage, new_sub_stage = target_stage, target_sub_stage

#                     print(f"ğŸ“ è¯¾ç¨‹å·²æ¨è¿›/è®¾ç½®ä¸ºæ–°é˜¶æ®µ: {new_stage}.{new_sub_stage}")

#                     # 5.5.3 é˜¶æ®µåˆ‡æ¢å¤„ç†
#                     try:
#                          print("  å¼€å§‹é˜¶æ®µåˆ‡æ¢æµç¨‹...")
#                          new_task_info = get_task_info(curriculum_config, new_stage)
#                          new_task_name = new_task_info['task_name']
#                          new_stage_params = new_task_info['stage_params']
#                          print(f"  æ–°é˜¶æ®µä»»åŠ¡: '{new_task_name}'")

#                          new_base_env_cfg_cls = task_registry.env_cfgs[new_task_name]
#                          new_base_train_cfg_cls = task_registry.train_cfgs[new_task_name]
#                          new_env_cfg = new_base_env_cfg_cls()
#                          new_train_cfg = new_base_train_cfg_cls()

#                          # Merge params
#                          if not hasattr(new_env_cfg, 'curriculum'): new_env_cfg.curriculum = DotDict()
#                          new_env_cfg.curriculum.stage = new_stage
#                          new_env_cfg.curriculum.sub_stage = new_sub_stage
#                          setattr(new_env_cfg.curriculum, f'stage{new_stage}_params', DotDict(new_stage_params))

#                          # Update args
#                          args.task = new_task_name
#                          new_num_envs = new_stage_params.get('num_envs', 1024)
#                          if cmd_line_num_envs is not None: new_num_envs = cmd_line_num_envs
#                          args.num_envs = new_num_envs
#                          new_env_cfg.env.num_envs = new_num_envs
#                          print(f"  æ–°ç¯å¢ƒæ•°é‡: {args.num_envs}")

#                          # --- !!! æ¸…ç†æ—§ç¯å¢ƒå’Œ Runner !!! ---
#                          print("  æ¸…ç†æ—§ç¯å¢ƒé€»è¾‘å®ä¾‹å’Œ Runner...")

#                          # åœ¨"5.5.3 é˜¶æ®µåˆ‡æ¢å¤„ç†"éƒ¨åˆ†
#                          print("  æ¸…ç†æ—§ç¯å¢ƒé€»è¾‘å®ä¾‹å’Œ Runner...")
#                          if env is not None:
#                              env.close()  # è¿™ä¼šè°ƒç”¨destroy_viewer
#                              print("  - ç¯å¢ƒå·²å…³é—­ï¼ˆæŸ¥çœ‹å™¨å·²é”€æ¯ï¼‰")
#                              del env
#                              env = None

#                          if runner is not None:
#                              del runner
#                              runner = None

#                          # æ¸…ç†GPUå†…å­˜
#                          torch.cuda.empty_cache()

#                          # é‡è¦ï¼šè§£ææ–°ç¯å¢ƒçš„simå‚æ•°
#                          new_sim_params_dict = {"sim": class_to_dict(new_env_cfg.sim)}
#                          new_sim_params = parse_sim_params(args, new_sim_params_dict)

#                          # é‡æ–°åˆ›å»ºsim
#                          print("  é‡æ–°åˆ›å»ºSimulationä»¥é€‚åº”æ–°ç¯å¢ƒ...")
#                          if sim is not None:
#                              gym.destroy_sim(sim)
#                              sim = None

#                          sim = gym.create_sim(sim_device_id, graphics_device_id, physics_engine, new_sim_params)
#                          if sim is None:
#                              raise RuntimeError("Failed to create new simulation!")
#                          print("  âœ… æ–°çš„Simåˆ›å»ºæˆåŠŸ")

#                          # Create new environment (pass existing gym, sim, sim_params)
#                          print("  åˆ›å»ºæ–°ç¯å¢ƒå®ä¾‹...")
#                          env, env_cfg = task_registry.make_env(
#                              name=args.task, args=args, env_cfg=new_env_cfg,
#                              gym_handle=gym, sim_handle=sim, sim_params=sim_params
#                          )

#                          # Update rewards
#                          update_env_rewards(env, reward_scheduler, new_stage, new_sub_stage)

#                          # Load old model state for transfer
#                          print("  å‡†å¤‡åŠ è½½/è¿ç§»æ¨¡å‹åˆ°æ–° Runner...")
#                          old_policy_state_dict, old_env_dims, _, _ = model_transfer.load_checkpoint(model_save_path)
#                          if old_policy_state_dict is None: raise RuntimeError(f"æ— æ³•ä» {model_save_path} åŠ è½½æ¨¡å‹ï¼")

#                          # Create new Runner
#                          # åˆ›å»ºæ–°Runner
#                          print("  åˆ›å»ºæ–° Runner...")
#                          new_train_cfg.runner.resume = (old_policy_state_dict is not None)  # åªåœ¨æœ‰æ¨¡å‹æ—¶æ¢å¤
#                          args.resume = (old_policy_state_dict is not None)
#                          args.checkpoint = model_save_path  # å³ä½¿ä¸ºNoneä¹Ÿæ²¡å…³ç³»ï¼Œrunnerä¼šå¤„ç†

#                          runner, train_cfg = task_registry.make_alg_runner(env=env, name=args.task, args=args,
#                                                                            train_cfg=new_train_cfg)

#                          # æ‰§è¡Œæ¨¡å‹è¿ç§»æˆ–åˆå§‹åŒ–
#                          if old_policy_state_dict is not None:
#                              target_policy = runner.alg.actor_critic
#                              old_env_cfg_for_dims = DotDict({'env': DotDict(old_env_dims)})
#                              current_env_cfg_for_dims = env_cfg

#                              print("  æ‰§è¡Œæ¨¡å‹çŠ¶æ€è¿ç§»/åŠ è½½...")
#                              # åœ¨train_curriculum.pyä¸­ï¼Œä¿®æ”¹æ¨¡å‹ä¿å­˜é€»è¾‘ï¼ˆçº¦ç¬¬687è¡Œï¼‰
#                              try:
#                                  # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
#                                  os.makedirs(runner.log_dir, exist_ok=True)

#                                  model_save_path = runner.save(
#                                      os.path.join(runner.log_dir, f'model_{current_iter + 1}_pre_transition.pt'))
#                                  if model_save_path is None or not os.path.exists(model_save_path):
#                                      # æ·»åŠ æ˜ç¡®çš„ä¿å­˜è·¯å¾„
#                                      model_save_path = os.path.join(runner.log_dir,
#                                                                     f'model_{current_iter + 1}_pre_transition.pt')
#                                      print(f"è­¦å‘Š: runner.saveè¿”å›Noneï¼Œä½¿ç”¨æ˜¾å¼è·¯å¾„: {model_save_path}")

#                                      # å†æ¬¡ç¡®ä¿ç›®å½•å­˜åœ¨
#                                      os.makedirs(os.path.dirname(model_save_path), exist_ok=True)

#                                      # æ‰‹åŠ¨ä¿å­˜æ¨¡å‹
#                                      if hasattr(runner, 'alg') and hasattr(runner.alg, 'actor_critic'):
#                                          state_dict = {
#                                              'model_state_dict': runner.alg.actor_critic.state_dict(),
#                                              'optimizer_state_dict': runner.alg.optimizer.state_dict() if hasattr(
#                                                  runner.alg, 'optimizer') else None,
#                                              'iter': current_iter,
#                                              'env_dims': {
#                                                  'num_observations': env.num_observations,
#                                                  'num_privileged_obs': env.num_privileged_obs if hasattr(env,
#                                                                                                          'num_privileged_obs') else None,
#                                                  'num_actions': env.num_actions,
#                                              },
#                                              'stage': stage,
#                                              'total_steps': total_env_steps
#                                          }
#                                          # æ‰“å°è·¯å¾„ä¿¡æ¯è¿›è¡Œè°ƒè¯•
#                                          print(f"ä¿å­˜æ¨¡å‹åˆ°: {model_save_path}")
#                                          print(f"ç›®å½•å­˜åœ¨?: {os.path.exists(os.path.dirname(model_save_path))}")

#                                          torch.save(state_dict, model_save_path)
#                                          print(f"  âœ… æ‰‹åŠ¨ä¿å­˜æ¨¡å‹åˆ°: {model_save_path}")
#                                  curriculum_mgr.set_latest_model_path(model_save_path)
#                                  print(f"  âœ… æ¨¡å‹å·²ä¿å­˜: {model_save_path}")
#                              except Exception as e:
#                                  print(f"  âŒ è¯¾ç¨‹æ¨è¿›å‰ä¿å­˜æ¨¡å‹å¤±è´¥: {str(e)}")
#                                  import traceback
#                                  traceback.print_exc()
#                                  # æŸ¥æ‰¾æ›¿ä»£æ¨¡å‹...
#                          else:
#                              print("  æ²¡æœ‰æ—§æ¨¡å‹å¯åŠ è½½ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–ç­–ç•¥")

#                          # è®¾ç½®è®¡æ•°å™¨
#                          runner.global_step = total_env_steps
#                          runner.current_learning_iteration = current_iter + 1
#                          print(f"    Runner çŠ¶æ€å·²æ›´æ–°/æ¢å¤ã€‚")

#                          print(f"âœ… é˜¶æ®µåˆ‡æ¢æˆåŠŸå®Œæˆï¼")

#                     except Exception as e:
#                          print(f"âŒâŒâŒ é˜¶æ®µåˆ‡æ¢å¤±è´¥: {str(e)} âŒâŒâŒ")
#                          if args.debug: import traceback; traceback.print_exc()
#                          print("æ— æ³•å®‰å…¨æ¢å¤ï¼Œåœæ­¢è®­ç»ƒã€‚")
#                          raise e # Re-raise

#                     # Update stage tracking vars
#                     stage = new_stage
#                     sub_stage = new_sub_stage

#                     # åœ¨å¼ºåˆ¶é˜¶æ®µè½¬æ¢éƒ¨åˆ†ï¼ˆçº¦ç¬¬700è¡Œï¼‰
#                     print("\n--- è¯¦ç»†è°ƒè¯•ä¿¡æ¯: é˜¶æ®µè½¬æ¢ ---")
#                     print(f"å½“å‰ç›®å½•: {os.getcwd()}")
#                     print(f"Runneræ—¥å¿—ç›®å½•: {runner.log_dir}")
#                     print(f"ç›®å½•æ˜¯å¦å­˜åœ¨: {os.path.exists(runner.log_dir)}")
#                     print(
#                         f"ç¯å¢ƒç»Ÿè®¡: num_envs={env.num_envs}, num_observations={env.num_observations}, num_actions={env.num_actions}")
#                     if hasattr(env, 'num_privileged_obs'):
#                         print(f"ç‰¹æƒè§‚å¯Ÿç»´åº¦: {env.num_privileged_obs}")
#                     print(f"é˜¶æ®µä¿¡æ¯: å½“å‰={stage}.{sub_stage}, ç›®æ ‡={target_stage}.{target_sub_stage}")
#                     print("--- è¯¦ç»†è°ƒè¯•ç»“æŸ ---\n")

#             # --- End of Advancement Check ---

#         # --- End of While Loop ---

#         except KeyboardInterrupt: print("\nğŸ›‘ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
#         # except Exception as train_loop_err: print(f"\nâŒâŒâŒ è®­ç»ƒå¾ªç¯é”™è¯¯: {train_loop_err}")

#         except Exception as train_loop_err:
#             import traceback  # å¯¼å…¥ traceback æ¨¡å—
#             print(f"\nâŒâŒâŒ è®­ç»ƒå¾ªç¯é”™è¯¯: {train_loop_err}")
#             print("--- å®Œæ•´é”™è¯¯å †æ ˆ (Traceback) ---")
#             traceback.print_exc()  # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
#             print("-------------------------------")
#             # å¯ä»¥é€‰æ‹©åœ¨è¿™é‡Œé€€å‡ºç¨‹åºï¼Œä»¥ä¾¿æ¸…æ™°åœ°çœ‹åˆ°é”™è¯¯
#             import sys
#             sys.exit(1)


#     finally:
#         # --- 6. æ”¶å°¾å·¥ä½œ ---
#         print("\n--- 6. è®­ç»ƒç»“æŸï¼Œæ‰§è¡Œæ”¶å°¾ ---")
#         current_steps = getattr(runner, 'global_step', total_env_steps)
#         try:
#             if 'curriculum_mgr' in locals() and curriculum_mgr: curriculum_mgr.save_curriculum_state(current_steps); print(f"  ğŸ’¾ æœ€ç»ˆè¯¾ç¨‹çŠ¶æ€å·²ä¿å­˜ã€‚")
#             if 'runner' in locals() and runner: final_iter = getattr(runner, 'current_learning_iteration', 'final'); final_model_path = os.path.join(runner.log_dir, f'model_{final_iter}.pt'); runner.save(final_model_path); print(f"  ğŸ’¾ æœ€ç»ˆæ¨¡å‹ä¿å­˜è·¯å¾„: {final_model_path}")
#             if env is not None: env.close() # Close viewer if open
#         except Exception as e: print(f"  âŒ ä¿å­˜æœ€ç»ˆçŠ¶æ€å¤±è´¥: {str(e)}")
#         # --- !!! é”€æ¯ Sim !!! ---
#         if sim is not None and gym is not None:
#             gym.destroy_sim(sim)
#             print("  âœ… Simulation å·²é”€æ¯ã€‚")
#         print("\nğŸ è®­ç»ƒæµç¨‹ç»“æŸ ğŸ")


# # ==============================================================================
# if __name__ == "__main__":
#     args = parse_args()
#     if args.seed is None: args.seed = int(time.time() * 1000) % 2**32
#     set_seed(args.seed)
#     print(f"ğŸ² ä½¿ç”¨éšæœºç§å­: {args.seed}")
#     train_curriculum(args)


# import traceback
# import argparse
# import numpy as np
# from datetime import datetime, timedelta
# import copy
# import inspect
# import os
# import time
# import yaml # For loading curriculum state

# # å¯¼å…¥IsaacGym
# import isaacgym
# from isaacgym import gymapi, gymutil # Import necessary gym modules
# import torch

# # å¯¼å…¥ G1 ç›¸å…³
# import g1.envs # Keep this for task registration trigger
# from g1.envs.curriculum.curriculum_manager import CurriculumManager
# from g1.envs.curriculum.model_transfer import ModelTransfer
# from g1.envs.curriculum.reward_scheduler import RewardScheduler
# from g1.envs.configs.curriculum.curriculum_manager_config import CurriculumManagerConfig # å¯¼å…¥é…ç½®ç±»

# # å¯¼å…¥å·¥å…·å‡½æ•°å’Œæ³¨å†Œè¡¨
# from g1.utils import get_args, task_registry, set_seed
# # !!! ç¡®ä¿ä» helpers å¯¼å…¥ DotDict !!!
# from g1.utils.helpers import update_cfg_from_args, class_to_dict, parse_sim_params, DotDict

# # --- è§£æå‚æ•° ---
# # (ä¿æŒ parse_args å‡½æ•°ä¸å˜)
# def parse_args():
#     """è§£æå‘½ä»¤è¡Œå‚æ•°"""
#     args = get_args()
#     parser = argparse.ArgumentParser(description='Train with curriculum', add_help=False)
#     # Add args only if not already present from get_args
#     if not hasattr(args, 'config_class'):
#         parser.add_argument('--config_class', type=str, default='CurriculumManagerConfig', help='è¯¾ç¨‹å­¦ä¹ é…ç½®ç±»åç§°')
#     if not hasattr(args, 'resume_curriculum'):
#         parser.add_argument('--resume_curriculum', type=str, default=None, help='æ¢å¤è®­ç»ƒçš„è¯¾ç¨‹çŠ¶æ€æ–‡ä»¶ (YAML)')
#     if not hasattr(args, 'debug'):
#         parser.add_argument('--debug', action='store_true', default=False, help='å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ‰“å°æ›´å¤šä¿¡æ¯')
#     # æ·»åŠ å¼ºåˆ¶é˜¶æ®µå‚æ•°
#     if not hasattr(args, 'force_stage'):
#          parser.add_argument('--force_stage', type=str, default=None, help='å¼ºåˆ¶è®¾ç½®åˆå§‹é˜¶æ®µ (ä¾‹å¦‚ "1.3" æˆ– "2.1")ï¼Œä¼šè¦†ç›–æ¢å¤çŠ¶æ€ä¸­çš„é˜¶æ®µ')

#     curriculum_args, _ = parser.parse_known_args()
#     for key, value in vars(curriculum_args).items():
#         setattr(args, key, value)

#     if args.headless is None: args.headless = True # Default headless for training
#     return args


# # --- åŠ è½½è¯¾ç¨‹é…ç½® ---
# # !!! ä¿®æ”¹ load_curriculum_config ä»¥åˆ›å»ºæ­£ç¡®çš„åµŒå¥—ç»“æ„ !!!
# def load_curriculum_config(config_class_name):
#     """åŠ è½½è¯¾ç¨‹å­¦ä¹ é…ç½®ç±»å®ä¾‹ï¼Œå¹¶è½¬æ¢ä¸ºåŒ…å« 'curriculum' é”®çš„åµŒå¥— DotDict"""
#     try:
#         if config_class_name == 'CurriculumManagerConfig':
#             config_obj = CurriculumManagerConfig()
#             # æå–éœ€è¦æ”¾åœ¨ 'curriculum' ä¸‹çš„å±æ€§
#             curriculum_attrs = {}
#             # æå–éœ€è¦æ”¾åœ¨é¡¶å±‚çš„å±æ€§
#             top_level_attrs = {}

#             for key, value in inspect.getmembers(config_obj):
#                  if not key.startswith('_') and not inspect.ismethod(value):
#                       # åˆ¤æ–­å±æ€§åº”è¯¥æ”¾åœ¨å“ªé‡Œ
#                       if key.startswith('stage') or key in [
#                           'initial_stage', 'initial_sub_stage',
#                           'max_stages', 'max_sub_stages_per_stage', 'max_sub_stages',
#                           'success_threshold', 'evaluation_window',
#                           'min_steps_between_eval', 'model_transfer'
#                       ]:
#                            curriculum_attrs[key] = copy.deepcopy(value)
#                       elif key in ['output', 'max_env_steps']: # å…¶ä»–é¡¶å±‚é…ç½®
#                            top_level_attrs[key] = copy.deepcopy(value)
#                       # else: # å¯ä»¥é€‰æ‹©å¿½ç•¥å…¶ä»–æœªåˆ†ç±»å±æ€§æˆ–æŠ¥é”™

#             # åˆ›å»ºæœ€ç»ˆçš„åµŒå¥— DotDict
#             nested_config = DotDict({
#                 'curriculum': DotDict(curriculum_attrs), # å°†è¯¾ç¨‹ç›¸å…³å±æ€§æ”¾å…¥ 'curriculum'
#                 **top_level_attrs # å°†å…¶ä»–é¡¶å±‚å±æ€§è§£åŒ…åˆå¹¶
#             })

#             print(f"âœ… æˆåŠŸåŠ è½½å¹¶è½¬æ¢è¯¾ç¨‹å­¦ä¹ é…ç½®: {config_class_name}")
#             # print("--- DEBUG: Final curriculum_config structure ---")
#             # import json
#             # print(json.dumps(nested_config, indent=2)) # æ‰“å°æœ€ç»ˆç»“æ„ç”¨äºè°ƒè¯•
#             # print("--- END DEBUG ---")
#             return nested_config
#         else:
#             raise ValueError(f"æœªçŸ¥çš„é…ç½®ç±»åç§°: {config_class_name}")
#     except Exception as e:
#         print(f"âŒ æ— æ³•åŠ è½½è¯¾ç¨‹å­¦ä¹ é…ç½® {config_class_name}: {str(e)}")
#         if args.debug: import traceback; traceback.print_exc()
#         return None

# # --- è·å–ä»»åŠ¡ä¿¡æ¯ ---
# # !!! ä¿®æ”¹ get_task_info ä»¥é€‚åº”æ–°çš„é…ç½®ç»“æ„ !!!
# def get_task_info(curriculum_config, stage):
#     """æ ¹æ®è¯¾ç¨‹é˜¶æ®µè·å–ä»»åŠ¡åç§°å’Œè¯¥é˜¶æ®µçš„ç‰¹å®šå‚æ•°"""
#     stage_key = f'stage{stage}'
#     # ç°åœ¨ä» curriculum_config.curriculum ä¸­è·å–é˜¶æ®µé…ç½®
#     if not hasattr(curriculum_config, 'curriculum'):
#          raise AttributeError("curriculum_config is missing the 'curriculum' attribute.")

#     stage_cfg_dotdict = curriculum_config.curriculum.get(stage_key)
#     if stage_cfg_dotdict is None:
#         # å°è¯•ä»é¡¶å±‚è·å–ï¼ˆä¸ºäº†å…¼å®¹æ—§æ ¼å¼ï¼Œä½†åº”è¯¥é¿å…ï¼‰
#         stage_cfg_dotdict = curriculum_config.get(stage_key)
#         if stage_cfg_dotdict is None:
#              raise ValueError(f"æœªæ‰¾åˆ°é˜¶æ®µ {stage} çš„é…ç½® (åœ¨ curriculum_config.curriculum ä¸‹æŸ¥æ‰¾ Key: '{stage_key}')")
#         else:
#              print(f"âš ï¸ Warning: Found stage config for {stage} at top level, expected under 'curriculum'.")


#     # stage_cfg_dotdict åº”è¯¥æ˜¯ DotDict æˆ–æ™®é€š dict
#     task_name = stage_cfg_dotdict.get('env_class')
#     if not task_name:
#         raise ValueError(f"é˜¶æ®µ {stage} é…ç½®ä¸­æœªæŒ‡å®š 'env_class'")

#     # è¿”å›æ•´ä¸ªé˜¶æ®µé…ç½®å­—å…¸ (å·²ç»æ˜¯ DotDict æˆ– dict)
#     return {'task_name': task_name, 'stage_params': dict(stage_cfg_dotdict)}

# # --- éªŒè¯ä»»åŠ¡å…¼å®¹æ€§ ---
# # (ä¿æŒ validate_task_compatibility å‡½æ•°ä¸å˜)
# def validate_task_compatibility(task_name):
#     """éªŒè¯ä»»åŠ¡æ˜¯å¦å·²åœ¨ task_registry ä¸­æ³¨å†Œ"""
#     if hasattr(task_registry, 'task_classes') and isinstance(task_registry.task_classes, dict):
#         available_tasks = list(task_registry.task_classes.keys())
#         if task_name not in available_tasks:
#             print(f"âŒ ä»»åŠ¡ '{task_name}' æœªåœ¨ task_registry ä¸­æ³¨å†Œ!")
#             print(f"   å¯ç”¨ä»»åŠ¡: {available_tasks}")
#             return False
#         return True
#     else:
#         print("âŒ é”™è¯¯: æ— æ³•è®¿é—® task_registry.task_classes æ¥éªŒè¯ä»»åŠ¡å…¼å®¹æ€§ã€‚")
#         return False

# # --- è·å–é˜¶æ®µé¢„æœŸç»´åº¦ ---
# # (ä¿æŒ get_stage_expected_dims å‡½æ•°ä¸å˜)
# def get_stage_expected_dims(task_registry, curriculum_config, stage, sub_stage):
#     """è·å–æŒ‡å®šé˜¶æ®µå’Œå­é˜¶æ®µçš„é¢„æœŸè§‚æµ‹å’ŒåŠ¨ä½œç»´åº¦"""
#     try:
#         task_info = get_task_info(curriculum_config, stage)
#         task_name = task_info['task_name']
#         env_cfg_cls = task_registry.env_cfgs[task_name]
#         env_cfg = env_cfg_cls() # åˆ›å»ºä¸´æ—¶å®ä¾‹

#         # æ£€æŸ¥æ˜¯å¦ä¸º Stage 1 ä¸”æœ‰åµŒå¥—é…ç½®
#         if stage == 1 and hasattr(env_cfg, 'nested_locomotion_curriculum') and env_cfg.nested_locomotion_curriculum:
#              if hasattr(env_cfg, 'sub_stage_params') and sub_stage in env_cfg.sub_stage_params:
#                   sub_params = env_cfg.sub_stage_params[sub_stage]
#                   obs_dim = sub_params.get('num_observations')
#                   act_dim = sub_params.get('num_actions')
#                   priv_obs_dim = sub_params.get('num_privileged_obs')
#                   if obs_dim is None or act_dim is None:
#                        print(f"âš ï¸ get_stage_expected_dims: Stage 1.{sub_stage} é…ç½®ç¼ºå°‘ç»´åº¦ä¿¡æ¯ï¼Œä½¿ç”¨ä¸»é…ç½®é»˜è®¤å€¼ã€‚")
#                        obs_dim = env_cfg.env.num_observations
#                        act_dim = env_cfg.env.num_actions
#                        priv_obs_dim = getattr(env_cfg.env, 'num_privileged_obs', None) # Use getattr for safety
#                   # å¤„ç†ç‰¹æƒè§‚æµ‹æœªå®šä¹‰çš„æƒ…å†µ
#                   if priv_obs_dim is None and obs_dim is not None:
#                        priv_obs_dim = obs_dim + 3 # å‡è®¾åŸºç¡€ç‰¹æƒè§‚æµ‹=æ™®é€šè§‚æµ‹+3
#                   return obs_dim, act_dim, priv_obs_dim
#              else:
#                   print(f"âš ï¸ get_stage_expected_dims: æ— æ³•æ‰¾åˆ° Stage 1.{sub_stage} çš„å‚æ•°ï¼Œä½¿ç”¨ä¸»é…ç½®é»˜è®¤å€¼ã€‚")
#                   # Fallback to main config dimensions if sub-stage config missing
#                   return env_cfg.env.num_observations, env_cfg.env.num_actions, getattr(env_cfg.env, 'num_privileged_obs', None)
#         else:
#             # å¯¹äºé Stage 1 æˆ–æ— åµŒå¥—è¯¾ç¨‹çš„æƒ…å†µï¼Œç›´æ¥ä½¿ç”¨ä¸»é…ç½®çš„ç»´åº¦
#             return env_cfg.env.num_observations, env_cfg.env.num_actions, getattr(env_cfg.env, 'num_privileged_obs', None)

#     except Exception as e:
#         print(f"âŒ è·å–é˜¶æ®µ {stage}.{sub_stage} é¢„æœŸç»´åº¦å¤±è´¥: {e}")
#         return None, None, None # Return None on error

# # --- æ›´æ–°ç¯å¢ƒå¥–åŠ± ---
# # (ä¿æŒ update_env_rewards å‡½æ•°ä¸å˜)
# def update_env_rewards(env, reward_scheduler, stage, sub_stage):
#     """æ ¹æ®è¯¾ç¨‹é˜¶æ®µä½¿ç”¨ RewardScheduler æ›´æ–°ç¯å¢ƒå®ä¾‹çš„å¥–åŠ±ç³»æ•°"""
#     print(f"--- Updating reward scales for Stage {stage}.{sub_stage} ---")
#     if not hasattr(env, 'cfg') or not hasattr(env.cfg, 'rewards') or not hasattr(env.cfg.rewards, 'scales'):
#         print("  âš ï¸ Cannot update rewards: env.cfg.rewards.scales structure missing.")
#         return False
#     try:
#         reward_scales_dict = reward_scheduler.get_reward_scales(stage, sub_stage)
#         scales_target = env.cfg.rewards.scales # Should be an object or dict
#         updated_count = 0
#         applied_scales = {}

#         # Get available scale names from the target object/dict
#         if isinstance(scales_target, dict): defined_in_env = list(scales_target.keys())
#         else: defined_in_env = [attr for attr in dir(scales_target) if not attr.startswith('_') and not callable(getattr(scales_target, attr))]

#         for reward_name, scale_value in reward_scales_dict.items():
#             if reward_name in defined_in_env:
#                 try:
#                     if isinstance(scales_target, dict): scales_target[reward_name] = scale_value
#                     else: setattr(scales_target, reward_name, scale_value)
#                     applied_scales[reward_name] = f"{scale_value:.4f}" # Format for printing
#                     updated_count += 1
#                 except Exception as e: print(f"  âš ï¸ Error updating scale '{reward_name}': {e}")
#             # else: print(f"  - Scale '{reward_name}' not found in env config scales.") # Optional warning

#         if updated_count > 0:
#             print(f"  âœ… Updated {updated_count} reward scales. Applied: {applied_scales}")
#             # Re-prepare reward functions to use new scales
#             if hasattr(env, '_prepare_reward_function'):
#                  print("    - Calling env._prepare_reward_function()")
#                  env._prepare_reward_function()
#             return True
#         else:
#             print(f"  âš ï¸ No matching reward scales found to update for Stage {stage}.{sub_stage}.")
#             print(f"    - Available scales in env config: {defined_in_env}")
#             print(f"    - Scales provided by scheduler: {list(reward_scales_dict.keys())}")
#             return False
#     except Exception as e:
#         print(f"âŒ Error during reward update: {str(e)}")
#         if args.debug: import traceback; traceback.print_exc()
#         return False


# # ==============================================================================
# # ä¸»è®­ç»ƒå‡½æ•°
# # ==============================================================================
# def train_curriculum(args):
#     """è¯¾ç¨‹å­¦ä¹ è®­ç»ƒä¸»å‡½æ•°"""
#     print("="*50); print("ğŸš€ å¼€å§‹ G1 è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ (æ”¯æŒåµŒå¥—) ğŸš€"); print("="*50)

#     # --- å…¨å±€ Gym å’Œ Sim ---
#     gym = None
#     sim = None
#     env = None # Initialize env to None
#     runner = None # Initialize runner to None
#     sim_params = None # Initialize sim_params
#     # !!! åˆå§‹åŒ– total_env_steps !!!
#     total_env_steps = 0

#     try:
#         # --- 1. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ ---
#         print("\n--- 1. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ ---")
#         curriculum_config = load_curriculum_config(args.config_class)
#         if curriculum_config is None: return

#         # !!! ç¡®ä¿ curriculum_config æœ‰ 'curriculum' å±æ€§å†ä¼ ç»™ CurriculumManager !!!
#         if not hasattr(curriculum_config, 'curriculum'):
#              print("âŒ CRITICAL: Loaded curriculum_config does not have 'curriculum' attribute!")
#              # print("Loaded config:", curriculum_config) # Debug print
#              return
#         curriculum_mgr = CurriculumManager(curriculum_config) # ç°åœ¨ curriculum_config.curriculum å­˜åœ¨
#         print(f"âœ… è¯¾ç¨‹ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ (Output: {curriculum_mgr.output_dir})")

#         device_arg = getattr(args, 'rl_device', 'cuda:0')
#         # !!! ä» curriculum_config.curriculum è·å– model_transfer !!!
#         if not hasattr(curriculum_config.curriculum, 'model_transfer'):
#             print("âŒ CRITICAL: curriculum_config.curriculum does not have 'model_transfer' attribute!")
#             return
#         model_transfer_cfg = curriculum_config.curriculum.model_transfer
#         if not isinstance(model_transfer_cfg, DotDict): model_transfer_cfg = DotDict(model_transfer_cfg)
#         model_transfer_cfg.device = device_arg
#         model_transfer = ModelTransfer(model_transfer_cfg)
#         print(f"âœ… æ¨¡å‹è¿ç§»å·¥å…·åˆ›å»ºæˆåŠŸ")

#         # !!! ä¼ é€’é¡¶å±‚ config ç»™ RewardScheduler (å¦‚æœå®ƒéœ€è¦è®¿é—®éè¯¾ç¨‹éƒ¨åˆ†) !!!
#         # reward_scheduler = RewardScheduler(curriculum_config.curriculum) # åªä¼ è¯¾ç¨‹éƒ¨åˆ†?
#         reward_scheduler = RewardScheduler(curriculum_config) # ä¼ é€’å®Œæ•´çš„ï¼Œè®©å®ƒè‡ªå·±è§£æ
#         print(f"âœ… å¥–åŠ±è°ƒåº¦å™¨åˆ›å»ºæˆåŠŸ")

#         # --- åˆå§‹åŒ– Gym ---
#         print("\n--- åˆå§‹åŒ– Isaac Gym ---")
#         gym = gymapi.acquire_gym()

#         # --- 2. æ¢å¤çŠ¶æ€ æˆ– å¤„ç†å¼ºåˆ¶é˜¶æ®µ ---
#         # (ä¿æŒæ¢å¤/å¼ºåˆ¶é˜¶æ®µé€»è¾‘ä¸å˜)
#         print("\n--- 2. æ¢å¤çŠ¶æ€ / å¼ºåˆ¶é˜¶æ®µæ£€æŸ¥ ---")
#         loaded_model_path = None
#         initial_stage_override = None
#         initial_sub_stage_override = None

#         if args.force_stage:
#              try:
#                   forced_stage_str = args.force_stage.split('.')
#                   initial_stage_override = int(forced_stage_str[0])
#                   initial_sub_stage_override = int(forced_stage_str[1]) if len(forced_stage_str) > 1 else 1
#                   print(f"âš¡ï¸ å¼ºåˆ¶è®¾ç½®åˆå§‹é˜¶æ®µä¸º: {initial_stage_override}.{initial_sub_stage_override}")
#                   args.resume_curriculum = None
#              except ValueError:
#                   print(f"âŒ æ— æ•ˆçš„ --force_stage å‚æ•°æ ¼å¼: '{args.force_stage}'. åº”ä¸º 'stage.sub_stage'.")
#                   return

#         if args.resume_curriculum:
#             print(f"å°è¯•ä»è¯¾ç¨‹çŠ¶æ€æ–‡ä»¶æ¢å¤: {args.resume_curriculum}")
#             success, loaded_model_path_from_state = curriculum_mgr.load_curriculum_state(args.resume_curriculum)
#             if success:
#                 print(f"âœ… å·²æ¢å¤è¯¾ç¨‹çŠ¶æ€ã€‚å½“å‰é˜¶æ®µ: {curriculum_mgr.current_stage}.{curriculum_mgr.current_sub_stage}")
#                 if loaded_model_path_from_state:
#                      print(f"  - ä»è¯¾ç¨‹çŠ¶æ€è·å–æ¨¡å‹è·¯å¾„: {loaded_model_path_from_state}")
#                      if args.checkpoint is None:
#                           args.checkpoint = loaded_model_path_from_state
#                           args.resume = True
#                      else:
#                           print(f"  - å‘½ä»¤è¡ŒæŒ‡å®šäº† --checkpoint {args.checkpoint}, ä¼˜å…ˆä½¿ç”¨ã€‚")
#                           loaded_model_path = args.checkpoint
#                           args.resume = True
#                 else: print("  - è¯¾ç¨‹çŠ¶æ€ä¸­æœªæ‰¾åˆ°æ¨¡å‹è·¯å¾„ã€‚")
#             else:
#                 print(f"âŒ æ— æ³•æ¢å¤è¯¾ç¨‹çŠ¶æ€ï¼Œå°†ä»å¤´å¼€å§‹ã€‚")
#                 args.resume_curriculum = None; args.checkpoint = None; args.resume = False
#         elif args.checkpoint:
#              print(f"ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„æ£€æŸ¥ç‚¹: {args.checkpoint}")
#              loaded_model_path = args.checkpoint
#              args.resume = True

#         if initial_stage_override is not None:
#              curriculum_mgr.current_stage = initial_stage_override
#              curriculum_mgr.current_sub_stage = initial_sub_stage_override
#              curriculum_mgr.reset_evaluation()
#              print(f"  åº”ç”¨å¼ºåˆ¶é˜¶æ®µè¦†ç›–ï¼Œå½“å‰é˜¶æ®µè®¾ä¸º: {curriculum_mgr.current_stage}.{curriculum_mgr.current_sub_stage}")


#         # --- 3. è®¾ç½®åˆå§‹é˜¶æ®µã€ä»»åŠ¡å’Œ Sim ---
#         print("\n--- 3. è®¾ç½®åˆå§‹é˜¶æ®µã€ä»»åŠ¡å’Œ Simulation ---")
#         stage, sub_stage = curriculum_mgr.get_current_stage_info()
#         print(f"æœ€ç»ˆåˆå§‹é˜¶æ®µ: {stage}.{sub_stage}")

#         task_name = None; stage_params = None
#         env_cfg = None; train_cfg = None
#         try:
#             # !!! ä½¿ç”¨æ­£ç¡®çš„ curriculum_config ç»“æ„ !!!
#             task_info = get_task_info(curriculum_config, stage)
#             task_name = task_info.get('task_name')
#             stage_params = task_info.get('stage_params')
#             if not task_name: raise ValueError("è·å–çš„ task_name ä¸ºç©º")
#             print(f"é˜¶æ®µ {stage} ä»»åŠ¡ä¿¡æ¯: Task='{task_name}', Params Keys={list(stage_params.keys()) if stage_params else 'N/A'}")

#             if not validate_task_compatibility(task_name): return
#             print(f"âœ… ä»»åŠ¡ '{task_name}' éªŒè¯é€šè¿‡ã€‚")

#             env_cfg, train_cfg = task_registry.get_cfgs(task_name)
#             print(f"  åŠ è½½ä»»åŠ¡ '{task_name}' çš„åŸºç¡€ env_cfg (ç±»å‹: {type(env_cfg)}) å’Œ train_cfg (ç±»å‹: {type(train_cfg)})")

#             # --- åˆå¹¶é˜¶æ®µå‚æ•°å¹¶è®¾ç½® Sim ---
#             if not hasattr(env_cfg, 'curriculum') or not isinstance(env_cfg.curriculum, DotDict): env_cfg.curriculum = DotDict()
#             env_cfg.curriculum.stage = stage
#             env_cfg.curriculum.sub_stage = sub_stage
#             # !!! ä½¿ç”¨æ­£ç¡®çš„ curriculum_config ç»“æ„ !!!
#             setattr(env_cfg.curriculum, f'stage{stage}_params', DotDict(stage_params))

#             expected_obs, expected_act, expected_priv_obs = get_stage_expected_dims(task_registry, curriculum_config, stage, sub_stage)
#             if expected_obs is None or expected_act is None: raise ValueError("æ— æ³•ç¡®å®šåˆå§‹é˜¶æ®µç»´åº¦")
#             env_cfg.env.num_observations = expected_obs
#             env_cfg.env.num_actions = expected_act
#             if expected_priv_obs is not None: env_cfg.env.num_privileged_obs = expected_priv_obs
#             print(f"  è®¾ç½®åˆå§‹ env_cfg ç»´åº¦: Obs={env_cfg.env.num_observations}, Act={env_cfg.env.num_actions}, PrivObs={getattr(env_cfg.env, 'num_privileged_obs', 'N/A')}")


#             args.task = task_name
#             stage_num_envs = stage_params.get('num_envs')
#             if args.num_envs is None:
#                  args.num_envs = stage_num_envs if stage_num_envs is not None else env_cfg.env.num_envs
#             env_cfg.env.num_envs = args.num_envs
#             print(f"  å°†ä½¿ç”¨çš„ç¯å¢ƒæ•°é‡: {args.num_envs}")

#             print("  åˆ›å»º Simulation...")
#             sim_params_dict = {"sim": class_to_dict(env_cfg.sim)}
#             sim_params = parse_sim_params(args, sim_params_dict)
#             physics_engine = gymapi.SIM_PHYSX
#             sim_device_type, sim_device_id = gymutil.parse_device_str(args.sim_device)
#             graphics_device_id = sim_device_id if not args.headless else -1
#             sim = gym.create_sim(sim_device_id, graphics_device_id, physics_engine, sim_params)
#             if sim is None: raise RuntimeError("Failed to create sim!")
#             print(f"âœ… Simulation åˆ›å»ºæˆåŠŸ (Sim Handle: {sim})")

#         except Exception as e:
#             print(f"âŒ è·å–æˆ–è®¾ç½®åˆå§‹é˜¶æ®µ/ä»»åŠ¡/Sim å¤±è´¥: {str(e)}")
#             if args.debug: import traceback; traceback.print_exc();
#             if sim: gym.destroy_sim(sim); gym = None
#             return


#         # --- 4. åˆ›å»ºåˆå§‹ç¯å¢ƒå’Œ Runner ---
#         # (ä¿æŒè¿™éƒ¨åˆ†é€»è¾‘ä¸å˜ï¼Œé™¤äº† train_cfg.runner.resume çš„è®¾ç½®)
#         print("\n--- 4. åˆ›å»ºåˆå§‹ç¯å¢ƒå’Œ Runner ---")
#         policy_state_dict = None
#         loaded_env_dims = None
#         loaded_steps = 0
#         loaded_stage_tuple = (0, 0)

#         try:
#             print(f"  å‡†å¤‡åˆ›å»ºç¯å¢ƒå®ä¾‹...")
#             env_cfg.curriculum.stage = stage
#             env_cfg.curriculum.sub_stage = sub_stage
#             env, env_cfg = task_registry.make_env(
#                 name=args.task, args=args, env_cfg=env_cfg,
#                 gym_handle=gym, sim_handle=sim, sim_params=sim_params
#             )

#             update_env_rewards(env, reward_scheduler, stage, sub_stage)

#             checkpoint_to_load = args.checkpoint
#             if checkpoint_to_load and os.path.exists(checkpoint_to_load):
#                  print(f"  å‡†å¤‡åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_to_load}")
#                  policy_state_dict, loaded_env_dims, loaded_steps, loaded_stage_tuple = model_transfer.load_checkpoint(checkpoint_to_load)
#                  if policy_state_dict is None:
#                       print("  âŒ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥ï¼Œå°†éšæœºåˆå§‹åŒ–æ¨¡å‹ã€‚")
#                       args.checkpoint = None; args.resume = False; loaded_model_path = None
#                  else:
#                       print(f"  âœ… æ£€æŸ¥ç‚¹æ•°æ®åŠ è½½æˆåŠŸ (æ¥è‡ªé˜¶æ®µ {loaded_stage_tuple[0]}.{loaded_stage_tuple[1]} @ {loaded_steps:,} æ­¥)")
#                       loaded_model_path = checkpoint_to_load
#                       args.resume = True # ç¡®ä¿ resume æ ‡å¿—è¢«è®¾ç½®
#             else:
#                  if checkpoint_to_load: print(f"  âš ï¸ æŒ‡å®šçš„æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_to_load}")
#                  print("  å°†éšæœºåˆå§‹åŒ–æ¨¡å‹ã€‚")
#                  args.checkpoint = None; args.resume = False; loaded_model_path = None

#             print(f"  å‡†å¤‡åˆ›å»º Runner (Task: {args.task})...")
#             # !!! ç¡®ä¿ train_cfg çš„ resume æ ‡å¿—æ­£ç¡® !!!
#             train_cfg.runner.resume = args.resume # ä½¿ç”¨ args ä¸­æœ€ç»ˆç¡®å®šçš„ resume çŠ¶æ€
#             runner, train_cfg = task_registry.make_alg_runner(env=env, name=args.task, args=args, train_cfg=train_cfg)

#             if policy_state_dict:
#                  target_policy = runner.alg.actor_critic
#                  current_env_dims = {
#                       'num_observations': env.num_observations,
#                       'num_privileged_obs': env.num_privileged_obs,
#                       'num_actions': env.num_actions
#                  }
#                  print(f"  å½“å‰ç¯å¢ƒç»´åº¦: Obs={current_env_dims['num_observations']}, Act={current_env_dims['num_actions']}")
#                  print(f"  åŠ è½½çš„æ¨¡å‹ç»´åº¦: Obs={loaded_env_dims['num_observations']}, Act={loaded_env_dims['num_actions']}")

#                  if loaded_env_dims['num_observations'] != current_env_dims['num_observations'] or \
#                     loaded_env_dims['num_actions'] != current_env_dims['num_actions']:
#                      print("  âš ï¸ æ¨¡å‹ç»´åº¦ä¸åŒ¹é…ï¼Œæ‰§è¡Œæ¨¡å‹è¿ç§»...")
#                      old_env_cfg_for_dims = DotDict({'env': DotDict(loaded_env_dims)})
#                      current_env_cfg_for_dims = DotDict({'env': DotDict(current_env_dims)})

#                      model_transfer.transfer_policy(
#                           old_policy_state_dict=policy_state_dict,
#                           old_cfg=old_env_cfg_for_dims,
#                           new_cfg=current_env_cfg_for_dims,
#                           target_policy=target_policy
#                      )
#                  else:
#                       print("  æ£€æŸ¥ç‚¹ç»´åº¦åŒ¹é…ï¼Œç›´æ¥åŠ è½½çŠ¶æ€å­—å…¸...")
#                       try:
#                            missing_keys, unexpected_keys = target_policy.load_state_dict(policy_state_dict, strict=False)
#                            if missing_keys: print(f"    - è­¦å‘Š: åŠ è½½æ—¶ç¼ºå°‘é”®: {missing_keys}")
#                            if unexpected_keys: print(f"    - è­¦å‘Š: åŠ è½½æ—¶å‘ç°æ„å¤–é”®: {unexpected_keys}")
#                            print("    âœ… çŠ¶æ€å­—å…¸åŠ è½½æˆåŠŸã€‚")
#                       except Exception as e:
#                            print(f"    âŒ ç›´æ¥åŠ è½½çŠ¶æ€å­—å…¸å¤±è´¥: {e}ã€‚")

#                  if args.resume: # åªæœ‰åœ¨æ˜ç¡®æ¢å¤æ—¶æ‰è®¾ç½® runner çŠ¶æ€
#                       runner.global_step = loaded_steps
#                       steps_per_iter = getattr(train_cfg.runner, 'num_steps_per_env', 24) * env.num_envs
#                       runner.current_learning_iteration = int(loaded_steps / steps_per_iter) if steps_per_iter > 0 else 0
#                       print(f"    æ¢å¤ Runner çŠ¶æ€: Global Steps={runner.global_step:,}, Approx Iteration={runner.current_learning_iteration}")

#             if loaded_model_path: curriculum_mgr.set_latest_model_path(loaded_model_path)

#         except Exception as e:
#             print(f"âŒ ç¯å¢ƒæˆ–è®­ç»ƒè¿è¡Œå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
#             if args.debug: import traceback; traceback.print_exc();
#             if sim: gym.destroy_sim(sim); gym = None
#             if env: env.close(); env = None
#             return


#         # --- 5. è®­ç»ƒå¾ªç¯ ---
#         print("\n--- 5. å¼€å§‹è®­ç»ƒå¾ªç¯ ---")
#         # !!! total_env_steps ç°åœ¨åœ¨ try å—å¤–éƒ¨åˆå§‹åŒ– !!!
#         total_env_steps = getattr(runner, 'global_step', 0) # è·å–æ¢å¤åçš„æ­¥æ•°
#         max_iterations = getattr(train_cfg.runner, 'max_iterations', 1500)
#         # !!! ä»é¡¶å±‚ curriculum_config è·å– max_env_steps !!!
#         max_env_steps = getattr(curriculum_config, 'max_env_steps', 100_000_000)

#         print(f"æœ€å¤§ç¯å¢ƒæ­¥æ•°: {max_env_steps:,}")
#         print(f"æœ€å¤§è¿­ä»£æ¬¡æ•°: {max_iterations:,}")
#         if total_env_steps > 0: print(f"ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œå½“å‰æ€»ç¯å¢ƒæ­¥æ•°: {total_env_steps:,}")

#         start_time_ts = time.time();
#         last_save_time_ts = start_time_ts;
#         last_log_time_ts = start_time_ts
#         if not hasattr(runner, 'current_learning_iteration'): runner.current_learning_iteration = 0

#         try:
#             while runner.current_learning_iteration < max_iterations and total_env_steps < max_env_steps:
#                 current_iter = runner.current_learning_iteration
#                 iter_start_time_ts = time.time()

#                 # --- 5.1 è¿è¡Œä¸€ä¸ªå­¦ä¹ è¿­ä»£ ---
#                 # (ä¿æŒä¸å˜)
#                 try:
#                     runner.learn(num_learning_iterations=1, init_at_random_ep_len=True)
#                 except RuntimeError as e: # ... (é”™è¯¯å¤„ç†ä¿æŒä¸å˜) ...
#                     if "CUDA out of memory" in str(e): print("\nâŒâŒâŒ CUDA Out of Memory! âŒâŒâŒ");torch.cuda.empty_cache(); raise e
#                     elif "tensor a" in str(e) and "tensor b" in str(e) or "mat1 and mat2 shapes cannot be multiplied" in str(e):
#                          print("\n\n" + "="*50); print(f"âŒ è¿è¡Œæ—¶é”™è¯¯ (å¼ é‡å½¢çŠ¶ä¸åŒ¹é…): {e}"); print("="*50)
#                          if hasattr(runner, 'alg') and hasattr(runner.alg, 'actor_critic'):
#                              actor = runner.alg.actor_critic.actor; print(f"æ¨¡å‹è¾“å…¥å±‚ In: {getattr(actor[0], 'in_features', 'N/A')}, Out: {getattr(actor[0], 'out_features', 'N/A')}")
#                          print(f"ç¯å¢ƒè§‚å¯Ÿç»´åº¦: Actual={env.obs_buf.shape}, Configured={env.num_observations}"); print("="*50)
#                          raise e # Re-raise to stop execution
#                     else: print(f"âŒ è®­ç»ƒè¿­ä»£è¿è¡Œæ—¶é”™è¯¯: {str(e)}"); raise e
#                 except Exception as e: print(f"âŒ è®­ç»ƒè¿­ä»£ä¸­å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {str(e)}"); raise e


#                 # --- 5.2 è·å–ç»Ÿè®¡æ•°æ®å’Œæ›´æ–°æ­¥æ•° ---
#                 # (ä¿æŒä¸å˜)
#                 train_info = runner.get_inference_stats()
#                 if train_info is None: train_info = {}
#                 steps_this_iter = runner.num_steps_per_env * env.num_envs
#                 total_env_steps += steps_this_iter


#                 # --- 5.3 æ—¥å¿—è®°å½• ---
#                 # (ä¿æŒä¸å˜)
#                 iter_time_sec = time.time() - iter_start_time_ts
#                 elapsed_time_sec = time.time() - start_time_ts
#                 elapsed_timedelta = timedelta(seconds=elapsed_time_sec)

#                 if time.time() - last_log_time_ts > 30: # Log every 30 seconds
#                      mean_reward = train_info.get('mean_reward', float('nan'))
#                      mean_ep_length = train_info.get('mean_episode_length', float('nan'))
#                      success_rate = train_info.get('success_rate', 0.0)

#                      log_msg = (f"S{stage}.{sub_stage} | It {current_iter+1:>5}/{max_iterations} | "
#                                 f"Steps {total_env_steps/1e6:>6.1f}M/{max_env_steps/1e6:.1f}M | "
#                                 f"Rew {mean_reward:>6.2f} | Len {mean_ep_length:>5.1f} | "
#                                 f"SR {success_rate:.3f} | iter time {iter_time_sec:.2f}s | total time {str(elapsed_timedelta).split('.')[0]}")
#                      print(log_msg)
#                      last_log_time_ts = time.time()

#                      curriculum_mgr.update_statistics(success_rate, mean_reward, steps_this_iter)


#                 # --- 5.4 ä¿å­˜æ£€æŸ¥ç‚¹ ---
#                 # (ä¿æŒä¸å˜)
#                 save_freq_iters = getattr(train_cfg.runner, 'save_interval', 50)
#                 time_based_save = (time.time() - last_save_time_ts) > 900 # Save every 15 minutes
#                 iter_based_save = (current_iter + 1) % save_freq_iters == 0
#                 if iter_based_save or time_based_save:
#                      print(f"\n--- Saving Checkpoint (Iteration {current_iter+1}) ---")
#                      try:
#                          if runner.log_dir: os.makedirs(runner.log_dir, exist_ok=True)
#                          model_save_path = runner.save(os.path.join(runner.log_dir, f'model_{current_iter+1}.pt'))
#                          if model_save_path:
#                              curriculum_mgr.set_latest_model_path(model_save_path)
#                              curriculum_mgr.save_curriculum_state(total_env_steps)
#                              print(f"âœ… Checkpoint and curriculum state saved successfully.")
#                          else: print("âŒ Runner save returned None.")
#                          last_save_time_ts = time.time()
#                      except Exception as e: print(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹æˆ–è¯¾ç¨‹çŠ¶æ€å¤±è´¥: {str(e)}")



#                 # --- 5.5 æ£€æŸ¥å¹¶æ¨è¿›è¯¾ç¨‹ ---
#                 # (ä¿æŒä¸å˜ - ä½¿ç”¨ä¿®æ­£åçš„åµŒå¥—é€»è¾‘)
#                 should_advance = curriculum_mgr.should_advance_curriculum(total_env_steps)

#                 if should_advance:
#                     print("\n" + "="*20 + " CURRICULUM ADVANCEMENT CHECK " + "="*20)
#                     print(f"æ¡ä»¶æ»¡è¶³ (SR >= Threshold æˆ–å…¶ä»–æ¡ä»¶)ï¼Œå‡†å¤‡ä»é˜¶æ®µ {stage}.{sub_stage} æ¨è¿›...")

#                     # --- 5.5.1 ä¿å­˜å½“å‰æ¨¡å‹ ---
#                     print("  ä¿å­˜å½“å‰æ¨¡å‹...")
#                     model_save_path = None
#                     try:
#                          if runner.log_dir: os.makedirs(runner.log_dir, exist_ok=True)
#                          save_filename = os.path.join(runner.log_dir, f'model_{current_iter+1}_pre_transition.pt')
#                          model_save_path = runner.save(save_filename)
#                          if model_save_path is None or not os.path.exists(model_save_path): model_save_path = save_filename # Fallback path
#                          curriculum_mgr.set_latest_model_path(model_save_path)
#                          print(f"  âœ… æ¨¡å‹å·²ä¿å­˜: {model_save_path}")
#                     except Exception as e:
#                          print(f"  âŒ è¯¾ç¨‹æ¨è¿›å‰ä¿å­˜æ¨¡å‹å¤±è´¥: {str(e)}")
#                          if runner.log_dir and os.path.exists(runner.log_dir):
#                              model_files = [os.path.join(runner.log_dir, f) for f in os.listdir(runner.log_dir) if f.startswith('model_') and f.endswith('.pt')]
#                              if model_files:
#                                  try: model_save_path = max(model_files, key=os.path.getmtime); print(f"  æ‰¾åˆ°å¤‡é€‰æ¨¡å‹: {model_save_path}")
#                                  except: pass

#                     # --- 5.5.2 è·å–æ—§ç¯å¢ƒçš„å®é™…ç»´åº¦ ---
#                     old_obs_dim = env.num_observations
#                     old_act_dim = env.num_actions
#                     old_priv_obs_dim = env.num_privileged_obs
#                     old_env_dims = {'num_observations': old_obs_dim, 'num_privileged_obs': old_priv_obs_dim, 'num_actions': old_act_dim}
#                     old_stage = stage
#                     old_sub_stage = sub_stage

#                     # --- 5.5.3 æ¨è¿›è¯¾ç¨‹ç®¡ç†å™¨ ---
#                     new_stage_str = curriculum_mgr.advance_curriculum(total_env_steps)
#                     new_stage, new_sub_stage = map(int, new_stage_str.split('.'))
#                     print(f"ğŸ“ è¯¾ç¨‹ç®¡ç†å™¨å·²æ¨è¿›åˆ°æ–°é˜¶æ®µ: {new_stage}.{new_sub_stage}")

#                     # --- 5.5.4 æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º ---
#                     trigger_recreation = False
#                     new_expected_obs_dim, new_expected_act_dim, new_expected_priv_obs_dim = get_stage_expected_dims(task_registry, curriculum_config, new_stage, new_sub_stage)

#                     if new_expected_obs_dim is None or new_expected_act_dim is None:
#                         print("âŒ æ— æ³•è·å–æ–°é˜¶æ®µçš„é¢„æœŸç»´åº¦ï¼Œåœæ­¢æ¨è¿›ï¼")
#                         continue

#                     if new_stage != old_stage:
#                         trigger_recreation = True
#                         print(f"  ä¸»é˜¶æ®µå˜åŒ– ({old_stage} -> {new_stage})ï¼Œè§¦å‘ç¯å¢ƒ/Runneré‡å»ºã€‚")
#                     elif new_stage == 1 and (new_expected_obs_dim != old_obs_dim or new_expected_act_dim != old_act_dim):
#                         trigger_recreation = True
#                         print(f"  Stage 1 å­é˜¶æ®µç»´åº¦å˜åŒ– (Obs: {old_obs_dim}->{new_expected_obs_dim}, Act: {old_act_dim}->{new_expected_act_dim})ï¼Œè§¦å‘ç¯å¢ƒ/Runneré‡å»ºã€‚")
#                     else:
#                         print(f"  é˜¶æ®µæ¨è¿›ä½†ä¸»é˜¶æ®µå’Œç»´åº¦æœªå˜ï¼Œä»…æ›´æ–°ç¯å¢ƒå†…éƒ¨çŠ¶æ€å’Œå¥–åŠ±ã€‚")

#                     # --- 5.5.5 æ‰§è¡Œé˜¶æ®µåˆ‡æ¢/ç¯å¢ƒæ›´æ–° ---
#                     if trigger_recreation:
#                         print(f"--- å¼€å§‹ç¯å¢ƒ/Runneré‡å»ºæµç¨‹ (Stage {old_stage}.{old_sub_stage} -> {new_stage}.{new_sub_stage}) ---")
#                         old_policy_state_dict = None
#                         if model_save_path and os.path.exists(model_save_path):
#                              print(f"  åŠ è½½æ¨¡å‹ {model_save_path} ç”¨äºè¿ç§»...")
#                              loaded_state_dict, _, _, _ = model_transfer.load_checkpoint(model_save_path)
#                              if loaded_state_dict is None: print(f"  âŒ åŠ è½½æ¨¡å‹çŠ¶æ€å¤±è´¥ï¼å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–ã€‚")
#                              else: old_policy_state_dict = loaded_state_dict; print("    âœ… æ¨¡å‹çŠ¶æ€åŠ è½½æˆåŠŸã€‚")
#                         else: print(f"  âš ï¸ æ— æœ‰æ•ˆæ¨¡å‹è·¯å¾„ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–ã€‚")

#                         try:
#                             new_task_info = get_task_info(curriculum_config, new_stage)
#                             new_task_name = new_task_info['task_name']
#                             new_stage_params = new_task_info['stage_params']
#                             print(f"  æ–°ä»»åŠ¡: '{new_task_name}'")

#                             new_base_env_cfg_cls = task_registry.env_cfgs[new_task_name]
#                             new_base_train_cfg_cls = task_registry.train_cfgs[new_task_name]
#                             new_env_cfg = new_base_env_cfg_cls()
#                             new_train_cfg = new_base_train_cfg_cls()

#                             if not hasattr(new_env_cfg, 'curriculum') or not isinstance(new_env_cfg.curriculum, DotDict): new_env_cfg.curriculum = DotDict()
#                             new_env_cfg.curriculum.stage = new_stage
#                             new_env_cfg.curriculum.sub_stage = new_sub_stage
#                             setattr(new_env_cfg.curriculum, f'stage{new_stage}_params', DotDict(new_stage_params))
#                             new_env_cfg.env.num_observations = new_expected_obs_dim
#                             new_env_cfg.env.num_actions = new_expected_act_dim
#                             if new_expected_priv_obs_dim is not None: new_env_cfg.env.num_privileged_obs = new_expected_priv_obs_dim

#                             args.task = new_task_name
#                             new_num_envs = new_stage_params.get('num_envs', args.num_envs)
#                             if cmd_line_num_envs is not None: new_num_envs = cmd_line_num_envs
#                             args.num_envs = new_num_envs
#                             new_env_cfg.env.num_envs = new_num_envs
#                             print(f"  æ–°ç¯å¢ƒæ•°é‡: {args.num_envs}")

#                             print("  æ¸…ç†æ—§ç¯å¢ƒå’Œ Runner...")
#                             if env is not None: env.close(); del env; env = None
#                             if runner is not None: del runner; runner = None
#                             torch.cuda.empty_cache()

#                             recreate_sim = (new_stage != old_stage)
#                             if recreate_sim:
#                                  print("  ä¸»é˜¶æ®µå˜åŒ–ï¼Œé‡æ–°åˆ›å»º Simulation...")
#                                  new_sim_params_dict = {"sim": class_to_dict(new_env_cfg.sim)}
#                                  new_sim_params = parse_sim_params(args, new_sim_params_dict)
#                                  if sim is not None: gym.destroy_sim(sim); sim = None
#                                  sim = gym.create_sim(sim_device_id, graphics_device_id, physics_engine, new_sim_params)
#                                  if sim is None: raise RuntimeError("Failed to create new sim!")
#                                  sim_params = new_sim_params
#                                  print("  âœ… æ–° Simulation åˆ›å»ºæˆåŠŸã€‚")
#                             else:
#                                  print("  å­é˜¶æ®µç»´åº¦å˜åŒ–ï¼Œä¿æŒç°æœ‰ Simulationã€‚")

#                             print("  åˆ›å»ºæ–°ç¯å¢ƒå®ä¾‹...")
#                             env, env_cfg = task_registry.make_env(
#                                 name=args.task, args=args, env_cfg=new_env_cfg,
#                                 gym_handle=gym, sim_handle=sim, sim_params=sim_params
#                             )

#                             update_env_rewards(env, reward_scheduler, new_stage, new_sub_stage)

#                             print("  åˆ›å»ºæ–° Runner...")
#                             new_train_cfg.runner.resume = False
#                             args.resume = False
#                             args.checkpoint = None
#                             runner, train_cfg = task_registry.make_alg_runner(env=env, name=args.task, args=args, train_cfg=new_train_cfg)

#                             if old_policy_state_dict:
#                                 print("  æ‰§è¡Œæ¨¡å‹çŠ¶æ€è¿ç§»...")
#                                 target_policy = runner.alg.actor_critic
#                                 old_env_cfg_for_dims = DotDict({'env': DotDict(old_env_dims)})
#                                 current_env_cfg_for_dims = env_cfg

#                                 model_transfer.transfer_policy(
#                                     old_policy_state_dict=old_policy_state_dict,
#                                     old_cfg=old_env_cfg_for_dims,
#                                     new_cfg=current_env_cfg_for_dims,
#                                     target_policy=target_policy
#                                 )
#                             else:
#                                 print("  æ— æ—§æ¨¡å‹çŠ¶æ€ï¼ŒRunner ä½¿ç”¨éšæœºåˆå§‹åŒ–ã€‚")

#                             runner.global_step = total_env_steps
#                             runner.current_learning_iteration = current_iter + 1
#                             print(f"  Runner çŠ¶æ€å·²æ›´æ–°ã€‚Global Steps: {runner.global_step:,}, Start Iteration: {runner.current_learning_iteration}")

#                             print(f"âœ… ç¯å¢ƒ/Runner é‡å»ºå’Œæ¨¡å‹è¿ç§»å®Œæˆï¼")

#                         except Exception as e:
#                              print(f"âŒâŒâŒ é˜¶æ®µ/ç»´åº¦åˆ‡æ¢å¤±è´¥: {str(e)} âŒâŒâŒ")
#                              if args.debug: import traceback; traceback.print_exc()
#                              print("æ— æ³•å®‰å…¨æ¢å¤ï¼Œåœæ­¢è®­ç»ƒã€‚")
#                              raise e

#                     else: # ç»´åº¦æœªå˜
#                         print("  å­é˜¶æ®µæ¨è¿›ä½†ä¸»é˜¶æ®µå’Œç»´åº¦æœªå˜ï¼Œä»…æ›´æ–°ç¯å¢ƒå†…éƒ¨çŠ¶æ€å’Œå¥–åŠ±...")
#                         if hasattr(env, 'cfg') and hasattr(env.cfg, 'curriculum'):
#                             env.cfg.curriculum.sub_stage = new_sub_stage
#                             if hasattr(env, 'current_sub_stage'):
#                                  env.current_sub_stage = new_sub_stage
#                             if hasattr(env, 'update_sub_stage_parameters'):
#                                  print("  è°ƒç”¨ env.update_sub_stage_parameters()")
#                                  env.update_sub_stage_parameters(new_sub_stage)
#                             print(f"  å·²æ›´æ–° env.cfg.curriculum.sub_stage = {new_sub_stage}")

#                         update_env_rewards(env, reward_scheduler, new_stage, new_sub_stage)
#                         print("  âœ… ç¯å¢ƒå†…éƒ¨çŠ¶æ€å’Œå¥–åŠ±æ›´æ–°å®Œæˆã€‚")

#                     # --- æ›´æ–°é˜¶æ®µè·Ÿè¸ªå˜é‡ ---
#                     stage = new_stage
#                     sub_stage = new_sub_stage
#                     print("="*20 + " CURRICULUM ADVANCEMENT END " + "="*20 + "\n")
#                 # --- End of Advancement Check ---

#             # --- End of While Loop ---

#         except KeyboardInterrupt: print("\nğŸ›‘ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
#         except Exception as train_loop_err:
#             print(f"\nâŒâŒâŒ è®­ç»ƒå¾ªç¯ä¸­å‘ç”Ÿæœªæ•è·çš„ä¸¥é‡é”™è¯¯: {train_loop_err}")
#             import traceback
#             traceback.print_exc()
#             print("-------------------------------")


#     finally:
#         # --- 6. æ”¶å°¾å·¥ä½œ ---
#         print("\n--- 6. è®­ç»ƒç»“æŸï¼Œæ‰§è¡Œæ”¶å°¾ ---")
#         # !!! ä½¿ç”¨ä¿®æ­£åçš„ current_steps !!!
#         current_steps = total_env_steps # total_env_steps ç°åœ¨æ€»æ˜¯æœ‰å®šä¹‰çš„
#         try:
#             if 'curriculum_mgr' in locals() and curriculum_mgr:
#                  curriculum_mgr.save_curriculum_state(current_steps)
#                  print(f"  ğŸ’¾ æœ€ç»ˆè¯¾ç¨‹çŠ¶æ€å·²ä¿å­˜ã€‚")
#             if 'runner' in locals() and runner and runner.log_dir:
#                  final_iter = getattr(runner, 'current_learning_iteration', 'final')
#                  final_model_path = os.path.join(runner.log_dir, f'model_{final_iter}.pt')
#                  runner.save(final_model_path)
#                  print(f"  ğŸ’¾ æœ€ç»ˆæ¨¡å‹ä¿å­˜è·¯å¾„: {final_model_path}")
#             if env is not None:
#                  env.close()
#                  print("  âœ… ç¯å¢ƒå·²å…³é—­ã€‚")
#         except Exception as e:
#             print(f"  âŒ ä¿å­˜æœ€ç»ˆçŠ¶æ€æˆ–å…³é—­ç¯å¢ƒå¤±è´¥: {str(e)}")
#         if sim is not None and gym is not None:
#             gym.destroy_sim(sim)
#             print("  âœ… Simulation å·²é”€æ¯ã€‚")
#         print("\nğŸ è®­ç»ƒæµç¨‹ç»“æŸ ğŸ")


# # ==============================================================================
# if __name__ == "__main__":
#     args = parse_args()
#     if args.seed is None: args.seed = int(time.time() * 1000) % 2**32
#     set_seed(args.seed)
#     print(f"ğŸ² ä½¿ç”¨éšæœºç§å­: {args.seed}")
#     train_curriculum(args)


# train_curriculum.py (Relevant sections modified)
import traceback
import argparse
import numpy as np
from datetime import datetime, timedelta
import copy
import inspect
import os
import time
import yaml # For loading curriculum state

# å¯¼å…¥IsaacGym
import isaacgym
from isaacgym import gymapi, gymutil # Import necessary gym modules
import torch

# å¯¼å…¥ G1 ç›¸å…³
import g1.envs # Keep this for task registration trigger
from g1.envs.curriculum.curriculum_manager import CurriculumManager
from g1.envs.curriculum.model_transfer import ModelTransfer
from g1.envs.curriculum.reward_scheduler import RewardScheduler
from g1.envs.configs.curriculum.curriculum_manager_config import CurriculumManagerConfig # å¯¼å…¥é…ç½®ç±»

# å¯¼å…¥å·¥å…·å‡½æ•°å’Œæ³¨å†Œè¡¨
from g1.utils import get_args, task_registry, set_seed
from g1.utils.helpers import update_cfg_from_args, class_to_dict, parse_sim_params, DotDict # Import DotDict

# --- DotDict Class (Assume defined in helpers.py) ---

# --- è§£æå‚æ•° --- (Keep previous parse_args)
def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    args = get_args()
    parser = argparse.ArgumentParser(description='Train with curriculum', add_help=False)
    if not hasattr(args, 'config_class'):
        parser.add_argument('--config_class', type=str, default='CurriculumManagerConfig', help='è¯¾ç¨‹å­¦ä¹ é…ç½®ç±»åç§°')
    if not hasattr(args, 'resume_curriculum'):
        parser.add_argument('--resume_curriculum', type=str, default=None, help='æ¢å¤è®­ç»ƒçš„è¯¾ç¨‹çŠ¶æ€æ–‡ä»¶ (YAML or _latest.yaml)')
    if not hasattr(args, 'debug'):
        parser.add_argument('--debug', action='store_true', default=False, help='å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ‰“å°æ›´å¤šä¿¡æ¯')
    if not hasattr(args, 'force_stage'):
         parser.add_argument('--force_stage', type=str, default=None, help='å¼ºåˆ¶è®¾ç½®åˆå§‹é˜¶æ®µ (ä¾‹å¦‚ "1.3" æˆ– "2.1")ï¼Œä¼šè¦†ç›–æ¢å¤çŠ¶æ€ä¸­çš„é˜¶æ®µ')

    curriculum_args, _ = parser.parse_known_args()
    for key, value in vars(curriculum_args).items():
        setattr(args, key, value)

    if args.headless is None: args.headless = True # Default headless for training
    return args

# --- åŠ è½½è¯¾ç¨‹é…ç½® --- (Keep previous load_curriculum_config)
def load_curriculum_config(config_class_name):
    """åŠ è½½è¯¾ç¨‹å­¦ä¹ é…ç½®ç±»å®ä¾‹ï¼Œå¹¶ç¡®ä¿ç”Ÿæˆæ­£ç¡®çš„åµŒå¥— DotDict ç»“æ„"""
    try:
        if config_class_name == 'CurriculumManagerConfig':
            config_obj = CurriculumManagerConfig()

            # 1. å°†ç±»å®ä¾‹è½¬æ¢ä¸ºå­—å…¸ (å¯ä»¥ä½¿ç”¨ä½ çš„ class_to_dict æˆ–ç®€åŒ–ç‰ˆ)
            config_dict_raw = {}
            for key, value in inspect.getmembers(config_obj):
                 if not key.startswith('_') and not inspect.ismethod(value):
                      # ç›´æ¥å¤åˆ¶å€¼ï¼ŒåµŒå¥—å­—å…¸ä¹Ÿä¿ç•™
                      config_dict_raw[key] = copy.deepcopy(value)

            # 2. ä½¿ç”¨åŸå§‹å­—å…¸åˆ›å»º DotDict
            config_dotdict = DotDict(config_dict_raw)

            # 3. --- ç¡®ä¿é¡¶å±‚ç»“æ„ç¬¦åˆé¢„æœŸ ---
            # CurriculumManager æœŸæœ› cfg æœ‰ .curriculum å’Œ .output å±æ€§
            final_config = DotDict()

            # åˆ›å»º final_config.curriculum
            final_config.curriculum = DotDict()
            curriculum_keys = [
                'initial_stage', 'initial_sub_stage', 'max_stages',
                'max_sub_stages_per_stage', 'max_sub_stages',
                'success_threshold', 'evaluation_window',
                'min_steps_between_eval', 'model_transfer'
            ]
            stage_keys = [k for k in config_dotdict if k.startswith('stage')]
            curriculum_keys.extend(stage_keys)

            for key in curriculum_keys:
                if key in config_dotdict:
                    # ä»åŸå§‹ DotDict å¤åˆ¶åˆ° final_config.curriculum
                    # model_transfer å’Œ stageN çš„å€¼å·²ç»æ˜¯å­—å…¸ï¼Œä¼šè¢« DotDict è‡ªåŠ¨è½¬æ¢
                    setattr(final_config.curriculum, key, config_dotdict[key])

            # åˆ›å»º final_config.output
            if 'output' in config_dotdict and isinstance(config_dotdict.output, dict):
                 final_config.output = DotDict(config_dotdict.output)
            else:
                 final_config.output = DotDict() # åˆ›å»ºç©ºçš„ï¼Œå¦‚æœåŸå§‹é…ç½®æ²¡æœ‰

            # æ·»åŠ å…¶ä»–é¡¶çº§å±æ€§ (ä¾‹å¦‚ max_env_steps)
            if 'max_env_steps' in config_dotdict:
                 final_config.max_env_steps = config_dotdict.max_env_steps

            # ------------------------------------

            print(f"âœ… æˆåŠŸåŠ è½½å¹¶è½¬æ¢è¯¾ç¨‹å­¦ä¹ é…ç½®: {config_class_name}")
            # --- Optional: Debug print structure ---
            # import pprint
            # print("--- Loaded final_config structure ---")
            # print("final_config.curriculum keys:", list(final_config.curriculum.keys()))
            # print("final_config.output keys:", list(final_config.output.keys()))
            # print("final_config other keys:", [k for k in final_config if k not in ['curriculum', 'output']])
            # pprint.pprint(final_config)
            # print("--- End structure ---")
            # -----------------------------------------
            return final_config
        else:
            raise ValueError(f"æœªçŸ¥çš„é…ç½®ç±»åç§°: {config_class_name}")
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½è¯¾ç¨‹å­¦ä¹ é…ç½® {config_class_name}: {str(e)}")
        if args.debug: import traceback; traceback.print_exc()
        return None

# --- è·å–ä»»åŠ¡ä¿¡æ¯ --- (Keep previous get_task_info)
def get_task_info(curriculum_config: DotDict, stage: int) -> dict:
    """æ ¹æ®è¯¾ç¨‹é˜¶æ®µè·å–ä»»åŠ¡åç§°å’Œè¯¥é˜¶æ®µçš„ç‰¹å®šå‚æ•°"""
    stage_key = f'stage{stage}'

    # --- !!! FIX: Access the .curriculum attribute first !!! ---
    if not hasattr(curriculum_config, 'curriculum'):
        raise ValueError("Provided curriculum_config object is missing the 'curriculum' attribute.")

    # Access the stage config *within* the curriculum attribute
    stage_cfg_dotdict = curriculum_config.curriculum.get(stage_key)
    # ---------------------------------------------------------

    if stage_cfg_dotdict is None:
        # Print available keys inside .curriculum for debugging
        available_keys = list(curriculum_config.curriculum.keys()) if hasattr(curriculum_config, 'curriculum') else []
        raise ValueError(f"æœªæ‰¾åˆ°é˜¶æ®µ {stage} çš„é…ç½® (Key: '{stage_key}' inside .curriculum). å¯ç”¨é”®: {available_keys}")

    # stage_cfg_dotdict should now be the DotDict for the specific stage
    task_name = stage_cfg_dotdict.get('env_class')
    if not task_name:
        raise ValueError(f"é˜¶æ®µ {stage} é…ç½®ä¸­æœªæŒ‡å®š 'env_class'")

    # Return the stage config as a standard dict
    return {'task_name': task_name, 'stage_params': dict(stage_cfg_dotdict)}

# --- éªŒè¯ä»»åŠ¡å…¼å®¹æ€§ --- (Keep previous validate_task_compatibility)
def validate_task_compatibility(task_name):
    if hasattr(task_registry, 'task_classes') and isinstance(task_registry.task_classes, dict):
        available_tasks = list(task_registry.task_classes.keys())
        if task_name not in available_tasks:
            print(f"âŒ ä»»åŠ¡ '{task_name}' æœªåœ¨ task_registry ä¸­æ³¨å†Œ! å¯ç”¨: {available_tasks}")
            return False
        return True
    else: print("âŒ é”™è¯¯: æ— æ³•è®¿é—® task_registry.task_classesã€‚"); return False

def get_stage_expected_dims(task_registry, curriculum_config, stage, sub_stage):
    """è·å–æŒ‡å®šé˜¶æ®µå’Œå­é˜¶æ®µçš„é¢„æœŸè§‚æµ‹å’ŒåŠ¨ä½œç»´åº¦"""
    try:
        task_info = get_task_info(curriculum_config, stage)
        task_name = task_info['task_name']
        env_cfg_cls = task_registry.env_cfgs[task_name]
        env_cfg = env_cfg_cls() # åˆ›å»ºä¸´æ—¶å®ä¾‹

        # æ£€æŸ¥æ˜¯å¦ä¸º Stage 1 ä¸”æœ‰åµŒå¥—é…ç½®
        is_nested = (stage == 1 and
                     hasattr(env_cfg, 'nested_locomotion_curriculum') and
                     env_cfg.nested_locomotion_curriculum and
                     hasattr(env_cfg, 'sub_stage_params'))

        if is_nested and sub_stage in env_cfg.sub_stage_params:
            sub_params_dict = env_cfg.sub_stage_params[sub_stage]
            # Safely get dimensions, fallback to main env_cfg defaults if missing in sub-stage
            obs_dim = sub_params_dict.get('num_observations', env_cfg.env.num_observations)
            act_dim = sub_params_dict.get('num_actions', env_cfg.env.num_actions)
            priv_obs_dim = sub_params_dict.get('num_privileged_obs')
            # If privileged obs not defined in sub-stage, try main cfg, else infer
            if priv_obs_dim is None:
                priv_obs_dim = getattr(env_cfg.env, 'num_privileged_obs', None)
            if priv_obs_dim is None and obs_dim is not None: # Infer if still None
                priv_obs_dim = obs_dim + 3 # Assume BaseLinVel added

            # Final check for None dimensions after trying fallbacks
            if obs_dim is None or act_dim is None:
                raise ValueError(f"Could not determine dimensions for Stage {stage}.{sub_stage}")

            return obs_dim, act_dim, priv_obs_dim
        else:
            # For non-Stage 1, non-nested, or missing sub-stage params, use main env_cfg defaults
            obs_dim = env_cfg.env.num_observations
            act_dim = env_cfg.env.num_actions
            priv_obs_dim = getattr(env_cfg.env, 'num_privileged_obs', None)
            if priv_obs_dim is None and obs_dim is not None:
                 priv_obs_dim = obs_dim + 3 # Infer if needed

            if obs_dim is None or act_dim is None:
                 raise ValueError(f"Could not determine dimensions for Stage {stage}.{sub_stage} from main config.")

            return obs_dim, act_dim, priv_obs_dim

    except Exception as e:
        print(f"âŒ è·å–é˜¶æ®µ {stage}.{sub_stage} é¢„æœŸç»´åº¦å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None # Return None on error


# --- æ›´æ–°ç¯å¢ƒå¥–åŠ± --- (Keep previous update_env_rewards)
def update_env_rewards(env, reward_scheduler, stage, sub_stage):
    """æ ¹æ®è¯¾ç¨‹é˜¶æ®µä½¿ç”¨ RewardScheduler æ›´æ–°ç¯å¢ƒå®ä¾‹çš„å¥–åŠ±ç³»æ•°"""
    print(f"--- Updating reward scales for Stage {stage}.{sub_stage} ---")
    if not hasattr(env, 'cfg') or not hasattr(env.cfg, 'rewards') or not hasattr(env.cfg.rewards, 'scales'):
        print("  âš ï¸ Cannot update rewards: env.cfg.rewards.scales structure missing.")
        return False
    try:
        reward_scales_dict = reward_scheduler.get_reward_scales(stage, sub_stage)
        scales_target = env.cfg.rewards.scales # Should be an object or dict
        updated_count = 0
        applied_scales = {}

        if isinstance(scales_target, dict): defined_in_env = list(scales_target.keys())
        else: defined_in_env = [attr for attr in dir(scales_target) if not attr.startswith('_') and not callable(getattr(scales_target, attr))]

        for reward_name, scale_value in reward_scales_dict.items():
            if reward_name in defined_in_env:
                try:
                    if isinstance(scales_target, dict): scales_target[reward_name] = scale_value
                    else: setattr(scales_target, reward_name, scale_value)
                    applied_scales[reward_name] = f"{scale_value:.4f}"
                    updated_count += 1
                except Exception as e: print(f"  âš ï¸ Error updating scale '{reward_name}': {e}")

        if updated_count > 0:
            print(f"  âœ… Updated {updated_count} reward scales. Applied: {applied_scales}")
            if hasattr(env, '_prepare_reward_function'):
                 print("    - Calling env._prepare_reward_function()")
                 env._prepare_reward_function()
            return True
        else:
            print(f"  âš ï¸ No matching reward scales found to update for Stage {stage}.{sub_stage}.")
            print(f"    - Available scales in env config: {defined_in_env}")
            print(f"    - Scales provided by scheduler: {list(reward_scales_dict.keys())}")
            return False
    except Exception as e:
        print(f"âŒ Error during reward update: {str(e)}")
        if args.debug: import traceback; traceback.print_exc()
        return False

# ==============================================================================
# ä¸»è®­ç»ƒå‡½æ•°
# ==============================================================================
def train_curriculum(args):
    """è¯¾ç¨‹å­¦ä¹ è®­ç»ƒä¸»å‡½æ•°"""
    print("="*50); print("ğŸš€ å¼€å§‹ G1 è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ (æ”¯æŒåµŒå¥—) ğŸš€"); print("="*50)

    # --- å…¨å±€ Gym å’Œ Sim ---
    gym = None
    sim = None
    env = None
    runner = None
    sim_params = None
    total_env_steps = 0

    try:
        # --- 1. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ ---
        print("\n--- 1. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ ---")
        # ä½¿ç”¨ä¿®æ­£åçš„åŠ è½½å‡½æ•°
        curriculum_config = load_curriculum_config(args.config_class)
        if curriculum_config is None:
            print("âŒ æ— æ³•åŠ è½½è¯¾ç¨‹é…ç½®ï¼Œé€€å‡ºã€‚")
            return # Exit if config loading failed

        # ç°åœ¨ curriculum_config åº”è¯¥å…·æœ‰æ­£ç¡®çš„åµŒå¥—ç»“æ„
        curriculum_mgr = CurriculumManager(curriculum_config) # Pass the correctly structured config
        print(f"âœ… è¯¾ç¨‹ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ (Output: {curriculum_mgr.output_dir})")

        device_arg = getattr(args, 'rl_device', 'cuda:0')
        # ä»æ­£ç¡®çš„åµŒå¥—ç»“æ„ä¸­è·å– model_transfer é…ç½®
        model_transfer_cfg = curriculum_config.curriculum.model_transfer
        if not isinstance(model_transfer_cfg, DotDict): model_transfer_cfg = DotDict(model_transfer_cfg)
        model_transfer_cfg.device = device_arg
        model_transfer = ModelTransfer(model_transfer_cfg)
        print(f"âœ… æ¨¡å‹è¿ç§»å·¥å…·åˆ›å»ºæˆåŠŸ")

        reward_scheduler = RewardScheduler(curriculum_config)
        print(f"âœ… å¥–åŠ±è°ƒåº¦å™¨åˆ›å»ºæˆåŠŸ")

        # --- åˆå§‹åŒ– Gym ---
        print("\n--- åˆå§‹åŒ– Isaac Gym ---")
        gym = gymapi.acquire_gym()

        # --- 2. æ¢å¤çŠ¶æ€ æˆ– å¤„ç†å¼ºåˆ¶é˜¶æ®µ ---
        # ... (æ­¤éƒ¨åˆ†é€»è¾‘ä¿æŒä¸å˜) ...
        print("\n--- 2. æ¢å¤çŠ¶æ€ / å¼ºåˆ¶é˜¶æ®µæ£€æŸ¥ ---")
        loaded_model_path = None # Track the model path intended for loading
        initial_stage_override = None
        initial_sub_stage_override = None
        loaded_steps = 0 # Track steps loaded from checkpoint

        if args.force_stage:
             try:
                  forced_stage_str = args.force_stage.split('.')
                  initial_stage_override = int(forced_stage_str[0])
                  initial_sub_stage_override = int(forced_stage_str[1]) if len(forced_stage_str) > 1 else 1
                  print(f"âš¡ï¸ å¼ºåˆ¶è®¾ç½®åˆå§‹é˜¶æ®µä¸º: {initial_stage_override}.{initial_sub_stage_override}")
                  args.resume_curriculum = None # Ignore curriculum state file if forcing stage
             except ValueError:
                  print(f"âŒ æ— æ•ˆçš„ --force_stage å‚æ•°æ ¼å¼: '{args.force_stage}'. åº”ä¸º 'stage.sub_stage'.")
                  return

        if args.resume_curriculum:
            print(f"å°è¯•ä»è¯¾ç¨‹çŠ¶æ€æ–‡ä»¶æ¢å¤: {args.resume_curriculum}")
            success, loaded_model_path_from_state = curriculum_mgr.load_curriculum_state(args.resume_curriculum)
            if success:
                print(f"âœ… å·²æ¢å¤è¯¾ç¨‹çŠ¶æ€ã€‚å½“å‰é˜¶æ®µ: {curriculum_mgr.current_stage}.{curriculum_mgr.current_sub_stage}")
                if args.checkpoint is None and loaded_model_path_from_state:
                     args.checkpoint = loaded_model_path_from_state
                     args.resume = True
                     loaded_model_path = args.checkpoint
                     print(f"  - ä½¿ç”¨è¯¾ç¨‹çŠ¶æ€ä¸­çš„æ¨¡å‹è·¯å¾„: {loaded_model_path}")
                elif args.checkpoint:
                     loaded_model_path = args.checkpoint
                     args.resume = True
                     print(f"  - å‘½ä»¤è¡Œ --checkpoint {args.checkpoint} ä¼˜å…ˆï¼Œå¿½ç•¥çŠ¶æ€æ–‡ä»¶ä¸­çš„æ¨¡å‹è·¯å¾„ã€‚")
            else:
                print(f"âŒ æ— æ³•æ¢å¤è¯¾ç¨‹çŠ¶æ€ï¼Œå°†ä»å¤´å¼€å§‹ã€‚")
                args.resume_curriculum = None; args.checkpoint = None; args.resume = False
        elif args.checkpoint:
             print(f"ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„æ£€æŸ¥ç‚¹: {args.checkpoint}")
             loaded_model_path = args.checkpoint
             args.resume = True

        # Apply forced stage override AFTER potentially loading curriculum state
        if initial_stage_override is not None:
             curriculum_mgr.current_stage = initial_stage_override
             curriculum_mgr.current_sub_stage = initial_sub_stage_override
             curriculum_mgr.reset_evaluation()
             print(f"  åº”ç”¨å¼ºåˆ¶é˜¶æ®µè¦†ç›–ï¼Œå½“å‰é˜¶æ®µè®¾ä¸º: {curriculum_mgr.current_stage}.{curriculum_mgr.current_sub_stage}")


        # --- 3. è®¾ç½®åˆå§‹é˜¶æ®µã€ä»»åŠ¡å’Œ Sim ---
        # ... (æ­¤éƒ¨åˆ†é€»è¾‘ä¿æŒä¸å˜ï¼Œä¾èµ–ä¿®æ­£åçš„ curriculum_config) ...
        print("\n--- 3. è®¾ç½®åˆå§‹é˜¶æ®µã€ä»»åŠ¡å’Œ Simulation ---")
        stage, sub_stage = curriculum_mgr.get_current_stage_info()
        print(f"æœ€ç»ˆåˆå§‹é˜¶æ®µ: {stage}.{sub_stage}")

        task_name = None; stage_params = None
        env_cfg = None; train_cfg = None
        try:
            task_info = get_task_info(curriculum_config, stage)
            task_name = task_info.get('task_name')
            stage_params = task_info.get('stage_params')
            if not task_name: raise ValueError("è·å–çš„ task_name ä¸ºç©º")
            print(f"é˜¶æ®µ {stage} ä»»åŠ¡ä¿¡æ¯: Task='{task_name}', Params Keys={list(stage_params.keys()) if stage_params else 'N/A'}")

            if not validate_task_compatibility(task_name): return
            print(f"âœ… ä»»åŠ¡ '{task_name}' éªŒè¯é€šè¿‡ã€‚")

            env_cfg, train_cfg = task_registry.get_cfgs(task_name)
            print(f"  åŠ è½½ä»»åŠ¡ '{task_name}' çš„åŸºç¡€ env_cfg å’Œ train_cfg")

            if not hasattr(env_cfg, 'curriculum') or not isinstance(env_cfg.curriculum, DotDict): env_cfg.curriculum = DotDict()
            env_cfg.curriculum.stage = stage
            env_cfg.curriculum.sub_stage = sub_stage
            setattr(env_cfg.curriculum, f'stage{stage}_params', DotDict(stage_params))

            expected_obs, expected_act, expected_priv_obs = get_stage_expected_dims(task_registry, curriculum_config, stage, sub_stage)
            if expected_obs is None or expected_act is None: raise ValueError("æ— æ³•ç¡®å®šåˆå§‹é˜¶æ®µç»´åº¦")
            env_cfg.env.num_observations = expected_obs
            env_cfg.env.num_actions = expected_act
            env_cfg.env.num_privileged_obs = expected_priv_obs # Can be None

            print(f"  è®¾ç½®åˆå§‹ env_cfg ç»´åº¦: Obs={env_cfg.env.num_observations}, Act={env_cfg.env.num_actions}, PrivObs={env_cfg.env.num_privileged_obs}")

            args.task = task_name
            stage_num_envs = stage_params.get('num_envs')
            cmd_line_num_envs = args.num_envs # Store cmd line arg before overwrite
            if args.num_envs is None:
                 args.num_envs = stage_num_envs if stage_num_envs is not None else env_cfg.env.num_envs
            env_cfg.env.num_envs = args.num_envs
            print(f"  å°†ä½¿ç”¨çš„ç¯å¢ƒæ•°é‡: {args.num_envs}")

            print("  åˆ›å»º Simulation...")
            sim_params_dict = {"sim": class_to_dict(env_cfg.sim)}
            sim_params = parse_sim_params(args, sim_params_dict)
            physics_engine = gymapi.SIM_PHYSX
            sim_device_type, sim_device_id = gymutil.parse_device_str(args.sim_device)
            graphics_device_id = sim_device_id if not args.headless else -1
            sim = gym.create_sim(sim_device_id, graphics_device_id, physics_engine, sim_params)
            if sim is None: raise RuntimeError("Failed to create sim!")
            print(f"âœ… Simulation åˆ›å»ºæˆåŠŸ (Sim Handle: {sim})")

        except Exception as e:
            print(f"âŒ è·å–æˆ–è®¾ç½®åˆå§‹é˜¶æ®µ/ä»»åŠ¡/Sim å¤±è´¥: {str(e)}")
            if args.debug: import traceback; traceback.print_exc();
            if sim: gym.destroy_sim(sim); gym = None
            return


        # --- 4. åˆ›å»ºåˆå§‹ç¯å¢ƒå’Œ Runner ---
        # ... (æ­¤éƒ¨åˆ†é€»è¾‘ä¿æŒä¸å˜) ...
        print("\n--- 4. åˆ›å»ºåˆå§‹ç¯å¢ƒå’Œ Runner ---")
        policy_state_dict = None
        loaded_env_dims = None
        loaded_stage_tuple = (0, 0)

        try:
            print(f"  å‡†å¤‡åˆ›å»ºç¯å¢ƒå®ä¾‹...")
            env_cfg.curriculum.stage = stage
            env_cfg.curriculum.sub_stage = sub_stage
            env, env_cfg = task_registry.make_env(
                name=args.task, args=args, env_cfg=env_cfg,
                gym_handle=gym, sim_handle=sim, sim_params=sim_params
            )

            update_env_rewards(env, reward_scheduler, stage, sub_stage)

            checkpoint_to_load = loaded_model_path
            if checkpoint_to_load and os.path.exists(checkpoint_to_load):
                 print(f"  å‡†å¤‡åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®: {checkpoint_to_load}")
                 policy_state_dict, loaded_env_dims, loaded_steps, loaded_stage_tuple = model_transfer.load_checkpoint(checkpoint_to_load)
                 if policy_state_dict is None:
                      print("  âŒ åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®å¤±è´¥ï¼Œå°†éšæœºåˆå§‹åŒ–æ¨¡å‹ã€‚")
                      args.checkpoint = None; args.resume = False; loaded_model_path = None; loaded_steps = 0;
                 else:
                      print(f"  âœ… æ£€æŸ¥ç‚¹æ•°æ®åŠ è½½æˆåŠŸ (æ¥è‡ªé˜¶æ®µ {loaded_stage_tuple[0]}.{loaded_stage_tuple[1]} @ {loaded_steps:,} æ­¥)")
                      args.resume = True
                      if hasattr(train_cfg, 'runner'): train_cfg.runner.resume = True
                      else: print("âš ï¸ Warning: train_cfg missing runner attribute for setting resume.")
            else:
                 if checkpoint_to_load: print(f"  âš ï¸ æŒ‡å®šçš„æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_to_load}")
                 print("  å°†éšæœºåˆå§‹åŒ–æ¨¡å‹ã€‚")
                 args.checkpoint = None; args.resume = False; loaded_model_path = None; loaded_steps = 0;
                 if hasattr(train_cfg, 'runner'): train_cfg.runner.resume = False

            print(f"  å‡†å¤‡åˆ›å»º Runner (Task: {args.task})...")
            runner, train_cfg = task_registry.make_alg_runner(env=env, name=args.task, args=args, train_cfg=train_cfg)

            if policy_state_dict:
                 target_policy = runner.alg.actor_critic
                 current_env_dims = {
                      'num_observations': env.num_observations,
                      'num_privileged_obs': env.num_privileged_obs,
                      'num_actions': env.num_actions
                 }
                 print(f"  å½“å‰ç¯å¢ƒå®é™…ç»´åº¦: Obs={current_env_dims['num_observations']}, PrivObs={current_env_dims['num_privileged_obs']}, Act={current_env_dims['num_actions']}")
                 print(f"  åŠ è½½çš„æ¨¡å‹ç»´åº¦: Obs={loaded_env_dims['num_observations']}, PrivObs={loaded_env_dims['num_privileged_obs']}, Act={loaded_env_dims['num_actions']}")

                 if loaded_env_dims['num_observations'] != current_env_dims['num_observations'] or \
                    loaded_env_dims['num_actions'] != current_env_dims['num_actions'] or \
                    (loaded_env_dims.get('num_privileged_obs') is not None and loaded_env_dims.get('num_privileged_obs') != current_env_dims.get('num_privileged_obs')):
                     print("  âš ï¸ æ¨¡å‹ç»´åº¦ä¸åŒ¹é…æˆ–ç‰¹æƒè§‚æµ‹ç»´åº¦å˜åŒ–ï¼Œæ‰§è¡Œæ¨¡å‹è¿ç§»...")
                     old_env_cfg_for_dims = DotDict({'env': DotDict(loaded_env_dims)})
                     current_env_cfg_for_dims = DotDict({'env': DotDict(current_env_dims)})

                     model_transfer.transfer_policy(
                          old_policy_state_dict=policy_state_dict,
                          old_cfg=old_env_cfg_for_dims,
                          new_cfg=current_env_cfg_for_dims,
                          target_policy=target_policy
                     )
                 else:
                      print("  æ£€æŸ¥ç‚¹ç»´åº¦åŒ¹é…ï¼Œç›´æ¥åŠ è½½çŠ¶æ€å­—å…¸...")
                      try:
                           missing_keys, unexpected_keys = target_policy.load_state_dict(policy_state_dict, strict=False)
                           if missing_keys: print(f"    - è­¦å‘Š: åŠ è½½æ—¶ç¼ºå°‘é”®: {[k for k in missing_keys if not k.startswith('optimizer')]}")
                           if unexpected_keys: print(f"    - è­¦å‘Š: åŠ è½½æ—¶å‘ç°æ„å¤–é”®: {unexpected_keys}")
                           print("    âœ… çŠ¶æ€å­—å…¸åŠ è½½æˆåŠŸã€‚")
                      except Exception as e:
                           print(f"    âŒ ç›´æ¥åŠ è½½çŠ¶æ€å­—å…¸å¤±è´¥: {e}ã€‚")

                 if args.resume:
                      runner.global_step = loaded_steps
                      steps_per_iter = getattr(train_cfg.runner, 'num_steps_per_env', 24) * env.num_envs
                      runner.current_learning_iteration = int(loaded_steps / steps_per_iter) if steps_per_iter > 0 else 0
                      total_env_steps = loaded_steps
                      print(f"    æ¢å¤ Runner çŠ¶æ€: Global Steps={runner.global_step:,}, Approx Iteration={runner.current_learning_iteration}")
                      curriculum_mgr.last_eval_step = loaded_steps

            if loaded_model_path: curriculum_mgr.set_latest_model_path(loaded_model_path)

        except Exception as e:
            print(f"âŒ ç¯å¢ƒæˆ–è®­ç»ƒè¿è¡Œå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            if args.debug: import traceback; traceback.print_exc();
            if sim: gym.destroy_sim(sim); gym = None
            if env: env.close(); env = None
            return


        # --- 5. è®­ç»ƒå¾ªç¯ ---
        # ... (è®­ç»ƒå¾ªç¯é€»è¾‘ä¿æŒä¸å˜, åŒ…æ‹¬æ¨è¿›æ£€æŸ¥å’Œé˜¶æ®µåˆ‡æ¢) ...
        print("\n--- 5. å¼€å§‹è®­ç»ƒå¾ªç¯ ---")
        max_iterations = getattr(train_cfg.runner, 'max_iterations', 1500)
        # ä½¿ç”¨ curriculum_config ä¸­çš„ max_env_steps
        max_env_steps = getattr(curriculum_config, 'max_env_steps', 100_000_000)

        print(f"æœ€å¤§ç¯å¢ƒæ­¥æ•°: {max_env_steps:,}")
        print(f"æœ€å¤§è¿­ä»£æ¬¡æ•°: {max_iterations:,}")
        print(f"å½“å‰æ€»ç¯å¢ƒæ­¥æ•°: {total_env_steps:,}")
        print(f"å½“å‰è¿­ä»£æ¬¡æ•°: {runner.current_learning_iteration}")

        start_time_ts = time.time();
        last_save_time_ts = start_time_ts;
        last_log_time_ts = start_time_ts
        if not hasattr(runner, 'current_learning_iteration'): runner.current_learning_iteration = 0

        try:
            while runner.current_learning_iteration < max_iterations and total_env_steps < max_env_steps:
                current_iter = runner.current_learning_iteration
                iter_start_time_ts = time.time()

                # --- 5.1 è¿è¡Œä¸€ä¸ªå­¦ä¹ è¿­ä»£ ---
                try:
                    runner.learn(num_learning_iterations=1, init_at_random_ep_len=True)
                except RuntimeError as e:
                    if "CUDA out of memory" in str(e): print("\nâŒâŒâŒ CUDA Out of Memory! âŒâŒâŒ");torch.cuda.empty_cache(); raise e
                    elif "tensor a" in str(e) and "tensor b" in str(e) or "mat1 and mat2 shapes cannot be multiplied" in str(e):
                         print("\n\n" + "="*50); print(f"âŒ è¿è¡Œæ—¶é”™è¯¯ (å¼ é‡å½¢çŠ¶ä¸åŒ¹é…): {e}"); print("="*50)
                         if hasattr(runner, 'alg') and hasattr(runner.alg, 'actor_critic'): actor = runner.alg.actor_critic.actor; print(f"æ¨¡å‹è¾“å…¥å±‚ In: {getattr(actor[0], 'in_features', 'N/A')}, Out: {getattr(actor[0], 'out_features', 'N/A')}")
                         print(f"ç¯å¢ƒè§‚å¯Ÿç»´åº¦: Actual={env.obs_buf.shape}, Configured={env.num_observations}"); print("="*50)
                         raise e
                    else: print(f"âŒ è®­ç»ƒè¿­ä»£è¿è¡Œæ—¶é”™è¯¯: {str(e)}"); raise e
                except Exception as e: print(f"âŒ è®­ç»ƒè¿­ä»£ä¸­å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {str(e)}"); raise e

                # --- 5.2 è·å–ç»Ÿè®¡æ•°æ®å’Œæ›´æ–°æ­¥æ•° ---
                train_info = runner.get_inference_stats(); train_info = train_info or {}
                new_total_env_steps = runner.global_step
                steps_this_iter = new_total_env_steps - total_env_steps
                total_env_steps = new_total_env_steps

                # --- 5.3 æ—¥å¿—è®°å½• ---
                iter_time_sec = time.time() - iter_start_time_ts
                elapsed_time_sec = time.time() - start_time_ts
                elapsed_timedelta = timedelta(seconds=int(elapsed_time_sec))

                if time.time() - last_log_time_ts > 30 or current_iter % 50 == 0:
                     mean_reward = train_info.get('mean_reward', float('nan')); mean_reward = float(mean_reward) if not isinstance(mean_reward, (int, float)) else mean_reward
                     mean_ep_length = train_info.get('mean_episode_length', float('nan')); mean_ep_length = float(mean_ep_length) if not isinstance(mean_ep_length, (int, float)) else mean_ep_length
                     # å°è¯•ä» env.extras è·å–æœ€æ–°çš„ success_rate
                     current_success_rate = env.extras.get('success_rate', train_info.get('success_rate')) # Runner å¯èƒ½ä¹Ÿä¼šè®¡ç®—
                     if current_success_rate is None: current_success_rate = 0.0 # Default if not found
                     else: current_success_rate = float(current_success_rate) if not isinstance(current_success_rate, (int, float)) else current_success_rate


                     log_msg = (f"S{stage}.{sub_stage} | It {current_iter+1:>5}/{max_iterations} | "
                                f"Steps {total_env_steps/1e6:>6.1f}M/{max_env_steps/1e6:.1f}M | "
                                f"Rew {mean_reward:>6.2f} | Len {mean_ep_length:>5.1f} | "
                                f"SR {current_success_rate:.3f} | iter time {iter_time_sec:.2f}s | Elap {str(elapsed_timedelta)}")
                     print(log_msg)
                     last_log_time_ts = time.time()

                     curriculum_mgr.update_statistics(current_success_rate, mean_reward, steps_this_iter)


                # --- 5.4 ä¿å­˜æ£€æŸ¥ç‚¹ ---
                save_freq_iters = getattr(train_cfg.runner, 'save_interval', 50)
                time_based_save = (time.time() - last_save_time_ts) > 900
                iter_based_save = (current_iter + 1) % save_freq_iters == 0
                is_last_iter = (current_iter + 1 >= max_iterations) or (total_env_steps >= max_env_steps)

                if iter_based_save or time_based_save or is_last_iter:
                     print(f"\n--- Saving Checkpoint (Iteration {current_iter+1}) ---")
                     try:
                         if runner.log_dir: os.makedirs(runner.log_dir, exist_ok=True)
                         save_filename = os.path.join(runner.log_dir, f'model_{current_iter+1}.pt')
                         model_save_path = runner.save(save_filename)
                         if model_save_path:
                             curriculum_mgr.set_latest_model_path(model_save_path)
                             curriculum_mgr.save_curriculum_state(total_env_steps)
                             print(f"âœ… Checkpoint and curriculum state saved successfully.")
                         else: print("âŒ Runner save returned None.")
                         last_save_time_ts = time.time()
                     except Exception as e: print(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹æˆ–è¯¾ç¨‹çŠ¶æ€å¤±è´¥: {str(e)}")


                # --- 5.5 æ£€æŸ¥å¹¶æ¨è¿›è¯¾ç¨‹ ---
                should_advance = curriculum_mgr.should_advance_curriculum(total_env_steps)

                if should_advance:
                    print("\n" + "="*20 + " CURRICULUM ADVANCEMENT TRIGGERED " + "="*20)
                    print(f"æ¡ä»¶æ»¡è¶³ï¼Œå‡†å¤‡ä»é˜¶æ®µ {stage}.{sub_stage} æ¨è¿›...")

                    # 5.5.1 å¼ºåˆ¶ä¿å­˜å½“å‰æ¨¡å‹
                    print("  å¼ºåˆ¶ä¿å­˜å½“å‰æ¨¡å‹...")
                    model_save_path = None
                    try:
                         if runner.log_dir: os.makedirs(runner.log_dir, exist_ok=True)
                         save_filename = os.path.join(runner.log_dir, f'model_{current_iter+1}_pre_transition.pt')
                         model_save_path = runner.save(save_filename)
                         if model_save_path is None or not os.path.exists(model_save_path): model_save_path = save_filename
                         curriculum_mgr.set_latest_model_path(model_save_path)
                         print(f"  âœ… æ¨¡å‹å·²ä¿å­˜: {model_save_path}")
                    except Exception as e:
                         print(f"  âŒ è¯¾ç¨‹æ¨è¿›å‰ä¿å­˜æ¨¡å‹å¤±è´¥: {str(e)}")
                         if runner.log_dir and os.path.exists(runner.log_dir):
                            model_files = [os.path.join(runner.log_dir, f) for f in os.listdir(runner.log_dir) if f.startswith('model_') and f.endswith('.pt')]
                            if model_files:
                                try:
                                    model_save_path = max(model_files, key=os.path.getmtime)
                                    print(f"  æ‰¾åˆ°å¤‡é€‰æ¨¡å‹: {model_save_path}")
                                except:
                                    pass

                    # 5.5.2 è·å–æ—§ç¯å¢ƒçš„å®é™…ç»´åº¦
                    old_obs_dim = env.num_observations
                    old_act_dim = env.num_actions
                    old_priv_obs_dim = env.num_privileged_obs
                    old_env_dims = {'num_observations': old_obs_dim, 'num_privileged_obs': old_priv_obs_dim, 'num_actions': old_act_dim}
                    old_stage = stage
                    old_sub_stage = sub_stage

                    # 5.5.3 æ¨è¿›è¯¾ç¨‹ç®¡ç†å™¨
                    new_stage_str = curriculum_mgr.advance_curriculum(total_env_steps)
                    new_stage, new_sub_stage = map(int, new_stage_str.split('.'))
                    print(f"ğŸ“ è¯¾ç¨‹ç®¡ç†å™¨å·²æ¨è¿›åˆ°æ–°é˜¶æ®µ: {new_stage}.{new_sub_stage}")

                    # 5.5.4 è·å–æ–°é˜¶æ®µé¢„æœŸç»´åº¦å¹¶æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º
                    trigger_recreation = False
                    new_expected_obs_dim, new_expected_act_dim, new_expected_priv_obs_dim = get_stage_expected_dims(task_registry, curriculum_config, new_stage, new_sub_stage)

                    if new_expected_obs_dim is None or new_expected_act_dim is None:
                        print("âŒ æ— æ³•è·å–æ–°é˜¶æ®µçš„é¢„æœŸç»´åº¦ï¼Œåœæ­¢æ¨è¿›ï¼")
                        continue

                    if new_stage != old_stage:
                        trigger_recreation = True
                        print(f"  ä¸»é˜¶æ®µå˜åŒ– ({old_stage} -> {new_stage})ï¼Œè§¦å‘ç¯å¢ƒ/Runneré‡å»ºã€‚")
                    elif new_expected_obs_dim != old_obs_dim or new_expected_act_dim != old_act_dim or new_expected_priv_obs_dim != old_priv_obs_dim:
                        trigger_recreation = True
                        print(f"  é˜¶æ®µå†…ç»´åº¦å˜åŒ– (Obs:{old_obs_dim}->{new_expected_obs_dim}, Act:{old_act_dim}->{new_expected_act_dim}, PrivObs:{old_priv_obs_dim}->{new_expected_priv_obs_dim})ï¼Œè§¦å‘é‡å»ºã€‚")
                    else:
                        print(f"  é˜¶æ®µæ¨è¿›ä½†ä¸»é˜¶æ®µå’Œç»´åº¦æœªå˜ï¼Œä»…æ›´æ–°ç¯å¢ƒå†…éƒ¨çŠ¶æ€å’Œå¥–åŠ±ã€‚")

                    # --- 5.5.5 æ‰§è¡Œé˜¶æ®µåˆ‡æ¢/ç¯å¢ƒæ›´æ–° ---
                    if trigger_recreation:
                        print(f"--- å¼€å§‹ç¯å¢ƒ/Runneré‡å»ºæµç¨‹ (Stage {old_stage}.{old_sub_stage} -> {new_stage}.{new_sub_stage}) ---")
                        old_policy_state_dict = None
                        if model_save_path and os.path.exists(model_save_path):
                             print(f"  åŠ è½½æ¨¡å‹ {model_save_path} ç”¨äºè¿ç§»...")
                             loaded_dict, _, _, _ = model_transfer.load_checkpoint(model_save_path) # Don't need other info here
                             if loaded_dict is None: print(f"  âŒ åŠ è½½æ¨¡å‹çŠ¶æ€å¤±è´¥ï¼å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–ã€‚")
                             else: old_policy_state_dict = loaded_dict; print("    âœ… æ¨¡å‹çŠ¶æ€åŠ è½½æˆåŠŸã€‚")
                        else: print(f"  âš ï¸ æ— æœ‰æ•ˆæ¨¡å‹è·¯å¾„ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–ã€‚")

                        try:
                            new_task_info = get_task_info(curriculum_config, new_stage)
                            new_task_name = new_task_info['task_name']
                            new_stage_params = new_task_info['stage_params']
                            print(f"  æ–°ä»»åŠ¡: '{new_task_name}'")

                            new_env_cfg, new_train_cfg = task_registry.get_cfgs(new_task_name)

                            if not hasattr(new_env_cfg, 'curriculum') or not isinstance(new_env_cfg.curriculum, DotDict): new_env_cfg.curriculum = DotDict()
                            new_env_cfg.curriculum.stage = new_stage
                            new_env_cfg.curriculum.sub_stage = new_sub_stage
                            setattr(new_env_cfg.curriculum, f'stage{new_stage}_params', DotDict(new_stage_params))
                            new_env_cfg.env.num_observations = new_expected_obs_dim
                            new_env_cfg.env.num_actions = new_expected_act_dim
                            new_env_cfg.env.num_privileged_obs = new_expected_priv_obs_dim

                            args.task = new_task_name
                            new_num_envs = new_stage_params.get('num_envs', args.num_envs)
                            if cmd_line_num_envs is not None: new_num_envs = cmd_line_num_envs
                            args.num_envs = new_num_envs
                            new_env_cfg.env.num_envs = new_num_envs
                            print(f"  æ–°ç¯å¢ƒæ•°é‡: {args.num_envs}")

                            print("  æ¸…ç†æ—§ç¯å¢ƒå’Œ Runner...")
                            if env is not None: env.close(); del env; env = None
                            if runner is not None: del runner; runner = None
                            torch.cuda.empty_cache()

                            recreate_sim = (new_stage != old_stage)
                            if recreate_sim:
                                 print("  ä¸»é˜¶æ®µå˜åŒ–ï¼Œé‡æ–°åˆ›å»º Simulation...")
                                 new_sim_params_dict = {"sim": class_to_dict(new_env_cfg.sim)}
                                 new_sim_params = parse_sim_params(args, new_sim_params_dict)
                                 if sim is not None: gym.destroy_sim(sim); sim = None
                                 sim = gym.create_sim(sim_device_id, graphics_device_id, physics_engine, new_sim_params)
                                 if sim is None: raise RuntimeError("Failed to create new sim!")
                                 sim_params = new_sim_params
                                 print("  âœ… æ–° Simulation åˆ›å»ºæˆåŠŸã€‚")

                            print("  åˆ›å»ºæ–°ç¯å¢ƒå®ä¾‹...")
                            env, env_cfg = task_registry.make_env( name=args.task, args=args, env_cfg=new_env_cfg, gym_handle=gym, sim_handle=sim, sim_params=sim_params)
                            update_env_rewards(env, reward_scheduler, new_stage, new_sub_stage)

                            print("  åˆ›å»ºæ–° Runner...")
                            new_train_cfg.runner.resume = False; args.resume = False; args.checkpoint = None
                            runner, train_cfg = task_registry.make_alg_runner(env=env, name=args.task, args=args, train_cfg=new_train_cfg)

                            if old_policy_state_dict:
                                print("  æ‰§è¡Œæ¨¡å‹çŠ¶æ€è¿ç§»...")
                                target_policy = runner.alg.actor_critic
                                old_env_cfg_for_dims = DotDict({'env': DotDict(old_env_dims)}) # Use OLD actual dims
                                new_env_dims = {'num_observations': new_expected_obs_dim, 'num_privileged_obs': new_expected_priv_obs_dim, 'num_actions': new_expected_act_dim}
                                new_env_cfg_for_dims = DotDict({'env': DotDict(new_env_dims)}) # Use NEW expected dims
                                model_transfer.transfer_policy( old_policy_state_dict=old_policy_state_dict, old_cfg=old_env_cfg_for_dims, new_cfg=new_env_cfg_for_dims, target_policy=target_policy)
                            else: print("  æ— æ—§æ¨¡å‹çŠ¶æ€ï¼ŒRunner ä½¿ç”¨éšæœºåˆå§‹åŒ–ã€‚")

                            runner.global_step = total_env_steps
                            runner.current_learning_iteration = current_iter + 1
                            print(f"  Runner çŠ¶æ€å·²æ›´æ–°ã€‚Global Steps: {runner.global_step:,}, Start Iteration: {runner.current_learning_iteration}")
                            print(f"âœ… ç¯å¢ƒ/Runner é‡å»ºå’Œæ¨¡å‹è¿ç§»å®Œæˆï¼")

                        except Exception as e:
                             print(f"âŒâŒâŒ é˜¶æ®µ/ç»´åº¦åˆ‡æ¢å¤±è´¥: {str(e)} âŒâŒâŒ")
                             if args.debug: import traceback; traceback.print_exc()
                             raise e

                    else: # ç»´åº¦æœªå˜
                        print("  é˜¶æ®µæ¨è¿›ä½†ä¸»é˜¶æ®µå’Œç»´åº¦æœªå˜ï¼Œä»…æ›´æ–°ç¯å¢ƒå†…éƒ¨çŠ¶æ€å’Œå¥–åŠ±...")
                        if hasattr(env, 'cfg') and hasattr(env.cfg, 'curriculum'):
                            env.cfg.curriculum.sub_stage = new_sub_stage
                            if hasattr(env, 'current_sub_stage'): env.current_sub_stage = new_sub_stage
                            if hasattr(env, 'update_sub_stage_parameters'):
                                 print("  è°ƒç”¨ env.update_sub_stage_parameters()")
                                 try: env.update_sub_stage_parameters(new_sub_stage)
                                 except Exception as e_update: print(f"  Error calling update_sub_stage_parameters: {e_update}")
                            print(f"  å·²æ›´æ–° env.cfg.curriculum.sub_stage = {new_sub_stage}")
                        update_env_rewards(env, reward_scheduler, new_stage, new_sub_stage)
                        print("  âœ… ç¯å¢ƒå†…éƒ¨çŠ¶æ€å’Œå¥–åŠ±æ›´æ–°å®Œæˆã€‚")

                    # --- æ›´æ–°é˜¶æ®µè·Ÿè¸ªå˜é‡ ---
                    stage = new_stage
                    sub_stage = new_sub_stage
                    print("="*20 + " CURRICULUM ADVANCEMENT END " + "="*20 + "\n")
                # --- End of Advancement Check ---

            # --- End of While Loop ---

        except KeyboardInterrupt: print("\nğŸ›‘ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as train_loop_err:
            print(f"\nâŒâŒâŒ è®­ç»ƒå¾ªç¯ä¸­å‘ç”Ÿæœªæ•è·çš„ä¸¥é‡é”™è¯¯: {train_loop_err}")
            import traceback
            traceback.print_exc()

    finally:
        # --- 6. æ”¶å°¾å·¥ä½œ ---
        print("\n--- 6. è®­ç»ƒç»“æŸï¼Œæ‰§è¡Œæ”¶å°¾ ---")
        current_steps = total_env_steps
        try:
            if 'curriculum_mgr' in locals() and curriculum_mgr:
                 curriculum_mgr.save_curriculum_state(current_steps)
                 print(f"  ğŸ’¾ æœ€ç»ˆè¯¾ç¨‹çŠ¶æ€å·²ä¿å­˜ã€‚")
            if 'runner' in locals() and runner and runner.log_dir:
                 final_iter = getattr(runner, 'current_learning_iteration', 'final')
                 final_model_path = os.path.join(runner.log_dir, f'model_{final_iter}.pt')
                 runner.save(final_model_path)
                 print(f"  ğŸ’¾ æœ€ç»ˆæ¨¡å‹ä¿å­˜è·¯å¾„: {final_model_path}")
            if 'env' in locals() and env is not None:
                 env.close()
                 print("  âœ… ç¯å¢ƒå·²å…³é—­ã€‚")
        except Exception as e:
            print(f"  âŒ ä¿å­˜æœ€ç»ˆçŠ¶æ€æˆ–å…³é—­ç¯å¢ƒå¤±è´¥: {str(e)}")

        if sim is not None and gym is not None:
            gym.destroy_sim(sim)
            print("  âœ… Simulation å·²é”€æ¯ã€‚")
        print("\nğŸ è®­ç»ƒæµç¨‹ç»“æŸ ğŸ")


# ==============================================================================
if __name__ == "__main__":
    args = parse_args()
    if args.seed is None: args.seed = int(time.time() * 1000) % 2**32
    set_seed(args.seed)
    print(f"ğŸ² ä½¿ç”¨éšæœºç§å­: {args.seed}")

    # --- æ³¨å†Œä»»åŠ¡ ---
    # (ä¿æŒä¹‹å‰çš„æ³¨å†Œä»£ç )
    from g1.envs.g1_basic_locomotion import G1BasicLocomotion
    from g1.envs.configs.curriculum.stage1_locomotion_config import Stage1LocomotionConfig, Stage1LocomotionConfigPPO
    from g1.envs.g1_kitchen_navigation import G1KitchenNavigation
    from g1.envs.configs.curriculum.stage2_kitchen_nav_config import Stage2KitchenNavConfig, Stage2KitchenNavCfgPPO
    # Import Stage 3 and 4 classes and configs...

    task_registry.register("G1BasicLocomotion", G1BasicLocomotion, Stage1LocomotionConfig, Stage1LocomotionConfigPPO)
    task_registry.register("G1KitchenNavigation", G1KitchenNavigation, Stage2KitchenNavConfig, Stage2KitchenNavCfgPPO)
    # task_registry.register("G1KitchenInteraction", ...)
    # task_registry.register("G1KitchenFullTask", ...)
    print("--- Tasks Registered ---")
    print(f"Available Tasks: {list(task_registry.task_classes.keys())}")

    train_curriculum(args)