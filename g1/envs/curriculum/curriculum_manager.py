
# # curriculum_manager.py
# import os
# import yaml
# import torch
# import numpy as np
# from collections import deque
# import matplotlib.pyplot as plt
# from datetime import datetime
# import time # For potential timing checks


# class CurriculumManager:
#     """è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨ï¼Œè´Ÿè´£ç®¡ç†è¯¾ç¨‹è¿›åº¦ã€çŠ¶æ€å’Œæ¨¡å‹è·¯å¾„"""

#     def __init__(self, cfg):
#         self.cfg = cfg # Keep the main config accessible

#         # è¯¾ç¨‹è®¾ç½® (ä»ä¼ å…¥çš„ cfg ä¸­è·å–)
#         curriculum_cfg = cfg.curriculum # Access the curriculum sub-config
#         self.current_stage = curriculum_cfg.initial_stage
#         self.current_sub_stage = curriculum_cfg.initial_sub_stage
#         self.max_stages = curriculum_cfg.max_stages
#         self.max_sub_stages = curriculum_cfg.max_sub_stages

#         # è¿›é˜¶æ¡ä»¶
#         self.success_threshold = curriculum_cfg.success_threshold
#         # self.min_episodes = curriculum_cfg.min_episodes # Might be better to use env steps
#         self.min_steps_between_eval = getattr(curriculum_cfg, 'min_steps_between_eval', 1000000) # Min steps before checking advancement
#         self.evaluation_window_size = curriculum_cfg.evaluation_window # Number of samples for rate calculation

#         # è®­ç»ƒè·Ÿè¸ª (ä½¿ç”¨æ ‡é‡æˆåŠŸç‡ï¼Œç”± Runner æä¾›)
#         # self.success_history = deque(maxlen=self.evaluation_window_size) # Not used if using scalar rate
#         self.scalar_success_rate_history = deque(maxlen=50) # Store recent scalar rates for smoothing/logging
#         self.reward_history = deque(maxlen=self.evaluation_window_size) # Still useful for logging rewards
#         self.total_env_steps_in_stage = 0 # Track steps within the current stage/sub-stage
#         self.last_eval_step = 0 # Track env step count at last advancement check

#         # è¯¾ç¨‹è¿›å±•è®°å½•
#         self.stage_history = []
#         self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#         # Use output path from main config
#         self.output_dir = os.path.join(cfg.output.output_dir, f"curriculum_{self.timestamp}")
#         os.makedirs(self.output_dir, exist_ok=True)
#         self.latest_model_path = None # Track the path of the last saved model

#         # è®°å½•æ—¥å¿—
#         self.log_file = os.path.join(self.output_dir, "curriculum_log.txt")
#         self._log(f"è¯¾ç¨‹å­¦ä¹ å¼€å§‹æ—¶é—´: {self.timestamp}")
#         self._log(f"åˆå§‹é˜¶æ®µ: {self.current_stage}.{self.current_sub_stage}")
#         self._log(f"æˆåŠŸç‡é˜ˆå€¼: {self.success_threshold}, è¯„ä¼°çª—å£: {self.evaluation_window_size}, æœ€å°è¯„ä¼°é—´éš”æ­¥æ•°: {self.min_steps_between_eval}")
#         self._log("-" * 30)


#     # def _log(self, message):
#     #     """Helper function to print and write to log file."""
#     #     print(message)
#     #     try:
#     #         with open(self.log_file, "a") as f:
#     #             f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {message}\n")
#     #     except Exception as e:
#     #         print(f"Error writing to log file: {e}")


#     def _log(self, message):
#         """Helper function to print and write to log file."""
#         print(message)
#         try:
#             # *** æŒ‡å®š encoding='utf-8' ***
#             with open(self.log_file, "a", encoding='utf-8') as f:
#                 f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {message}\n")
#         except Exception as e:
#             print(f"Error writing to log file: {e}")


#     def update_statistics(self, success_rate_scalar, mean_reward, env_steps_this_iter):
#         """ä½¿ç”¨ Runner æä¾›çš„æ ‡é‡ç»Ÿè®¡æ•°æ®æ›´æ–°ç®¡ç†å™¨çŠ¶æ€"""
#         # Update step counts
#         self.total_env_steps_in_stage += env_steps_this_iter

#         # Store scalar success rate and mean reward
#         if isinstance(success_rate_scalar, (float, int, np.number)):
#              self.scalar_success_rate_history.append(float(success_rate_scalar))
#         else:
#              print(f"âš ï¸ CurriculumManager Warning: Received non-scalar success rate: {success_rate_scalar} (type: {type(success_rate_scalar)})")

#         if isinstance(mean_reward, (float, int, np.number)):
#              self.reward_history.append(float(mean_reward))

#         # Optional: Log every update (can be verbose)
#         # self._log(f"é˜¶æ®µ {self.current_stage}.{self.current_sub_stage} - Steps in stage: {self.total_env_steps_in_stage}, Current SR: {success_rate_scalar:.4f}, Mean Rew: {mean_reward:.4f}")


#     def get_smoothed_success_rate(self):
#         """è·å–å¹³æ»‘åçš„æˆåŠŸç‡ (ä¾‹å¦‚ï¼Œæœ€è¿‘Næ¬¡çš„å¹³å‡å€¼)"""
#         if not self.scalar_success_rate_history:
#             return 0.0
#         # Calculate moving average over the history deque
#         return np.mean(self.scalar_success_rate_history)

#     def get_average_reward(self):
#         """è·å–è¯„ä¼°çª—å£å†…çš„å¹³å‡å¥–åŠ±"""
#         if not self.reward_history:
#             return 0.0
#         return np.mean(self.reward_history)


#     def should_advance_curriculum(self, current_total_env_steps):
#         """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ¨è¿›è¯¾ç¨‹ (åŸºäºæ ‡é‡æˆåŠŸç‡å’Œå¹³æ»‘)"""

#         # force_advance_iterations = 10  # è®¾ç½®å¼ºåˆ¶æ¨è¿›çš„è¿­ä»£æ¬¡æ•°é˜ˆå€¼
#         # if self.current_stage == 1 and current_iteration is not None and current_iteration >= force_advance_iterations:
#         #     self._log(
#         #         f"*** å¼ºåˆ¶æ¨è¿›æ¡ä»¶æ»¡è¶³: Stage 1 ä¸”å½“å‰è¿­ä»£æ¬¡æ•° ({current_iteration}) >= {force_advance_iterations} ***")
#         #     self.last_eval_step = current_total_env_steps  # æ›´æ–°è¯„ä¼°æ­¥æ•°ï¼Œé¿å…ç«‹å³å†æ¬¡è§¦å‘
#         #     return True




#         # 1. æ£€æŸ¥è‡ªä¸Šæ¬¡è¯„ä¼°/æ¨è¿›ä»¥æ¥æ˜¯å¦ç»è¿‡äº†è¶³å¤Ÿçš„æ­¥æ•°
#         steps_since_last_eval = current_total_env_steps - self.last_eval_step
#         if steps_since_last_eval < self.min_steps_between_eval:
#             # print(f"  Curriculum check: Not enough steps since last eval ({steps_since_last_eval}/{self.min_steps_between_eval}).")
#             return False

#         # 2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„æˆåŠŸç‡æ•°æ®ç‚¹è¿›è¡Œå¹³æ»‘
#         if len(self.scalar_success_rate_history) < self.evaluation_window_size: # Use eval window for history size check
#              # print(f"  Curriculum check: Not enough success rate data points ({len(self.scalar_success_rate_history)}/{self.evaluation_window_size}).")
#              return False

#         # 3. è®¡ç®—å¹³æ»‘åçš„æˆåŠŸç‡
#         smoothed_success_rate = self.get_smoothed_success_rate()
#         # print(f"  Curriculum check: Steps since last eval: {steps_since_last_eval}. Smoothed SR ({len(self.scalar_success_rate_history)} samples): {smoothed_success_rate:.4f}")


#         # 4. æ£€æŸ¥æˆåŠŸç‡æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
#         if smoothed_success_rate >= self.success_threshold:
#              self._log(f"*** æ¡ä»¶æ»¡è¶³: å¹³æ»‘æˆåŠŸç‡ {smoothed_success_rate:.4f} >= é˜ˆå€¼ {self.success_threshold} ***")
#              self.last_eval_step = current_total_env_steps # Update last evaluation step count
#              return True
#         else:
#              # Threshold not met, but enough steps have passed, so update last_eval_step anyway
#              # to reset the min_steps counter for the *next* check.
#              self.last_eval_step = current_total_env_steps
#              # print(f"  Curriculum check: Threshold not met. Updated last_eval_step to {current_total_env_steps}.")
#              return False


#     def advance_curriculum(self, current_total_env_steps):
#         """æ¨è¿›è¯¾ç¨‹é˜¶æ®µï¼Œè¿”å›æ–°çš„é˜¶æ®µå­—ç¬¦ä¸² "stage.sub_stage" """
#         old_stage_str = f"{self.current_stage}.{self.current_sub_stage}"
#         smoothed_success_rate = self.get_smoothed_success_rate() # Get rate before clearing history
#         avg_reward = self.get_average_reward()

#         # æ›´æ–°å­é˜¶æ®µ
#         self.current_sub_stage += 1

#         # å¦‚æœå­é˜¶æ®µè¾¾åˆ°æœ€å¤§å€¼ï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªä¸»é˜¶æ®µ
#         if self.current_sub_stage > self.max_sub_stages:
#             self.current_stage += 1
#             self.current_sub_stage = 1

#         # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§é˜¶æ®µ (ä¿æŒåœ¨æœ€å¤§é˜¶æ®µ)
#         if self.current_stage > self.max_stages:
#             self.current_stage = self.max_stages
#             self.current_sub_stage = self.max_sub_stages
#             self._log("å·²è¾¾åˆ°æœ€å¤§è¯¾ç¨‹é˜¶æ®µã€‚")
#             # è¿”å›æ—§é˜¶æ®µå­—ç¬¦ä¸²è¡¨ç¤ºä¸å†æ¨è¿›? æˆ–è€…è¿”å›æœ€å¤§é˜¶æ®µ?
#             # Let's return the max stage string.
#             return f"{self.current_stage}.{self.current_sub_stage}"

#         new_stage_str = f"{self.current_stage}.{self.current_sub_stage}"

#         # è®°å½•é˜¶æ®µå˜åŒ–
#         stage_entry = {
#             "old_stage": old_stage_str,
#             "new_stage": new_stage_str,
#             "total_env_steps": current_total_env_steps, # Record total steps at transition
#             "steps_in_stage": self.total_env_steps_in_stage,
#             "success_rate_at_transition": smoothed_success_rate,
#             "average_reward_at_transition": avg_reward,
#             "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
#             "model_checkpoint": self.latest_model_path # Record model used for transition
#         }
#         self.stage_history.append(stage_entry)

#         # é‡ç½®è¯„ä¼°çŠ¶æ€
#         self.reset_evaluation()

#         # è®°å½•æ—¥å¿—
#         self._log(f"\n======== ğŸ“ è¯¾ç¨‹æ¨è¿› ğŸ“ ========")
#         self._log(f"ä»é˜¶æ®µ {old_stage_str} æ¨è¿›åˆ° {new_stage_str}")
#         self._log(f"æ€»ç¯å¢ƒæ­¥æ•°: {stage_entry['total_env_steps']:,}")
#         self._log(f"æ­¤é˜¶æ®µæ­¥æ•°: {stage_entry['steps_in_stage']:,}")
#         self._log(f"è¿‡æ¸¡æ—¶æˆåŠŸç‡: {stage_entry['success_rate_at_transition']:.4f}")
#         self._log(f"è¿‡æ¸¡æ—¶å¹³å‡å¥–åŠ±: {stage_entry['average_reward_at_transition']:.4f}")
#         self._log(f"æ¨¡å‹æ£€æŸ¥ç‚¹: {stage_entry['model_checkpoint']}")
#         self._log(f"æ—¶é—´: {stage_entry['timestamp']}")
#         self._log("================================\n")

#         # ç»˜åˆ¶å¹¶ä¿å­˜è¯¾ç¨‹è¿›å±•å›¾è¡¨
#         self._save_curriculum_progress_plot()
#         # ä¿å­˜ä¸€æ¬¡è¯¾ç¨‹çŠ¶æ€
#         self.save_curriculum_state(current_total_env_steps)

#         return new_stage_str

#     def reset_evaluation(self):
#         """é‡ç½®ç”¨äºè¯„ä¼°é˜¶æ®µè¿›å±•çš„å†å²è®°å½•å’Œè®¡æ•°å™¨"""
#         self.scalar_success_rate_history.clear()
#         self.reward_history.clear()
#         self.total_env_steps_in_stage = 0
#         # self.last_eval_step is updated in should_advance_curriculum or advance_curriculum
#         self._log("è¯„ä¼°çª—å£å’Œé˜¶æ®µæ­¥æ•°å·²é‡ç½®ã€‚")

#     def _save_curriculum_progress_plot(self):
#         """ä¿å­˜è¯¾ç¨‹è¿›å±•å›¾è¡¨ï¼ˆæˆåŠŸç‡å’Œå¥–åŠ±ï¼‰"""
#         if len(self.stage_history) == 0:
#             return

#         # å‡†å¤‡æ•°æ®
#         stages_labels = [entry["old_stage"] for entry in self.stage_history] # Label points with the stage *before* transition
#         success_rates = [entry["success_rate_at_transition"] for entry in self.stage_history]
#         rewards = [entry["average_reward_at_transition"] for entry in self.stage_history]
#         steps_at_transition = [entry["total_env_steps"] for entry in self.stage_history]

#         try:
#             # åˆ›å»ºå›¾è¡¨
#             fig, ax1 = plt.subplots(figsize=(12, 6))

#             # æˆåŠŸç‡ (å·¦ Y è½´)
#             color = 'tab:blue'
#             ax1.set_xlabel('æ€»ç¯å¢ƒæ­¥æ•° (Environment Steps)')
#             ax1.set_ylabel('å¹³æ»‘æˆåŠŸç‡ (Smoothed Success Rate)', color=color)
#             ax1.plot(steps_at_transition, success_rates, 'o-', color=color, label='Success Rate')
#             ax1.tick_params(axis='y', labelcolor=color)
#             ax1.set_ylim(0, 1.05) # Extend ylim slightly
#             ax1.grid(True, axis='y', linestyle='--', alpha=0.6)

#              # åœ¨ç‚¹ä¸Šæ ‡æ³¨é˜¶æ®µå·
#             for i, label in enumerate(stages_labels):
#                  ax1.text(steps_at_transition[i], success_rates[i] + 0.02, f"->{self.stage_history[i]['new_stage']}", ha='center', va='bottom', fontsize=8, color=color)

#             # å¹³å‡å¥–åŠ± (å³ Y è½´)
#             ax2 = ax1.twinx() # å…±äº« X è½´
#             color = 'tab:orange'
#             ax2.set_ylabel('å¹³å‡å¥–åŠ± (Mean Reward)', color=color)
#             ax2.plot(steps_at_transition, rewards, 's--', color=color, label='Mean Reward')
#             ax2.tick_params(axis='y', labelcolor=color)

#             # æ·»åŠ æ ‡é¢˜å’Œå›¾ä¾‹
#             plt.title(f'è¯¾ç¨‹å­¦ä¹ è¿›å±• ({self.timestamp})')
#             fig.legend(loc="upper center", bbox_to_anchor=(0.5, -0.05), ncol=2) # Combined legend below plot
#             fig.tight_layout(rect=[0, 0.05, 1, 1]) # Adjust layout to make space for legend

#             # ä¿å­˜å›¾è¡¨
#             plot_filename = os.path.join(self.output_dir, "curriculum_progress.png")
#             plt.savefig(plot_filename)
#             plt.close(fig) # Close the figure to free memory
#             self._log(f"è¯¾ç¨‹è¿›å±•å›¾è¡¨å·²ä¿å­˜: {plot_filename}")

#         except Exception as e:
#             self._log(f"âŒ ç»˜åˆ¶æˆ–ä¿å­˜è¯¾ç¨‹è¿›å±•å›¾è¡¨å¤±è´¥: {e}")


#     def save_curriculum_state(self, current_total_env_steps):
#         """ä¿å­˜å½“å‰è¯¾ç¨‹çŠ¶æ€åˆ° YAML æ–‡ä»¶"""
#         state = {
#             "current_stage": self.current_stage,
#             "current_sub_stage": self.current_sub_stage,
#             "total_env_steps_at_save": current_total_env_steps,
#             "last_eval_step": self.last_eval_step,
#             "stage_history": self.stage_history,
#             "latest_model_path": self.latest_model_path, # Save the model path
#             "save_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
#         }

#         # ä¿å­˜ä¸ºYAMLæ–‡ä»¶
#         # Include step count in filename for easier identification
#         filename = os.path.join(self.output_dir, f"curriculum_state_S{self.current_stage}_{self.current_sub_stage}_T{current_total_env_steps}.yaml")
#         try:
#             with open(filename, "w") as f:
#                 yaml.dump(state, f, default_flow_style=False)
#             self._log(f"ğŸ’¾ å½“å‰è¯¾ç¨‹çŠ¶æ€å·²ä¿å­˜: {filename}")
#             return filename
#         except Exception as e:
#             self._log(f"âŒ ä¿å­˜è¯¾ç¨‹çŠ¶æ€å¤±è´¥: {e}")
#             return None

#     def load_curriculum_state(self, filename):
#         """ä» YAML æ–‡ä»¶åŠ è½½è¯¾ç¨‹çŠ¶æ€"""
#         if not os.path.exists(filename):
#             self._log(f"âŒ è¯¾ç¨‹çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
#             return False

#         try:
#             with open(filename, "r") as f:
#                 state = yaml.safe_load(f)

#             self.current_stage = state["current_stage"]
#             self.current_sub_stage = state["current_sub_stage"]
#             # total_env_steps should be obtained from the loaded runner checkpoint
#             self.last_eval_step = state.get("last_eval_step", 0) # Load last eval step
#             self.stage_history = state["stage_history"]
#             self.latest_model_path = state.get("latest_model_path") # Load model path

#             # é‡ç½®è¯„ä¼°çª—å£ï¼Œå› ä¸ºå†å²æ•°æ®æ²¡æœ‰ä¿å­˜
#             self.reset_evaluation()

#             self._log(f"\n======== ğŸ”„ åŠ è½½è¯¾ç¨‹çŠ¶æ€ ========")
#             self._log(f"æ–‡ä»¶: {filename}")
#             self._log(f"æ¢å¤åˆ°é˜¶æ®µ: {self.current_stage}.{self.current_sub_stage}")
#             self._log(f"ä¸Šæ¬¡è¯„ä¼°æ­¥æ•°: {self.last_eval_step}")
#             self._log(f"æ¢å¤çš„æ¨¡å‹è·¯å¾„: {self.latest_model_path}")
#             self._log(f"åŠ è½½æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
#             self._log("================================\n")

#             # è¿”å›åŠ è½½çš„æ¨¡å‹è·¯å¾„ï¼Œè®© train_curriculum ä½¿ç”¨
#             return True, self.latest_model_path

#         except Exception as e:
#             self._log(f"âŒ åŠ è½½è¯¾ç¨‹çŠ¶æ€å¤±è´¥: {e}")
#             import traceback
#             traceback.print_exc()
#             return False, None

#     def get_current_stage_info(self):
#          """è¿”å›å½“å‰é˜¶æ®µå’Œå­é˜¶æ®µ"""
#          return self.current_stage, self.current_sub_stage

#     def set_latest_model_path(self, path):
#          """è®¾ç½®æœ€è¿‘ä¿å­˜çš„æ¨¡å‹è·¯å¾„"""
#          self.latest_model_path = path

import os
import yaml
import torch
import numpy as np
from collections import deque
import matplotlib.pyplot as plt
from datetime import datetime
import time # For potential timing checks
# No need to import DotDict here if cfg is already DotDict when passed in
import traceback # For debug printing

class CurriculumManager:
    """è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨ï¼Œè´Ÿè´£ç®¡ç†è¯¾ç¨‹è¿›åº¦ã€çŠ¶æ€å’Œæ¨¡å‹è·¯å¾„"""

    def __init__(self, cfg): # cfg is expected to be the DotDict created in train_curriculum
        self.cfg = cfg

        # è¯¾ç¨‹è®¾ç½®
        # Access the 'curriculum' sub-dict/DotDict within cfg
        curriculum_cfg = cfg.get('curriculum')
        if curriculum_cfg is None:
            raise ValueError("Curriculum configuration ('curriculum' key) missing in provided cfg.")

        self.current_stage = curriculum_cfg.initial_stage
        self.current_sub_stage = curriculum_cfg.initial_sub_stage
        self.max_stages = curriculum_cfg.max_stages

        # --- !!! FIX: Access data attribute directly, don't call method !!! ---
        max_subs_dict = getattr(curriculum_cfg, 'max_sub_stages_per_stage', None)
        global_max_subs = getattr(curriculum_cfg, 'max_sub_stages', 5) # Default global max

        if isinstance(max_subs_dict, dict):
            # Safely get value for the current stage, fallback to global max
            self.current_max_sub_stages = max_subs_dict.get(str(self.current_stage), global_max_subs) # Use str key for safety if needed
            if str(self.current_stage) not in max_subs_dict:
                 print(f"âš ï¸ CM __init__ Warning: Stage {self.current_stage} not found in max_sub_stages_per_stage dict. Using global max {global_max_subs}.")
        else:
            # Fallback if max_sub_stages_per_stage is missing or not a dict
            print("âš ï¸ CM __init__ Warning: 'max_sub_stages_per_stage' missing or invalid in config. Using global 'max_sub_stages'.")
            self.current_max_sub_stages = global_max_subs
        # --------------------------------------------------------------------

        # è¿›é˜¶æ¡ä»¶
        self.success_threshold = curriculum_cfg.success_threshold
        self.min_steps_between_eval = getattr(curriculum_cfg, 'min_steps_between_eval', 1_000_000)
        self.evaluation_window_size = curriculum_cfg.evaluation_window

        # è®­ç»ƒè·Ÿè¸ª
        self.scalar_success_rate_history = deque(maxlen=self.evaluation_window_size)
        self.reward_history = deque(maxlen=self.evaluation_window_size)
        self.total_env_steps_in_stage = 0
        self.last_eval_step = 0

        # è¯¾ç¨‹è¿›å±•è®°å½•
        self.stage_history = []
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # Access output config safely
        output_cfg = cfg.get('output', {})
        output_dir_base = output_cfg.get('output_dir', "output/curriculum_default") # Default if missing
        self.output_dir = os.path.join(output_dir_base, f"curriculum_{self.timestamp}")
        os.makedirs(self.output_dir, exist_ok=True)
        self.latest_model_path = None

        # è®°å½•æ—¥å¿—
        self.log_file = os.path.join(self.output_dir, "curriculum_log.txt")
        self._log(f"è¯¾ç¨‹å­¦ä¹ å¼€å§‹æ—¶é—´: {self.timestamp}")
        self._log(f"åˆå§‹é˜¶æ®µ: {self.current_stage}.{self.current_sub_stage} (Max Sub: {self.current_max_sub_stages})")
        self._log(f"æˆåŠŸç‡é˜ˆå€¼: {self.success_threshold}, è¯„ä¼°çª—å£: {self.evaluation_window_size}, æœ€å°è¯„ä¼°é—´éš”æ­¥æ•°: {self.min_steps_between_eval}")
        self._log("-" * 30)


    def _log(self, message):
        # (Keep previous implementation)
        print(message)
        try:
            with open(self.log_file, "a", encoding='utf-8') as f:
                f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {message}\n")
        except Exception as e: print(f"Error writing to log file: {e}")

    def update_statistics(self, success_rate_scalar, mean_reward, env_steps_this_iter):
        # (Keep previous implementation)
        self.total_env_steps_in_stage += env_steps_this_iter
        if success_rate_scalar is not None and isinstance(success_rate_scalar, (float, int, np.number)) and np.isfinite(success_rate_scalar):
             self.scalar_success_rate_history.append(float(success_rate_scalar))
        if mean_reward is not None and isinstance(mean_reward, (float, int, np.number)) and np.isfinite(mean_reward):
             self.reward_history.append(float(mean_reward))

    def get_smoothed_success_rate(self):
        # (Keep previous implementation)
        if not self.scalar_success_rate_history: return 0.0
        return np.mean(self.scalar_success_rate_history)

    def get_average_reward(self):
        # (Keep previous implementation)
        if not self.reward_history: return 0.0
        return np.mean(self.reward_history)

    def should_advance_curriculum(self, current_total_env_steps):
        # (Keep previous implementation)
        steps_since_last_eval = current_total_env_steps - self.last_eval_step
        if steps_since_last_eval < self.min_steps_between_eval: return False
        if len(self.scalar_success_rate_history) < self.evaluation_window_size: return False
        smoothed_success_rate = self.get_smoothed_success_rate()
        if smoothed_success_rate >= self.success_threshold:
             self._log(f"*** æ¡ä»¶æ»¡è¶³: å¹³æ»‘æˆåŠŸç‡ {smoothed_success_rate:.4f} >= é˜ˆå€¼ {self.success_threshold} ***")
             self.last_eval_step = current_total_env_steps
             return True
        else:
             self.last_eval_step = current_total_env_steps
             return False

    def advance_curriculum(self, current_total_env_steps):
        """æ¨è¿›è¯¾ç¨‹é˜¶æ®µï¼Œè¿”å›æ–°çš„é˜¶æ®µå­—ç¬¦ä¸² "stage.sub_stage" """
        old_stage_str = f"{self.current_stage}.{self.current_sub_stage}"
        smoothed_success_rate = self.get_smoothed_success_rate()
        avg_reward = self.get_average_reward()

        # æ›´æ–°å­é˜¶æ®µ
        self.current_sub_stage += 1

        # --- !!! FIX: Access data attribute directly !!! ---
        max_subs_dict = getattr(self.cfg.curriculum, 'max_sub_stages_per_stage', None)
        global_max_subs = getattr(self.cfg.curriculum, 'max_sub_stages', 5)

        # Determine max sub-stages for the *current* stage before potentially advancing it
        max_subs_for_current_stage = self.current_max_sub_stages # Use the value stored in the instance

        if self.current_sub_stage > max_subs_for_current_stage:
            self.current_stage += 1
            self.current_sub_stage = 1

            if self.current_stage > self.max_stages:
                self.current_stage = self.max_stages
                # Get max sub for the *final* stage
                if isinstance(max_subs_dict, dict):
                    self.current_max_sub_stages = max_subs_dict.get(str(self.current_stage), global_max_subs)
                else: self.current_max_sub_stages = global_max_subs
                self.current_sub_stage = self.current_max_sub_stages # Stay at last sub-stage
                self._log("å·²è¾¾åˆ°æœ€å¤§è¯¾ç¨‹é˜¶æ®µã€‚")
            else:
                 # Update current_max_sub_stages for the NEW main stage
                 if isinstance(max_subs_dict, dict):
                     self.current_max_sub_stages = max_subs_dict.get(str(self.current_stage), global_max_subs)
                     if str(self.current_stage) not in max_subs_dict:
                          print(f"âš ï¸ CM advance Warning: Stage {self.current_stage} not found in max_sub_stages_per_stage dict. Using global max {global_max_subs}.")
                 else:
                     print("âš ï¸ CM advance Warning: 'max_sub_stages_per_stage' missing or invalid. Using global 'max_sub_stages'.")
                     self.current_max_sub_stages = global_max_subs
        # ----------------------------------------------------

        new_stage_str = f"{self.current_stage}.{self.current_sub_stage}"

        # è®°å½•é˜¶æ®µå˜åŒ– (Keep as before)
        stage_entry = {
            "old_stage": old_stage_str, "new_stage": new_stage_str,
            "total_env_steps": current_total_env_steps, "steps_in_stage": self.total_env_steps_in_stage,
            "success_rate_at_transition": smoothed_success_rate, "average_reward_at_transition": avg_reward,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"), "model_checkpoint": self.latest_model_path
        }
        self.stage_history.append(stage_entry)

        self.reset_evaluation()

        # è®°å½•æ—¥å¿— (Keep as before, added current_max_sub_stages)
        self._log(f"\n======== ğŸ“ è¯¾ç¨‹æ¨è¿› ğŸ“ ========")
        self._log(f"ä»é˜¶æ®µ {old_stage_str} æ¨è¿›åˆ° {new_stage_str}")
        self._log(f"æ–°é˜¶æ®µæœ€å¤§å­é˜¶æ®µæ•°: {self.current_max_sub_stages}") # Log the max for the new stage
        self._log(f"æ€»ç¯å¢ƒæ­¥æ•°: {stage_entry['total_env_steps']:,}")
        self._log(f"æ—§é˜¶æ®µæ­¥æ•°: {stage_entry['steps_in_stage']:,}")
        self._log(f"è¿‡æ¸¡æ—¶æˆåŠŸç‡: {stage_entry['success_rate_at_transition']:.4f}")
        self._log(f"è¿‡æ¸¡æ—¶å¹³å‡å¥–åŠ±: {stage_entry['average_reward_at_transition']:.4f}")
        self._log(f"æ¨¡å‹æ£€æŸ¥ç‚¹: {stage_entry['model_checkpoint']}")
        self._log(f"æ—¶é—´: {stage_entry['timestamp']}")
        self._log("================================\n")

        self._save_curriculum_progress_plot()
        self.save_curriculum_state(current_total_env_steps)

        return new_stage_str

    def reset_evaluation(self):
        # (Keep previous implementation)
        self.scalar_success_rate_history.clear()
        self.reward_history.clear()
        self.total_env_steps_in_stage = 0
        self._log("è¯„ä¼°çª—å£å’Œé˜¶æ®µæ­¥æ•°å·²é‡ç½®ã€‚")

    def _save_curriculum_progress_plot(self):
        # (Keep previous implementation)
        if len(self.stage_history) == 0: return
        stages_labels = [entry["old_stage"] for entry in self.stage_history]
        new_stages_labels = [entry["new_stage"] for entry in self.stage_history]
        success_rates = [entry["success_rate_at_transition"] for entry in self.stage_history]
        rewards = [entry["average_reward_at_transition"] for entry in self.stage_history]
        steps_at_transition = [entry["total_env_steps"] for entry in self.stage_history]
        try:
            plt.style.use('seaborn-v0_8-darkgrid')
            fig, ax1 = plt.subplots(figsize=(15, 7))
            color = 'tab:blue'
            ax1.set_xlabel('æ€»ç¯å¢ƒæ­¥æ•° (Environment Steps)')
            ax1.set_ylabel('å¹³æ»‘æˆåŠŸç‡ (Smoothed Success Rate)', color=color, fontsize=12)
            ax1.plot(steps_at_transition, success_rates, marker='o', linestyle='-', color=color, linewidth=2, markersize=8, label='Success Rate @ Transition')
            ax1.tick_params(axis='y', labelcolor=color, labelsize=10)
            ax1.tick_params(axis='x', labelsize=10)
            ax1.set_ylim(0, 1.05)
            ax1.grid(True, axis='y', linestyle='--', alpha=0.7)
            ax1.axhline(y=self.success_threshold, color='r', linestyle=':', linewidth=1.5, label=f'Success Threshold ({self.success_threshold})')
            for i, label in enumerate(new_stages_labels):
                 offset = 0.03 if i % 2 == 0 else -0.05
                 ax1.text(steps_at_transition[i], success_rates[i] + offset, f"{label}", ha='center', va='bottom', fontsize=9, color=color, bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.7))
            ax2 = ax1.twinx()
            color = 'tab:orange'
            ax2.set_ylabel('å¹³å‡å¥–åŠ± (Mean Reward)', color=color, fontsize=12)
            ax2.plot(steps_at_transition, rewards, marker='s', linestyle='--', color=color, linewidth=2, markersize=8, label='Mean Reward @ Transition')
            ax2.tick_params(axis='y', labelcolor=color, labelsize=10)
            plt.title(f'è¯¾ç¨‹å­¦ä¹ è¿›å±• - {self.timestamp}', fontsize=16)
            lines, labels = ax1.get_legend_handles_labels()
            lines2, labels2 = ax2.get_legend_handles_labels()
            ax2.legend(lines + lines2, labels + labels2, loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3, fontsize=10, frameon=True, shadow=True)
            fig.tight_layout(rect=[0, 0.05, 1, 0.96])
            plot_filename = os.path.join(self.output_dir, "curriculum_progress.png")
            plt.savefig(plot_filename, dpi=150, bbox_inches='tight')
            plt.close(fig)
            self._log(f"è¯¾ç¨‹è¿›å±•å›¾è¡¨å·²ä¿å­˜: {plot_filename}")
        except Exception as e:
            self._log(f"âŒ ç»˜åˆ¶æˆ–ä¿å­˜è¯¾ç¨‹è¿›å±•å›¾è¡¨å¤±è´¥: {e}")
            traceback.print_exc()

    def save_curriculum_state(self, current_total_env_steps):
        # (Keep previous implementation)
        state = {
            "current_stage": self.current_stage, "current_sub_stage": self.current_sub_stage,
            "total_env_steps_at_save": current_total_env_steps, "last_eval_step": self.last_eval_step,
            "stage_history": self.stage_history, "latest_model_path": self.latest_model_path,
            "save_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        filename = os.path.join(self.output_dir, f"curriculum_state_S{self.current_stage}_{self.current_sub_stage}_T{current_total_env_steps}.yaml")
        try:
            os.makedirs(os.path.dirname(filename), exist_ok=True)
            with open(filename, "w", encoding='utf-8') as f: yaml.dump(state, f, default_flow_style=False, allow_unicode=True)
            self._log(f"ğŸ’¾ å½“å‰è¯¾ç¨‹çŠ¶æ€å·²ä¿å­˜: {filename}")
            latest_filename = os.path.join(self.output_dir, "curriculum_state_latest.yaml")
            with open(latest_filename, "w", encoding='utf-8') as f: yaml.dump(state, f, default_flow_style=False, allow_unicode=True)
            return filename
        except Exception as e: self._log(f"âŒ ä¿å­˜è¯¾ç¨‹çŠ¶æ€å¤±è´¥: {e}"); return None

    def load_curriculum_state(self, filename):
        # (Keep previous implementation, added update for current_max_sub_stages)
        if not os.path.exists(filename):
            latest_filename = os.path.join(os.path.dirname(filename), "curriculum_state_latest.yaml")
            if filename.endswith("_latest.yaml") or not os.path.exists(latest_filename):
                 self._log(f"âŒ è¯¾ç¨‹çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨: {filename}"); return False, None
            else: print(f"æŒ‡å®šæ–‡ä»¶ {filename} ä¸å­˜åœ¨ï¼Œå°è¯•åŠ è½½ latest: {latest_filename}"); filename = latest_filename
        try:
            with open(filename, "r", encoding='utf-8') as f: state = yaml.safe_load(f)
            self.current_stage = state["current_stage"]
            self.current_sub_stage = state["current_sub_stage"]
            self.last_eval_step = state.get("last_eval_step", 0)
            self.stage_history = state.get("stage_history", [])
            self.latest_model_path = state.get("latest_model_path")
            self.reset_evaluation()
            # --- FIX: Update current_max_sub_stages after loading ---
            max_subs_dict = getattr(self.cfg.curriculum, 'max_sub_stages_per_stage', None)
            global_max_subs = getattr(self.cfg.curriculum, 'max_sub_stages', 5)
            if isinstance(max_subs_dict, dict):
                self.current_max_sub_stages = max_subs_dict.get(str(self.current_stage), global_max_subs)
            else: self.current_max_sub_stages = global_max_subs
            # --- End Fix ---
            self._log(f"\n======== ğŸ”„ åŠ è½½è¯¾ç¨‹çŠ¶æ€ ========")
            self._log(f"æ–‡ä»¶: {filename}")
            self._log(f"æ¢å¤åˆ°é˜¶æ®µ: {self.current_stage}.{self.current_sub_stage} (Max Sub: {self.current_max_sub_stages})") # Log updated max
            self._log(f"ä¸Šæ¬¡è¯„ä¼°æ­¥æ•°: {self.last_eval_step}")
            self._log(f"æ¢å¤çš„æ¨¡å‹è·¯å¾„: {self.latest_model_path}")
            self._log(f"åŠ è½½æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            self._log("================================\n")
            return True, self.latest_model_path
        except Exception as e:
            self._log(f"âŒ åŠ è½½è¯¾ç¨‹çŠ¶æ€å¤±è´¥ ({filename}): {e}"); traceback.print_exc(); return False, None

    def get_current_stage_info(self):
         # (Keep previous implementation)
         return self.current_stage, self.current_sub_stage

    def set_latest_model_path(self, path):
         # (Keep previous implementation)
         self.latest_model_path = path
