
# curriculum_manager.py
import os
import yaml
import torch
import numpy as np
from collections import deque
import matplotlib.pyplot as plt
from datetime import datetime
import time # For potential timing checks


class CurriculumManager:
    """è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨ï¼Œè´Ÿè´£ç®¡ç†è¯¾ç¨‹è¿›åº¦ã€çŠ¶æ€å’Œæ¨¡å‹è·¯å¾„"""

    def __init__(self, cfg):
        self.cfg = cfg # Keep the main config accessible

        # è¯¾ç¨‹è®¾ç½® (ä»ä¼ å…¥çš„ cfg ä¸­è·å–)
        curriculum_cfg = cfg.curriculum # Access the curriculum sub-config
        self.current_stage = curriculum_cfg.initial_stage
        self.current_sub_stage = curriculum_cfg.initial_sub_stage
        self.max_stages = curriculum_cfg.max_stages
        self.max_sub_stages = curriculum_cfg.max_sub_stages

        # è¿›é˜¶æ¡ä»¶
        self.success_threshold = curriculum_cfg.success_threshold
        # self.min_episodes = curriculum_cfg.min_episodes # Might be better to use env steps
        self.min_steps_between_eval = getattr(curriculum_cfg, 'min_steps_between_eval', 1000000) # Min steps before checking advancement
        self.evaluation_window_size = curriculum_cfg.evaluation_window # Number of samples for rate calculation

        # è®­ç»ƒè·Ÿè¸ª (ä½¿ç”¨æ ‡é‡æˆåŠŸç‡ï¼Œç”± Runner æä¾›)
        # self.success_history = deque(maxlen=self.evaluation_window_size) # Not used if using scalar rate
        self.scalar_success_rate_history = deque(maxlen=50) # Store recent scalar rates for smoothing/logging
        self.reward_history = deque(maxlen=self.evaluation_window_size) # Still useful for logging rewards
        self.total_env_steps_in_stage = 0 # Track steps within the current stage/sub-stage
        self.last_eval_step = 0 # Track env step count at last advancement check

        # è¯¾ç¨‹è¿›å±•è®°å½•
        self.stage_history = []
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # Use output path from main config
        self.output_dir = os.path.join(cfg.output.output_dir, f"curriculum_{self.timestamp}")
        os.makedirs(self.output_dir, exist_ok=True)
        self.latest_model_path = None # Track the path of the last saved model

        # è®°å½•æ—¥å¿—
        self.log_file = os.path.join(self.output_dir, "curriculum_log.txt")
        self._log(f"è¯¾ç¨‹å­¦ä¹ å¼€å§‹æ—¶é—´: {self.timestamp}")
        self._log(f"åˆå§‹é˜¶æ®µ: {self.current_stage}.{self.current_sub_stage}")
        self._log(f"æˆåŠŸç‡é˜ˆå€¼: {self.success_threshold}, è¯„ä¼°çª—å£: {self.evaluation_window_size}, æœ€å°è¯„ä¼°é—´éš”æ­¥æ•°: {self.min_steps_between_eval}")
        self._log("-" * 30)


    # def _log(self, message):
    #     """Helper function to print and write to log file."""
    #     print(message)
    #     try:
    #         with open(self.log_file, "a") as f:
    #             f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {message}\n")
    #     except Exception as e:
    #         print(f"Error writing to log file: {e}")


    def _log(self, message):
        """Helper function to print and write to log file."""
        print(message)
        try:
            # *** æŒ‡å®š encoding='utf-8' ***
            with open(self.log_file, "a", encoding='utf-8') as f:
                f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {message}\n")
        except Exception as e:
            print(f"Error writing to log file: {e}")


    def update_statistics(self, success_rate_scalar, mean_reward, env_steps_this_iter):
        """ä½¿ç”¨ Runner æä¾›çš„æ ‡é‡ç»Ÿè®¡æ•°æ®æ›´æ–°ç®¡ç†å™¨çŠ¶æ€"""
        # Update step counts
        self.total_env_steps_in_stage += env_steps_this_iter

        # Store scalar success rate and mean reward
        if isinstance(success_rate_scalar, (float, int, np.number)):
             self.scalar_success_rate_history.append(float(success_rate_scalar))
        else:
             print(f"âš ï¸ CurriculumManager Warning: Received non-scalar success rate: {success_rate_scalar} (type: {type(success_rate_scalar)})")

        if isinstance(mean_reward, (float, int, np.number)):
             self.reward_history.append(float(mean_reward))

        # Optional: Log every update (can be verbose)
        # self._log(f"é˜¶æ®µ {self.current_stage}.{self.current_sub_stage} - Steps in stage: {self.total_env_steps_in_stage}, Current SR: {success_rate_scalar:.4f}, Mean Rew: {mean_reward:.4f}")


    def get_smoothed_success_rate(self):
        """è·å–å¹³æ»‘åçš„æˆåŠŸç‡ (ä¾‹å¦‚ï¼Œæœ€è¿‘Næ¬¡çš„å¹³å‡å€¼)"""
        if not self.scalar_success_rate_history:
            return 0.0
        # Calculate moving average over the history deque
        return np.mean(self.scalar_success_rate_history)

    def get_average_reward(self):
        """è·å–è¯„ä¼°çª—å£å†…çš„å¹³å‡å¥–åŠ±"""
        if not self.reward_history:
            return 0.0
        return np.mean(self.reward_history)


    def should_advance_curriculum(self, current_total_env_steps):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ¨è¿›è¯¾ç¨‹ (åŸºäºæ ‡é‡æˆåŠŸç‡å’Œå¹³æ»‘)"""

        # force_advance_iterations = 10  # è®¾ç½®å¼ºåˆ¶æ¨è¿›çš„è¿­ä»£æ¬¡æ•°é˜ˆå€¼
        # if self.current_stage == 1 and current_iteration is not None and current_iteration >= force_advance_iterations:
        #     self._log(
        #         f"*** å¼ºåˆ¶æ¨è¿›æ¡ä»¶æ»¡è¶³: Stage 1 ä¸”å½“å‰è¿­ä»£æ¬¡æ•° ({current_iteration}) >= {force_advance_iterations} ***")
        #     self.last_eval_step = current_total_env_steps  # æ›´æ–°è¯„ä¼°æ­¥æ•°ï¼Œé¿å…ç«‹å³å†æ¬¡è§¦å‘
        #     return True




        # 1. æ£€æŸ¥è‡ªä¸Šæ¬¡è¯„ä¼°/æ¨è¿›ä»¥æ¥æ˜¯å¦ç»è¿‡äº†è¶³å¤Ÿçš„æ­¥æ•°
        steps_since_last_eval = current_total_env_steps - self.last_eval_step
        if steps_since_last_eval < self.min_steps_between_eval:
            # print(f"  Curriculum check: Not enough steps since last eval ({steps_since_last_eval}/{self.min_steps_between_eval}).")
            return False

        # 2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„æˆåŠŸç‡æ•°æ®ç‚¹è¿›è¡Œå¹³æ»‘
        if len(self.scalar_success_rate_history) < self.evaluation_window_size: # Use eval window for history size check
             # print(f"  Curriculum check: Not enough success rate data points ({len(self.scalar_success_rate_history)}/{self.evaluation_window_size}).")
             return False

        # 3. è®¡ç®—å¹³æ»‘åçš„æˆåŠŸç‡
        smoothed_success_rate = self.get_smoothed_success_rate()
        # print(f"  Curriculum check: Steps since last eval: {steps_since_last_eval}. Smoothed SR ({len(self.scalar_success_rate_history)} samples): {smoothed_success_rate:.4f}")


        # 4. æ£€æŸ¥æˆåŠŸç‡æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
        if smoothed_success_rate >= self.success_threshold:
             self._log(f"*** æ¡ä»¶æ»¡è¶³: å¹³æ»‘æˆåŠŸç‡ {smoothed_success_rate:.4f} >= é˜ˆå€¼ {self.success_threshold} ***")
             self.last_eval_step = current_total_env_steps # Update last evaluation step count
             return True
        else:
             # Threshold not met, but enough steps have passed, so update last_eval_step anyway
             # to reset the min_steps counter for the *next* check.
             self.last_eval_step = current_total_env_steps
             # print(f"  Curriculum check: Threshold not met. Updated last_eval_step to {current_total_env_steps}.")
             return False


    def advance_curriculum(self, current_total_env_steps):
        """æ¨è¿›è¯¾ç¨‹é˜¶æ®µï¼Œè¿”å›æ–°çš„é˜¶æ®µå­—ç¬¦ä¸² "stage.sub_stage" """
        old_stage_str = f"{self.current_stage}.{self.current_sub_stage}"
        smoothed_success_rate = self.get_smoothed_success_rate() # Get rate before clearing history
        avg_reward = self.get_average_reward()

        # æ›´æ–°å­é˜¶æ®µ
        self.current_sub_stage += 1

        # å¦‚æœå­é˜¶æ®µè¾¾åˆ°æœ€å¤§å€¼ï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªä¸»é˜¶æ®µ
        if self.current_sub_stage > self.max_sub_stages:
            self.current_stage += 1
            self.current_sub_stage = 1

        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§é˜¶æ®µ (ä¿æŒåœ¨æœ€å¤§é˜¶æ®µ)
        if self.current_stage > self.max_stages:
            self.current_stage = self.max_stages
            self.current_sub_stage = self.max_sub_stages
            self._log("å·²è¾¾åˆ°æœ€å¤§è¯¾ç¨‹é˜¶æ®µã€‚")
            # è¿”å›æ—§é˜¶æ®µå­—ç¬¦ä¸²è¡¨ç¤ºä¸å†æ¨è¿›? æˆ–è€…è¿”å›æœ€å¤§é˜¶æ®µ?
            # Let's return the max stage string.
            return f"{self.current_stage}.{self.current_sub_stage}"

        new_stage_str = f"{self.current_stage}.{self.current_sub_stage}"

        # è®°å½•é˜¶æ®µå˜åŒ–
        stage_entry = {
            "old_stage": old_stage_str,
            "new_stage": new_stage_str,
            "total_env_steps": current_total_env_steps, # Record total steps at transition
            "steps_in_stage": self.total_env_steps_in_stage,
            "success_rate_at_transition": smoothed_success_rate,
            "average_reward_at_transition": avg_reward,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "model_checkpoint": self.latest_model_path # Record model used for transition
        }
        self.stage_history.append(stage_entry)

        # é‡ç½®è¯„ä¼°çŠ¶æ€
        self.reset_evaluation()

        # è®°å½•æ—¥å¿—
        self._log(f"\n======== ğŸ“ è¯¾ç¨‹æ¨è¿› ğŸ“ ========")
        self._log(f"ä»é˜¶æ®µ {old_stage_str} æ¨è¿›åˆ° {new_stage_str}")
        self._log(f"æ€»ç¯å¢ƒæ­¥æ•°: {stage_entry['total_env_steps']:,}")
        self._log(f"æ­¤é˜¶æ®µæ­¥æ•°: {stage_entry['steps_in_stage']:,}")
        self._log(f"è¿‡æ¸¡æ—¶æˆåŠŸç‡: {stage_entry['success_rate_at_transition']:.4f}")
        self._log(f"è¿‡æ¸¡æ—¶å¹³å‡å¥–åŠ±: {stage_entry['average_reward_at_transition']:.4f}")
        self._log(f"æ¨¡å‹æ£€æŸ¥ç‚¹: {stage_entry['model_checkpoint']}")
        self._log(f"æ—¶é—´: {stage_entry['timestamp']}")
        self._log("================================\n")

        # ç»˜åˆ¶å¹¶ä¿å­˜è¯¾ç¨‹è¿›å±•å›¾è¡¨
        self._save_curriculum_progress_plot()
        # ä¿å­˜ä¸€æ¬¡è¯¾ç¨‹çŠ¶æ€
        self.save_curriculum_state(current_total_env_steps)

        return new_stage_str

    def reset_evaluation(self):
        """é‡ç½®ç”¨äºè¯„ä¼°é˜¶æ®µè¿›å±•çš„å†å²è®°å½•å’Œè®¡æ•°å™¨"""
        self.scalar_success_rate_history.clear()
        self.reward_history.clear()
        self.total_env_steps_in_stage = 0
        # self.last_eval_step is updated in should_advance_curriculum or advance_curriculum
        self._log("è¯„ä¼°çª—å£å’Œé˜¶æ®µæ­¥æ•°å·²é‡ç½®ã€‚")

    def _save_curriculum_progress_plot(self):
        """ä¿å­˜è¯¾ç¨‹è¿›å±•å›¾è¡¨ï¼ˆæˆåŠŸç‡å’Œå¥–åŠ±ï¼‰"""
        if len(self.stage_history) == 0:
            return

        # å‡†å¤‡æ•°æ®
        stages_labels = [entry["old_stage"] for entry in self.stage_history] # Label points with the stage *before* transition
        success_rates = [entry["success_rate_at_transition"] for entry in self.stage_history]
        rewards = [entry["average_reward_at_transition"] for entry in self.stage_history]
        steps_at_transition = [entry["total_env_steps"] for entry in self.stage_history]

        try:
            # åˆ›å»ºå›¾è¡¨
            fig, ax1 = plt.subplots(figsize=(12, 6))

            # æˆåŠŸç‡ (å·¦ Y è½´)
            color = 'tab:blue'
            ax1.set_xlabel('æ€»ç¯å¢ƒæ­¥æ•° (Environment Steps)')
            ax1.set_ylabel('å¹³æ»‘æˆåŠŸç‡ (Smoothed Success Rate)', color=color)
            ax1.plot(steps_at_transition, success_rates, 'o-', color=color, label='Success Rate')
            ax1.tick_params(axis='y', labelcolor=color)
            ax1.set_ylim(0, 1.05) # Extend ylim slightly
            ax1.grid(True, axis='y', linestyle='--', alpha=0.6)

             # åœ¨ç‚¹ä¸Šæ ‡æ³¨é˜¶æ®µå·
            for i, label in enumerate(stages_labels):
                 ax1.text(steps_at_transition[i], success_rates[i] + 0.02, f"->{self.stage_history[i]['new_stage']}", ha='center', va='bottom', fontsize=8, color=color)

            # å¹³å‡å¥–åŠ± (å³ Y è½´)
            ax2 = ax1.twinx() # å…±äº« X è½´
            color = 'tab:orange'
            ax2.set_ylabel('å¹³å‡å¥–åŠ± (Mean Reward)', color=color)
            ax2.plot(steps_at_transition, rewards, 's--', color=color, label='Mean Reward')
            ax2.tick_params(axis='y', labelcolor=color)

            # æ·»åŠ æ ‡é¢˜å’Œå›¾ä¾‹
            plt.title(f'è¯¾ç¨‹å­¦ä¹ è¿›å±• ({self.timestamp})')
            fig.legend(loc="upper center", bbox_to_anchor=(0.5, -0.05), ncol=2) # Combined legend below plot
            fig.tight_layout(rect=[0, 0.05, 1, 1]) # Adjust layout to make space for legend

            # ä¿å­˜å›¾è¡¨
            plot_filename = os.path.join(self.output_dir, "curriculum_progress.png")
            plt.savefig(plot_filename)
            plt.close(fig) # Close the figure to free memory
            self._log(f"è¯¾ç¨‹è¿›å±•å›¾è¡¨å·²ä¿å­˜: {plot_filename}")

        except Exception as e:
            self._log(f"âŒ ç»˜åˆ¶æˆ–ä¿å­˜è¯¾ç¨‹è¿›å±•å›¾è¡¨å¤±è´¥: {e}")


    def save_curriculum_state(self, current_total_env_steps):
        """ä¿å­˜å½“å‰è¯¾ç¨‹çŠ¶æ€åˆ° YAML æ–‡ä»¶"""
        state = {
            "current_stage": self.current_stage,
            "current_sub_stage": self.current_sub_stage,
            "total_env_steps_at_save": current_total_env_steps,
            "last_eval_step": self.last_eval_step,
            "stage_history": self.stage_history,
            "latest_model_path": self.latest_model_path, # Save the model path
            "save_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        # ä¿å­˜ä¸ºYAMLæ–‡ä»¶
        # Include step count in filename for easier identification
        filename = os.path.join(self.output_dir, f"curriculum_state_S{self.current_stage}_{self.current_sub_stage}_T{current_total_env_steps}.yaml")
        try:
            with open(filename, "w") as f:
                yaml.dump(state, f, default_flow_style=False)
            self._log(f"ğŸ’¾ å½“å‰è¯¾ç¨‹çŠ¶æ€å·²ä¿å­˜: {filename}")
            return filename
        except Exception as e:
            self._log(f"âŒ ä¿å­˜è¯¾ç¨‹çŠ¶æ€å¤±è´¥: {e}")
            return None

    def load_curriculum_state(self, filename):
        """ä» YAML æ–‡ä»¶åŠ è½½è¯¾ç¨‹çŠ¶æ€"""
        if not os.path.exists(filename):
            self._log(f"âŒ è¯¾ç¨‹çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
            return False

        try:
            with open(filename, "r") as f:
                state = yaml.safe_load(f)

            self.current_stage = state["current_stage"]
            self.current_sub_stage = state["current_sub_stage"]
            # total_env_steps should be obtained from the loaded runner checkpoint
            self.last_eval_step = state.get("last_eval_step", 0) # Load last eval step
            self.stage_history = state["stage_history"]
            self.latest_model_path = state.get("latest_model_path") # Load model path

            # é‡ç½®è¯„ä¼°çª—å£ï¼Œå› ä¸ºå†å²æ•°æ®æ²¡æœ‰ä¿å­˜
            self.reset_evaluation()

            self._log(f"\n======== ğŸ”„ åŠ è½½è¯¾ç¨‹çŠ¶æ€ ========")
            self._log(f"æ–‡ä»¶: {filename}")
            self._log(f"æ¢å¤åˆ°é˜¶æ®µ: {self.current_stage}.{self.current_sub_stage}")
            self._log(f"ä¸Šæ¬¡è¯„ä¼°æ­¥æ•°: {self.last_eval_step}")
            self._log(f"æ¢å¤çš„æ¨¡å‹è·¯å¾„: {self.latest_model_path}")
            self._log(f"åŠ è½½æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            self._log("================================\n")

            # è¿”å›åŠ è½½çš„æ¨¡å‹è·¯å¾„ï¼Œè®© train_curriculum ä½¿ç”¨
            return True, self.latest_model_path

        except Exception as e:
            self._log(f"âŒ åŠ è½½è¯¾ç¨‹çŠ¶æ€å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None

    def get_current_stage_info(self):
         """è¿”å›å½“å‰é˜¶æ®µå’Œå­é˜¶æ®µ"""
         return self.current_stage, self.current_sub_stage

    def set_latest_model_path(self, path):
         """è®¾ç½®æœ€è¿‘ä¿å­˜çš„æ¨¡å‹è·¯å¾„"""
         self.latest_model_path = path